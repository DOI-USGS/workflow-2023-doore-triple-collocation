{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ca4982e-7af2-4b34-b0bf-e591053fc207",
   "metadata": {},
   "source": [
    "## Randomized Data Example\n",
    "\n",
    "To test the general implementation of Triple Collocation (TC) and Extended Collocation (EC), we will use a randomly generated time series. This will be done in a similar way to that in Section 6 of [Zwieback+2014](https://npg.copernicus.org/articles/19/69/2012/).\n",
    "\n",
    "To get our \"true\" time series values $\\boldsymbol{t}$, we will randomly generate uniform values between 0 and 10, and then convolve this with a boxcar filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075dbd28-c922-4b6e-b9e5-40435613a4eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "\n",
    "nsamples = 1e4\n",
    "t_original = np.random.uniform(0, 10, int(nsamples))\n",
    "t = np.convolve(t_original, np.ones(5)/5, 'same')\n",
    "\n",
    "# Lets plot this true data to see what it looks like\n",
    "fig_convolve = hv.Curve(zip(np.arange(0, nsamples), t), label='Convolved').opts(height=300, width=800, color='blue', xlabel='Sample', ylabel='t')\n",
    "fig_original = hv.Curve(zip(np.arange(0, nsamples), t_original), label='Original').opts(color='red')\n",
    "display(fig_original * fig_convolve)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d96a169-fb38-48e5-8d30-b444724d3520",
   "metadata": {},
   "source": [
    "Now, we will make a set of observations from the true data. Since TC assumes that the error on the true observation is additive, we will add some uncorrelated random noise to the true data along with some affine transformations. For the uncorrelated noise, we will assume:\n",
    "\n",
    "$\\Sigma = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{pmatrix}$,\n",
    "\n",
    "where $\\Sigma$ is the error covariance matrix of the observations.\n",
    "\n",
    "Additionally, we will assume some affine transformations in the data to show that any bias in the observations will not effect the error variance estimates. For this example, we will use the transformations of:\n",
    "\n",
    "$\\begin{align} \\alpha_i = 0; \\beta_i = 1 \\\\ \\alpha_j = 2; \\beta_j = 3 \\\\ \\alpha_k = 4; \\beta_k = 5 \\\\  \\end{align}$,\n",
    "\n",
    "where $\\boldsymbol{X}_i = \\alpha_i + \\beta_i \\boldsymbol{t} + \\boldsymbol{\\varepsilon}_i$ and $\\boldsymbol{X}_i$ and $\\boldsymbol{\\varepsilon}_i$ are the observations and random errors from a given system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02171086-e395-42a3-9a17-363f1faff8e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Sigma = np.array([[1, 0, 0], [0, 2, 0], [0, 0, 3]])\n",
    "errors = np.random.multivariate_normal([0, 0, 0], Sigma, int(nsamples))\n",
    "\n",
    "X = np.zeros((int(nsamples), 3))\n",
    "X[:, 0] = 0 + 1 * t + errors[:, 0]\n",
    "X[:, 1] = 2 + 3 * t + errors[:, 1]\n",
    "X[:, 2] = 4 + 5 * t + errors[:, 2]\n",
    "\n",
    "# Lets plot this observed data over the true data to see what it looks like\n",
    "fig_Xi = hv.Scatter(zip(np.arange(0, nsamples), X[:, 0]), label='i').opts(color='green', marker='square')\n",
    "fig_Xj = hv.Scatter(zip(np.arange(0, nsamples), X[:, 1]), label='j').opts(color='orange', marker='triangle')\n",
    "fig_Xk = hv.Scatter(zip(np.arange(0, nsamples), X[:, 2]), label='k').opts(color='red')\n",
    "display(fig_Xi * fig_Xj * fig_Xk * fig_convolve)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b7a40b-33de-43d5-b7e4-9f0b3f93214c",
   "metadata": {},
   "source": [
    "Now that we have our observations, let's perform the TC estimation. To do this, let's load the TC function we have already made, and check the `help` to get the input and output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dd5bb4-6e6b-452b-8f78-5ace6629c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../TC/TC_function.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b758dd-57c2-4e58-9015-c9b8e7e492f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(tc_covar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2b88a8-fde3-4200-affd-f5f4fd2c6545",
   "metadata": {},
   "source": [
    "So, it takes our input observation array as is and outputs the estimated error variance of the observations. Let's give it a test to see if it works on our simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10744f16-3fd5-481c-88b3-63712f7a45d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "evar = tc_covar(X)\n",
    "print('Expected values:', np.diagonal(Sigma))\n",
    "print('Estimated values:', evar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dce650-ac7c-46df-a14a-d44a992904f0",
   "metadata": {},
   "source": [
    "Great! The estimated values are very close to the expected values. Let's try this whole process again, but this time with multiple samples of different sizes. This way we can see how the number of samples in our data set influences the estimated variances. To create these samples, we have created a simple function to output data sets given the number of samples, covariance matrix, and affine transformation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8667cc76-a7de-4a00-babd-111305708a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(nsamples, Sigma, alpha, beta):\n",
    "    t_original = np.random.uniform(0, 10, int(nsamples))\n",
    "    t = np.convolve(t_original, np.ones(5)/5, 'same')\n",
    " \n",
    "    errors = np.random.multivariate_normal(np.repeat(0, len(Sigma[:,0])), Sigma, int(nsamples))\n",
    "\n",
    "    X = np.zeros((int(nsamples), len(Sigma[:, 0])))\n",
    "    for i in range(len(Sigma[:, 0])):\n",
    "        X[:, i] = alpha[i] + beta[i] * t + errors[:, i]\n",
    "   \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d7facc-5fc1-47be-86f6-bed6a9cc8c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evar = np.zeros((9996, 3))\n",
    "# Need to have at least 5 samples as we use a boxcar filter of width 5\n",
    "for i in range(5, 10001):\n",
    "    X = generate_sample(i, Sigma, alpha=[0, 2, 4], beta=[1, 3, 5])\n",
    "    evar[i-5, :] = tc_covar(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35507aba-f3d9-493a-a750-033301a5852d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig_samplesi = hv.Scatter(zip(np.arange(5, 10001), evar[:, 0]), label='i').opts(color='green', marker='square', height=300, width=800, \n",
    "                                                                               xlabel='Number of Samples', ylabel='Estimated Error Variance',\n",
    "                                                                               xlim=(0, 10000), ylim=(0, 5))\n",
    "fig_truevari = hv.Curve(zip(np.array([0, 10001]), np.repeat(np.diagonal(Sigma)[0], 2)), label='True i').opts(color='lime')\n",
    "\n",
    "fig_samplesj = hv.Scatter(zip(np.arange(5, 10001), evar[:, 1]), label='j').opts(color='orange', marker='triangle')\n",
    "fig_truevarj = hv.Curve(zip(np.array([0, 10001]), np.repeat(np.diagonal(Sigma)[1], 2)), label='True j').opts(color='gold')\n",
    "\n",
    "fig_samplesk = hv.Scatter(zip(np.arange(5, 10001), evar[:, 2]), label='k').opts(color='red')\n",
    "fig_truevark = hv.Curve(zip(np.array([0, 10001]), np.repeat(np.diagonal(Sigma)[2], 2)), label='True k').opts(color='darkred')\n",
    "\n",
    "display(fig_samplesk * fig_samplesj * fig_samplesi * fig_truevari * fig_truevarj * fig_truevark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a696539-854f-439d-811e-5d5eb0b068f0",
   "metadata": {},
   "source": [
    "As expected, variance in the estimated variance decreases with increasing the number of samples. This result is the same as found in [Zwieback+2012](https://npg.copernicus.org/articles/19/69/2012/), where they estimate at least 500 samples are needed to have the variance estimated within 10% of its true value on average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddb4229-79c5-4dc7-ad8d-b58d038c6237",
   "metadata": {
    "tags": []
   },
   "source": [
    "Using this 500 value as the minimum number of samples we would typically want, let's see what happens when we violate assumption 2 of TC (i.e., we add correlation between observing systems).\n",
    "To do this we will simply add some off diagonal terms to the error covariance matrix giving:\n",
    "\n",
    "$\\Sigma = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 2 & 0 \\\\ 1 & 0 & 3 \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67acd731-a0e2-4e62-a811-b33af39e1977",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma_offdiag = np.array([[1, 0, 1], [0, 2, 0], [1, 0, 3]])\n",
    "X = generate_sample(500, Sigma_offdiag, alpha=[0, 2, 4], beta=[1, 3, 5])\n",
    "evar = tc_covar(X)\n",
    "print('Expected values if not correlated:', np.diagonal(Sigma_offdiag))\n",
    "print('Estimated values:', evar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6880e968-9014-4bc0-bd97-6fcdfba6ef12",
   "metadata": {},
   "source": [
    "So, as we can see, adding any sort of correlation can cause serious discrepancies in our estimated values, especially since $\\sigma_{\\varepsilon_k}$ is a negative value. To account for this, we will need to implement a more generalized version of TC to get more accurate estimates. This more generalized version of TC is EC, which utilizes additional observing systems to account for any potential correlation in the errors of the data sets. \n",
    "\n",
    "> While the covariances in the errors caused a negative variance estimate, error variance estimates can also be negative when correlations are not present. This results from situations where two observing systems have approximately order of magnitude larger error variances compared to the third. These larger values dominate the TC calculation and cause a poor estimate of the smaller variance. Therefore, any estimate of an error variance that results in a negative value should be flagged as incorrect and that its error is likely much less than the other observing systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95084b6c-f7d7-45ad-9f31-5b3be411eea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../TC/EC_function.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31680297-2cba-4389-b659-cd06672c1461",
   "metadata": {},
   "source": [
    "To show that our EC function outputs the same result as the TC function, let's test it on this last example that had correlated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4775153d-9f0a-41d4-9b1a-1245f37105ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecovar_matrix = ec_covar(X, corr_sets=[0,1,2])\n",
    "print('Expected values if not correlated:', np.diagonal(Sigma_offdiag))\n",
    "print('Estimated values:', np.diagonal(ecovar_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5ac14e-6746-45e2-a3c0-d3d16a92b98e",
   "metadata": {},
   "source": [
    "Great! So, the EC function returned the exact same estimates as the TC function. Its only additional requirement was for us to say what data sets we thought would be correlated (independent). For this example, we assumed all were independent, even though we knew the first and last were correlated. Now, let's try adding an additional observing system that is independent of the other three to see if we can recover covariances in the errors. To do so, we will add another row and column to the previous error covariance matrix giving:\n",
    "\n",
    "$\\Sigma = \\begin{pmatrix} 1 & 0 & 1 & 0 \\\\ 0 & 2 & 0 & 0 \\\\ 1 & 0 & 3 & 0 \\\\ 0 & 0 & 0 & 2 \\end{pmatrix}$,\n",
    "\n",
    "and use affine transformation parameters of $\\alpha_l = 1$ and $\\beta_l = 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c86f5b-1dfe-482c-acee-eef380f783dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Sigma_offdiag = np.array([[1, 0, 1, 0], [0, 2, 0, 0], [1, 0, 3, 0], [0, 0, 0, 2]])\n",
    "X = generate_sample(500, Sigma_offdiag, alpha=[0, 2, 4, 1], beta=[1, 3, 5, 2])\n",
    "\n",
    "ecovar_matrix = ec_covar(X, corr_sets=[0, 1, 0, 2])\n",
    "print('Expected values error covariance matrix: \\n', Sigma_offdiag)\n",
    "print('Estimated error covariance matrix: \\n', ecovar_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4070f4d2-896f-4e1d-8dde-3cc02e400672",
   "metadata": {},
   "source": [
    "As we can see, adding this additional data set allowed use to have a better estimate of the error variances and covariance. While not all estimates are within 10% of the true value, additional samples would increase the accuracy, similar to what is shown above.\n",
    "\n",
    "> Note that the error covariance estimates on the off-diagonals are not the same value. This difference is a result of how these terms can be estimated. The error covariance term is determined as $\\sigma_{\\varepsilon_i, \\varepsilon_j} = \\sigma_{ij} - \\frac{\\sigma_{ik} \\sigma_{jl}}{\\sigma_{kl}}$, where $\\sigma_{ij}$ is the covariance of observing systems $i$ and $j$ (and similarly for $k$ and $l$). With this formulation, it is theoretically expected that $\\sigma_{\\varepsilon_i, \\varepsilon_j} = \\sigma_{\\varepsilon_j, \\varepsilon_i}$. However, while $\\sigma_{ij} = \\sigma_{ji}$, $\\sigma_{ik} \\sigma_{jl} \\ne \\sigma_{jk} \\sigma_{il}$ in practice due to random noise in the data causing slight non-zero values in these covariances. Therefore, $\\sigma_{\\varepsilon_i, \\varepsilon_j} \\ne \\sigma_{\\varepsilon_j, \\varepsilon_i}$ leading to the differences in off diagonal values. When quoting $\\sigma_{\\varepsilon_i, \\varepsilon_j}$ instead of the whole error covariance matrix, we recommend averaging the corresponding off-diagonal values\n",
    "\n",
    "Finally, while we only show and EC example for four observing systems, EC can be expanded to include any number of observing systems. By adding more systems, more correlations of errors between data sets are allowed, with the only requirement being that at least three of the observing systems are independent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "et_tc",
   "language": "python",
   "name": "et_tc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
