{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e9a6f7c-6e61-4f85-a1e4-2cfb38e3b395",
   "metadata": {},
   "source": [
    "## Simulated Correlated Time Series\n",
    "\n",
    "To test the efficacy of Triple Collocation (TC), we have simulated a correlated time series using the [CoSMoS](https://cran.r-project.org/web/packages/CoSMoS/vignettes/vignette.html) package in R (see [simulation.R](R/simulation.R)). The package uses spatiotemporal correlation structures to generate a time series correlated in time and space. We have chosen to use the Clayton-Weibull spatiotemporal correlation structure, which has correlations exponentially decrease as elements in the series increase in separation in space and time. This is given by\n",
    "\n",
    "$\\rho_{\\rm CW}(\\delta, \\tau) = \\Big(\\exp\\Big(\\theta\\Big(\\frac{\\delta}{b_{\\rm S}}\\Big)^{c_{\\rm S}}\\Big) + \\exp\\Big(\\theta\\Big(\\frac{\\tau}{b_{\\rm T}}\\Big)^{c_{\\rm T}}\\Big) - 1 \\Big)^{-\\frac{1}{\\theta}}$,\n",
    "\n",
    "where $\\rho_{\\rm CW}$ is the correlation structure, $\\delta$ is space, $\\tau$ is time, $\\theta$ is the copula parameter, $b_{\\rm S}$ and $b_{\\rm T}$ are the space and time scale parameters, and $c_{\\rm S}$ and $c_{\\rm T}$ are the space and time shape parameters, respectfully. Since we look at the time series of each spatial pixel individually, this leads to $\\delta = 0$, which simplifies the CW equation to\n",
    "\n",
    "$\\rho_{\\rm CW}(\\tau) = \\Big(\\exp\\Big(\\theta\\Big(\\frac{\\tau}{b_{\\rm T}}\\Big)^{c_{\\rm T}}\\Big)\\Big)^{-\\frac{1}{\\theta}}$.\n",
    "\n",
    "Additionally, the maginal distribution from which to sample was chosen to be a normal distribution. We then ran the simulation three independent times for a 5x5 grid of spatial points and 5000 temporal points. Each of the three simulations utilized different normal distribution shape properties (i.e., mean and variance). Finally, we saved the output (which were matrices in R) from [CoSMoS](https://cran.r-project.org/web/packages/CoSMoS/vignettes/vignette.html) to a hdf5 file for use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40370f9-d4e8-4366-99c0-91fc1f06d99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import holoviews as hv\n",
    "\n",
    "hv.extension(\"bokeh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172f27fb-8dca-41c1-8b15-8f1f432a3e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = h5py.File(\"../Data/examples/simulation.h5\", \"r\")\n",
    "# The parameter names indicate each maginal distributions variance (i.e., 0.3, 0.2, 0.15)\n",
    "sim3 = np.array(f[\"sim3\"])\n",
    "sim2 = np.array(f[\"sim2\"])\n",
    "sim15 = np.array(f[\"sim15\"])\n",
    "\n",
    "print(sim15.shape, sim2.shape, sim3.shape)\n",
    "\n",
    "# Let's stack the three different simulations along a third dimension and adjust dimensions\n",
    "#   to have ndata x nsim x npix\n",
    "sim = np.stack((sim15, sim2, sim3), axis=2).transpose((1, 2, 0))\n",
    "print(sim.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84684812-b0a7-4586-b88b-2abb1a65cd69",
   "metadata": {},
   "source": [
    "Now that we have read in the simulated data, let's estimate the error variances using TC. Since [Zwieback et al. (2012)](https://npg.copernicus.org/articles/19/69/2012/) showed that we only need 500 or so samples to estimate the variances within 10%, we will compute the variances for our simulations using 500 up to 5000 samples to see if including more samples makes a significant difference.\n",
    "\n",
    "> Note that we will use the multi-dimensional EC function to do this. As shown in the [random dataset example](Examples/random_dataset.ipynb), the EC function is equivalent to using the TC function if only three datasets are used. Therefore by using the multi-dimensional EC function, it will be much faster since it parallelizes along the spatial dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3261896c-c085-46f8-b188-6f88bb605f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../TC/EC_function.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34382a53-f699-430b-abcd-7959b006a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It takes a little while to run, about 60s or so\n",
    "evar = np.zeros((3, 25, 4500))\n",
    "for i in range(500, 5000):\n",
    "    # Only need the diagonal, so extract it\n",
    "    evar[:, :, i - 500] = np.diagonal(\n",
    "        ec_covar_multi(sim[0:i, ...], corr_sets=[1, 2, 3])\n",
    "    ).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728fcd85-3b68-4e4a-a480-70811f1decde",
   "metadata": {},
   "source": [
    "Now, let's plot this to see if the variances are estimated well and if including more samples improves our esimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb145292-731b-45ff-9600-9d3e17876f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_samplesi = hv.Curve(\n",
    "    zip(np.arange(500, 5000), np.nanmedian(evar[0, :, :], axis=0)), label=\"i\"\n",
    ").opts(\n",
    "    color=\"green\",\n",
    "    height=300,\n",
    "    width=800,\n",
    "    xlabel=\"Number of Samples\",\n",
    "    ylabel=\"Estimated Error Variance\",\n",
    "    xlim=(500, 5700),\n",
    "    ylim=(0.08, 0.37),\n",
    ")\n",
    "fig_samplesj = hv.Curve(\n",
    "    zip(np.arange(500, 5000), np.nanmedian(evar[1, :, :], axis=0)), label=\"j\"\n",
    ").opts(color=\"orange\")\n",
    "fig_samplesk = hv.Curve(\n",
    "    zip(np.arange(500, 5000), np.nanmedian(evar[2, :, :], axis=0)), label=\"k\"\n",
    ").opts(color=\"red\")\n",
    "\n",
    "fig_truthi = hv.Curve(zip([500, 5000], [0.15, 0.15]), label=\"i Truth\").opts(\n",
    "    color=\"lime\"\n",
    ")\n",
    "fig_truthj = hv.Curve(zip([500, 5000], [0.2, 0.2]), label=\"j Truth\").opts(color=\"gold\")\n",
    "fig_truthk = hv.Curve(zip([500, 5000], [0.3, 0.3]), label=\"k Truth\").opts(\n",
    "    color=\"darkred\"\n",
    ")\n",
    "\n",
    "fig_samplesi * fig_samplesj * fig_samplesk * fig_truthi * fig_truthj * fig_truthk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe0029f-8a93-49d2-b63a-73974551df5e",
   "metadata": {},
   "source": [
    "So, the estimates align fairly well with the truth values, increasing in agreement as more samples are included as we would expect. Therefore, TC works well at estimating each observation system's error variance, if each system has independent errors.\n",
    "\n",
    "Now, let's use TC on a simulation where the errors of different observation systems are correlated. To do this, we ran another [CoSMoS](https://cran.r-project.org/web/packages/CoSMoS/vignettes/vignette.html) simulation using the Clayton-Weibull spatiotemporal correlation structure, normal marginalized distribution, 5x5 spatial pixels, and 40000 time steps. To create correlated observational systems from this simulation, we split the simulation into three observation systems. This is done by distributing the first 3 elements in each time series of each pixel (i.e., indices 0, 1, 2) to each observing system. This is then repeated for the next three elements in the time series, and the next three, and so on until each observing system has 500 time elements.\n",
    "\n",
    "Additionally, to show how decreasing correlation effects the error variance estimates, we can do a similar method of distrubuting the simulated data to each observing system. Instead of the first three elements, we can do every other element till we have three (i.e., indices 0, 2, 4) and repeat till each system has 500 elements. This method can keep being repeated (e.g., indices 0, 3, 6; indices 0, 4, 8, etc.) to make less and less correlated observing systems by continually spreading out the elements distributed to each observing system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6e4172-ff8d-4720-8509-c15c55bf8b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_long = np.array(f[\"sim\"])\n",
    "\n",
    "sim_corr = np.zeros(\n",
    "    (3, sim_long.shape[0], 500, int(np.floor(sim_long.shape[1] / 3 / 500)))\n",
    ")\n",
    "\n",
    "for i in range(int(np.floor(sim_long.shape[1] / 3 / 500))):\n",
    "    obs1 = sim_long[:, slice(0, 500 * (i + 1) * 3, (3 * (i + 1)))]\n",
    "    obs2 = sim_long[:, slice(1 * (i + 1), 500 * (i + 1) * 3 + 1 * (i + 1), 3 * (i + 1))]\n",
    "    obs3 = sim_long[:, slice(2 * (i + 1), 500 * (i + 1) * 3 + 2 * (i + 1), 3 * (i + 1))]\n",
    "    sim_corr[:, :, :, i] = np.stack((obs1, obs2, obs3), axis=0)\n",
    "\n",
    "sim_corr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2e767f-5065-42f4-a0c6-11c41e441457",
   "metadata": {},
   "source": [
    "Now that we have our correlated observing system data from the simulation, we can estimate the error variances using TC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229a0ded-0dc5-4854-99bf-b8881ceffc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "evar_corr = np.diagonal(\n",
    "    ec_covar_multi(sim_corr.transpose((2, 0, 1, 3)), corr_sets=[1, 2, 3])\n",
    ").transpose((2, 0, 1))\n",
    "evar_med = np.nanmedian(evar_corr, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406d2af0-3f74-4d2d-9d60-7463085b778f",
   "metadata": {},
   "source": [
    "Using the median error variance of all the pixels, we can plot how the error variance estimates change with decreasing correlation (i.e., increasing separation of distributed time elements). Since we know the temporal distance between each element, we can directly calculate the correlation using the chosen parameters in the simulation and use this in our plot.\n",
    "\n",
    "First, let's make a simple function to calculate the correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e9a28-8f2a-4cf5-98da-ded5d1e90f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cw_stcs(tau, theta, bt, ct):\n",
    "    rho = (np.exp(theta * (tau / bt) ** ct)) ** (-1 / theta)\n",
    "    return rho"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027c522f-3183-496a-abeb-9199f7abd39b",
   "metadata": {},
   "source": [
    "Then, we can plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f0ba9b-4594-4920-9888-4451531faa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = cw_stcs(np.arange(np.floor(sim_long.shape[1] / 3 / 500)) + 1, 2, 3, 0.7)\n",
    "\n",
    "fig_var_corr_i = hv.Curve(zip(rho, evar_med[0, :]), label=\"i\").opts(\n",
    "    color=\"green\",\n",
    "    height=300,\n",
    "    width=800,\n",
    "    xlabel=\"CW correlation\",\n",
    "    ylabel=\"Estimated Error Variance\",\n",
    "    logx=True,\n",
    ")\n",
    "fig_var_corr_j = hv.Curve(zip(rho, evar_med[1, :]), label=\"j\").opts(color=\"orange\")\n",
    "fig_var_corr_k = hv.Curve(zip(rho, evar_med[2, :]), label=\"k\").opts(color=\"red\")\n",
    "\n",
    "fig_truth = hv.Curve(\n",
    "    zip(rho, np.repeat(0.15, np.floor(sim_long.shape[1] / 3 / 500))), label=\"Truth\"\n",
    ").opts(color=\"black\")\n",
    "\n",
    "(fig_var_corr_i * fig_var_corr_j * fig_var_corr_k * fig_truth).opts(\n",
    "    legend_position=\"bottom_left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c89f2c8-fd65-42ff-8df7-ed9967ab5398",
   "metadata": {},
   "source": [
    "As we can see, the decrease in correlation (increase distance between distributed points) between observing systems results in error variance estimates that are closer to the truth values. Notice that $i$ and $k$ result in better estimates of the error variance at higher correlations compared to $j$. Since correlation is between pairs of systems and the simulated data was distributed with $i$ getting the first index, $j$ getting the second index, and $k$ getting the third index; we have the correlation pair of $ik$ having less correlation due to them having double the distance in time. Therefore, $j$ is more highly correlated with both $i$ and $k$ compared to $i$ with $k$. This leads to the less accurate estimates of the error variance in $j$ as seen in the plot.\n",
    "\n",
    "To ensure our correlations in our simulated data are being calculated from the CW function as we would expect, we can calculate the Pearson correlation and plot it against our derived value from the CW function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16686096-30a1-4568-8c9e-74e03ed9e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_coef = np.zeros(\n",
    "    (3, 3, sim_long.shape[0], int(np.floor(sim_long.shape[1] / 3 / 500)))\n",
    ")\n",
    "\n",
    "for i in range(int(np.floor(sim_long.shape[1] / 3 / 500))):\n",
    "    for j in range(sim_long.shape[0]):\n",
    "        pearson_coef[:, :, j, i] = np.corrcoef(sim_corr[:, j, :, i])\n",
    "\n",
    "# Use the median correlation of all pixels\n",
    "pearson_med = np.median(pearson_coef, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7167142-fdca-4c08-b226-d884e54d820d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = cw_stcs(\n",
    "    np.stack(\n",
    "        (\n",
    "            (np.arange(np.floor(sim_long.shape[1] / 3 / 500))) + 1,\n",
    "            ((np.arange(np.floor(sim_long.shape[1] / 3 / 500))) + 1) * 2,\n",
    "        ),\n",
    "        axis=1,\n",
    "    ),\n",
    "    2,\n",
    "    3,\n",
    "    0.7,\n",
    ")\n",
    "\n",
    "fig_corrij = hv.Curve(zip(rho[:, 0], pearson_med[0, 1, :]), label=\"ij\").opts(\n",
    "    color=\"green\",\n",
    "    height=500,\n",
    "    width=500,\n",
    "    xlabel=\"CW correlation\",\n",
    "    ylabel=\"Estimated Correlation (Pearson)\",\n",
    ")\n",
    "fig_corrik = hv.Curve(zip(rho[:, 1], pearson_med[0, 2, :]), label=\"ik\").opts(\n",
    "    color=\"orange\"\n",
    ")\n",
    "fig_corrjk = hv.Curve(zip(rho[:, 0], pearson_med[1, 2, :]), label=\"jk\").opts(\n",
    "    color=\"red\"\n",
    ")\n",
    "\n",
    "fig_one2one = hv.Curve(zip([0, 0.7], [0, 0.7])).opts(color=\"black\", line_dash=\"dashed\")\n",
    "\n",
    "(fig_corrij * fig_corrik * fig_corrjk * fig_one2one).opts(\n",
    "    legend_position=\"bottom_right\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c322cc-287e-45b7-a95c-049494682d25",
   "metadata": {},
   "source": [
    "From this plot, we can see that the correlations are close to what we are expecting. Therefore, this shows that in order to use TC to get estimates of the error variance we must have observing systems that have errors that are minimally correlated.\n",
    "\n",
    "Now, let's apply Extended Collocation (EC) to see if adding additional independent data sets allows for us to get better estimates of the error (co)variances when we do have correlation between observing systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1af9a2-42b8-473a-9ce3-bab0eda97e3e",
   "metadata": {},
   "source": [
    "Since EC requires at least three independent observing systems, we will add two additional independent systems to this previous set of three correlated systems, giving us a total of five systems. Of these five, the two added systems are completely independent of each other and the correlated three, meaning we meet the requirements of at least three independent systems. These two systems will be two of the simulated independent systems from the beginning of this notebook. Therefore, with these two new observing systems, we can better estimated the error variance and covariance of the three correlated systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335f65a8-3062-4b72-9fb3-3e2a2d60b6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_EC_corr = np.zeros((5, 25, 500, int(np.floor(sim_long.shape[1] / 3 / 500))))\n",
    "sim_EC_corr[:3, :, :, :] = sim_corr\n",
    "sim_EC_corr[3, :, :, :] = (\n",
    "    sim2[:, :500]\n",
    "    .reshape(25, 500, 1)\n",
    "    .repeat(int(np.floor(sim_long.shape[1] / 3 / 500)), 2)\n",
    ")\n",
    "sim_EC_corr[4, :, :, :] = (\n",
    "    sim3[:, :500]\n",
    "    .reshape(25, 500, 1)\n",
    "    .repeat(int(np.floor(sim_long.shape[1] / 3 / 500)), 2)\n",
    ")\n",
    "\n",
    "# The first three systems are correlated, so we indicate such by giving each associated index the same value in corr_sets\n",
    "ecovar = ec_covar_multi(sim_EC_corr.transpose((2, 0, 1, 3)), corr_sets=[0, 0, 0, 1, 2])\n",
    "ecovar_med = np.nanmedian(ecovar, axis=2)\n",
    "\n",
    "print(\n",
    "    \"Estimated values of l for each correlation of ijk (Expected value = 0.2):\",\n",
    "    ecovar_med[3, 3, :],\n",
    ")\n",
    "print(\n",
    "    \"Estimated values of m for each correlation of ijk (Expected value = 0.3):\",\n",
    "    ecovar_med[4, 4, :],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a734321f-cf2d-4871-948e-0dbd0d47915b",
   "metadata": {},
   "source": [
    "From the EC error estimation, we can see that the two independent data sets had their error variances estimated near their expected values. `l` is slightly below the expectation, and `m` is slightly elevated. These biases could be reduced by including more than the 500 samples we are currently using as shown above, but they are close to being within 10% of the expected value. Finally, let's plot the estimated error (co)variances of the three correlated systems versus the CW correlation to check that using EC effectively estimated and accounted for the correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d373b11-823a-4daa-a8af-7e9c72261bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_vari = hv.Curve(zip(rho[:, 0], ecovar_med[0, 0, :]), label=\"i\").opts(\n",
    "    color=\"green\",\n",
    "    height=300,\n",
    "    width=800,\n",
    "    xlabel=\"CW correlation\",\n",
    "    ylabel=\"Estimated Error (Co)Variance\",\n",
    "    logx=True,\n",
    ")\n",
    "fig_varj = hv.Curve(zip(rho[:, 0], ecovar_med[1, 1, :]), label=\"j\").opts(color=\"orange\")\n",
    "fig_vark = hv.Curve(zip(rho[:, 0], ecovar_med[2, 2, :]), label=\"k\").opts(color=\"red\")\n",
    "\n",
    "fig_covij = hv.Curve(\n",
    "    zip(rho[:, 0], np.mean([ecovar_med[0, 1, :], ecovar_med[1, 0, :]], axis=0)),\n",
    "    label=\"ij covar\",\n",
    ").opts(color=\"green\")\n",
    "fig_covij = fig_covij * hv.Curve(\n",
    "    zip(rho[:, 0], np.mean([ecovar_med[0, 1, :], ecovar_med[1, 0, :]], axis=0)),\n",
    "    label=\"ij covar\",\n",
    ").opts(color=\"orange\", line_dash=\"dashed\")\n",
    "\n",
    "fig_covik = hv.Curve(\n",
    "    zip(rho[:, 1], np.mean([ecovar_med[0, 2, :], ecovar_med[2, 0, :]], axis=0)),\n",
    "    label=\"ik covar\",\n",
    ").opts(color=\"green\")\n",
    "fig_covik = fig_covik * hv.Curve(\n",
    "    zip(rho[:, 1], np.mean([ecovar_med[0, 2, :], ecovar_med[2, 0, :]], axis=0)),\n",
    "    label=\"ik covar\",\n",
    ").opts(color=\"red\", line_dash=\"dashed\")\n",
    "\n",
    "fig_covjk = hv.Curve(\n",
    "    zip(rho[:, 0], np.mean([ecovar_med[1, 2, :], ecovar_med[2, 1, :]], axis=0)),\n",
    "    label=\"jk covar\",\n",
    ").opts(color=\"orange\")\n",
    "fig_covjk = fig_covjk * hv.Curve(\n",
    "    zip(rho[:, 0], np.mean([ecovar_med[1, 2, :], ecovar_med[2, 1, :]], axis=0)),\n",
    "    label=\"jk covar\",\n",
    ").opts(color=\"red\", line_dash=\"dashed\")\n",
    "\n",
    "fig_truth = hv.Curve(\n",
    "    zip(rho[:, 0], np.repeat(0.15, np.floor(sim_long.shape[1] / 3 / 500))),\n",
    "    label=\"Var Truth\",\n",
    ").opts(color=\"black\")\n",
    "fig_truthcov = hv.Curve(\n",
    "    zip(np.sort(rho.flatten()), np.sort(rho.flatten()) * 0.15), label=\"Covar Truth\"\n",
    ").opts(color=\"black\", line_dash=\"dashed\")\n",
    "\n",
    "(\n",
    "    fig_vari\n",
    "    * fig_varj\n",
    "    * fig_vark\n",
    "    * fig_truth\n",
    "    * fig_covij\n",
    "    * fig_covik\n",
    "    * fig_covjk\n",
    "    * fig_truthcov\n",
    ").opts(legend_position=\"top_left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f9e6dc-4c3a-4cdb-b477-8b5663ff51e4",
   "metadata": {},
   "source": [
    "Great! So, EC appropriately accounted for the correlations and estimated the error (co)variances accurately. Therefore, correlations in the errors can be present in the data sets as long as at least three observing systems are not correlated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
