{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "410f15c3-784d-4547-9dc4-ab424c898332",
   "metadata": {},
   "source": [
    "# Triple Collocation Uncertainty Analysis\n",
    "\n",
    "Now that we have all of our monthly ET datasets spatially collocated, we are ready to perform a Triple Collocation (TC) analysis on the common date ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cab12df-6303-4efe-9729-c39b2b0c96e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.xarray\n",
    "import panel as pn\n",
    "import cartopy.crs as ccrs\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import itertools\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a370a119-c6d8-4063-b501-53672e816e32",
   "metadata": {},
   "source": [
    "First, we will run in the Extended Collocation notebook to create our TC (or EC) function that runs each spatial point simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff608c6-411d-4af9-b135-75ccd9bc6a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run TC/EC_function.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10712275-5f82-4a6b-878f-999873578ac0",
   "metadata": {},
   "source": [
    "## Combine Datasets in Xarray\n",
    "Next, we need to load in our datasets and limit them to a common date range. Since we need at least three datasets to utilized TC, we will restrict the data ranges to have a beginning date of the third oldest starting date and the third most recent ending date. This choice allows us to save memory usage, while also utilizing the largest amount of usable data. For triplets with a more restricted date range, due to one data set having a smaller date range, we will limit the date range further at the time of the TC computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c070ad-347a-4f15-8540-3f180f0d4e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['ssebop/ssebop_aet_regridded.nc',\n",
    "         'gleam/gleam_aet.nc',\n",
    "         'era5/era5_aet_regridded.nc',\n",
    "         'nldas/nldas_aet_regridded.nc',\n",
    "         'terraclimate/terraclimate_aet_regridded.nc',        \n",
    "         'wbet/wbet_aet_regridded.nc',\n",
    "         ]\n",
    "dataset_name = ['SSEBop', 'GLEAM', 'ERA5', 'NLDAS', 'TerraClimate', 'WBET']\n",
    "dataset_abrv = ['S', 'G', 'E', 'N', 'T', 'W']\n",
    "\n",
    "date_ranges = np.zeros((2, len(files)), dtype='datetime64[ns]')\n",
    "for i, file in enumerate(files):\n",
    "    set = xr.open_dataset(file, engine='netcdf4', chunks={'lon': -1, 'lat': -1, 'time': -1})\n",
    "    date_ranges[:, i] = [set.time.min().values, set.time.max().values]\n",
    "\n",
    "# Take the third oldest start and third most recent end dates\n",
    "date_range = [np.sort(date_ranges[0, :])[2], np.sort(date_ranges[1, :])[3]]\n",
    "date_range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d9e767-3d97-4f32-a0a3-36c2c36b8920",
   "metadata": {},
   "source": [
    "Using the date range, we can now combine all of the datasets into a single `Xarray` Data set for easy computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c50291-2fc7-4996-852b-8e28822214c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(ds):\n",
    "    \"\"\"\n",
    "    Keep only the specified time range for each file.\n",
    "    \"\"\"\n",
    "    return ds.sel(time=slice(date_range[0], date_range[1]))\n",
    "\n",
    "ds = xr.open_mfdataset(files, engine='netcdf4', preprocess=preprocess, combine='nested', concat_dim='dataset_name')\n",
    "ds = ds.assign_coords({'dataset_name': dataset_name})\n",
    "ds.dataset_name.attrs['description'] = 'Dataset name'\n",
    "\n",
    "# Need time as first index for TC computation\n",
    "ds = ds.transpose('time', ...)\n",
    "# The data set is less than 1GiB, so let's read it into memory vs keeping as a dask array\n",
    "ds.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3501d7d-950f-4e60-9332-c3cd4dc657dd",
   "metadata": {},
   "source": [
    "## Time Series Exploration\n",
    "In case we want to explore the time series of each pixel later, let's make an interactive figure where we can select the latitude and longitude and plot the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f7f805-15dc-44f4-9041-46357b42005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_timseries(lat=40, lon=-90):\n",
    "    map = ds.aet.isel(time=0, dataset_name=0)\n",
    "    map = map * 0\n",
    "    map.loc[dict(lat=lat, lon=lon)] = 1\n",
    "    plt = map.hvplot(geo=True, coastline=True, clim=(0, 1), title='Time Series Location  (Red dot indicates current pixel)', colorbar=False, cmap='kr')\n",
    "    plt = plt + ds.aet.sel(lat=lat, lon=lon, method='nearest').hvplot(groupby='dataset_name',\n",
    "                                                                      title=\"Datasets' ET Time Series\").overlay().opts(legend_position='right')\n",
    "\n",
    "    return plt\n",
    "\n",
    "lat_widget = pn.widgets.FloatSlider(name='lat', start=ds.lat.min().item(), end=ds.lat.max().item(), step=0.25, value=ds.lat.min().item())\n",
    "lon_widget = pn.widgets.FloatSlider(name='lon', start=ds.lon.min().item(), end=ds.lon.max().item(), step=0.25, value=ds.lon.min().item())\n",
    "\n",
    "bound_plot = pn.bind(create_timseries, lat=lat_widget, lon=lon_widget)\n",
    "\n",
    "pn.Column(lat_widget, lon_widget, bound_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f235cab-dd47-4927-8ccc-d060a2381f28",
   "metadata": {},
   "source": [
    "## TC Estimation\n",
    "\n",
    "Time to compute the TC uncertainty estimates. To do that, we first need to decide on datasets that have \"independent\" errors in order to group them together into TC sets.\n",
    "\n",
    "Here is a table of the data and method used for calculating each ET data set:\n",
    "\n",
    "| Dataset | SSEBop | GLEAM v3b | ERA5 | NLDAS | TerraClimate | WBET |\n",
    "| ------  |  ----  | -----     | ---- | ----- | ----         | ---- |\n",
    "| Resolution | 1 km | 0.25 deg | 0.1 deg (9 km) | 0.125 deg | 0.04166 deg | 800 m |\n",
    "| Measurement System | Satellite | Satellite | Reanalysis | Land Surface Model | Satellite + Water Balance | Water Balance + Satellite |\n",
    "| Calculation Method | \"hot\" /\"cold\" reference pixels | Priestley and Taylor equation; Gash's alaytical model; Soil moisture based | Reanalysis | Land Surface Model | Penman-Monteith equation + Thornthwaite-Mather WBM | Water Balance, Meteorlogical/Climate regression, ensemble averaging |\n",
    "| Input Data | **STRM** elevn; **PRISM** Ta; **MODIS** Ts, emissivity, albedo, and NDVI; **GDAS** ETo | **CERES** radiation; **TMPA** precip; **AIRS** Ta; **GLOBSNOW** snow-water equiv; **CCI** vegetation optical depth; **GLDAS** and **CCI** Soil moisture; **MODIS** GVCF (global vegetation continuous fields); **IGBP-DIS** soil properties; **CGLFRD** lightning flash rate for rainfall inference | **CHTESSEL** Land surface model using model cycle Cy45r1 (2018) | **NARR** (North American Regional Reanalysis) atmospheric forcing data; **PRISM** precip | **WorldClim** Ta, vapor, precip, solar radiation, wind (Uses **MODIS** Ts, cloud cover; **STRM** elevn); **CRU** Ts4.0, Tmax, Tmin, vapor, precip, Ta; **JRA-55** Ta, vapor, precip, radiation, wind | **PRISM** precip, mean Ta, max Ta, min Ta; **USGS** water use irrigaion, national elevation dataset, NWIS gage II discharge; **EROS** land cover (1938-1999); **Landsat** NLCD land cover (2000-2018); **gridMT** wind; **Koppen-Geiger** climate classification; **Fenneman & Johnson** physiographic province classification; **EPA** level III ecoregions; **STATSGO2** soil saturated hydraulic conductivity, porosity, field capacity, thickness, available water capacity |\n",
    "\n",
    "From this table, we can group the datasets into measurement systems that \"should\" be independent:\n",
    "\n",
    "1) Satellite\n",
    "2) Reanalysis\n",
    "3) Land Surface Model (LSM)\n",
    "4) Water Balance\n",
    "\n",
    "Since TerraClimate and WBET are both mixed water balance and satellite datasets, we will keep treat them as just water balance for now. This give us 12 different possible combinations of datasets. However, since the computation is fast and resulting TC error estimates will be small in memory (~500kiB), we will just compute all 20 combinations and can filter them out later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ddf1e9-ddfc-4a57-a08a-06767d9f7f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of the combinations\n",
    "combos = list(itertools.combinations(dataset_abrv, 3))\n",
    "combos = [list(combo) for combo in combos]\n",
    "combos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adec689-7c6b-42a6-9c5e-fd361cd520b3",
   "metadata": {},
   "source": [
    "Since we have datasets with different date ranges, we will need to trim the date ranges here before computing the TC error variances. This will be slightly complicated. So, let's make it the date range selection its own function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82c2479-0bca-451f-bfa1-d81aca05c85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_date_range(ds, combo):\n",
    "    \"\"\"Return the common date slice of the datasets.\"\"\"\n",
    "    old_common_date = []\n",
    "    recent_common_date = []\n",
    "    for abrv in combo:\n",
    "        idx = [j for j in range(len(ds['dataset_name'])) if abrv == ds['dataset_name'][j]][0]\n",
    "        old_common_date.append(date_ranges[0, idx])\n",
    "        recent_common_date.append(date_ranges[1, idx])\n",
    "    \n",
    "    return slice(np.max(old_common_date), np.min(recent_common_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f1bdec-224f-4bbb-963c-5597675b786d",
   "metadata": {},
   "source": [
    "Now that we have the ability to select the common date range, let's compute the TC error standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68004cc6-3c96-4b16-9503-4b5c13ed312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to ignore all of the sqrt and log warnings with negative values\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# Override the name to the abbreviation for easier indexing\n",
    "ds['dataset_name'] = dataset_abrv\n",
    "\n",
    "tc_est = []\n",
    "ndates_per_combo = []\n",
    "for combo in combos:\n",
    "    ds_combo = ds.sel(time=common_date_range(ds, combo), dataset_name=combo)\n",
    "    ndates_per_combo.append(len(ds_combo.time))\n",
    "    \n",
    "    tc_covar, snr_temp = ec_covar_multi(ds_combo.aet.data, corr_sets=[1, 2, 3])\n",
    "\n",
    "    tc_est.append(xr.Dataset(data_vars={'error': (['dataset_combo', 'combo_idx', 'lat', 'lon'], \n",
    "                                                  np.sqrt(np.diagonal(tc_covar)).transpose((2, 0, 1))[None, ...]),\n",
    "                                        'snr': (['dataset_combo', 'combo_idx', 'lat', 'lon'], (10 ** np.log10(snr_temp[None, ...])))},\n",
    "                             coords={'dataset_combo': [''.join(combo)], 'combo_idx': [0, 1, 2], 'lat': ds.lat, 'lon': ds.lon}))\n",
    "\n",
    "tc_est = xr.concat(tc_est, dim='dataset_combo')\n",
    "\n",
    "tc_est.error.attrs['description'] = 'TC error estimate for the dataset_combo triplet.'\n",
    "tc_est.snr.attrs['description'] = 'TC unbiased SNR estimate for the dataset_combo triplet.'\n",
    "tc_est.dataset_combo.attrs['description'] = ('Dataset combination used in TC evaluation '\n",
    "                                             '(abbriviations: T=TerraClimate, E=ERA5, '\n",
    "                                             'N=NLDAS, G=GLEAM, W=WBET, S=SSEBop).')\n",
    "tc_est.combo_idx.attrs['description'] = 'String index of \"dataset_combo\" coordinate associated with the dataset.'\n",
    "tc_est.error.attrs['units'] = 'mm.month-1'\n",
    "tc_est = tc_est.compute()\n",
    "\n",
    "# Reset the name back from the abbreviation\n",
    "ds['dataset_name'] = dataset_name\n",
    "\n",
    "tc_est"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242842d8-0674-49ec-be7f-de8971ba240c",
   "metadata": {},
   "source": [
    "Let's see how the resulting error estimates look. As a reminder the data set abbreviation are:\n",
    "**T=TerraClimate, E=ERA5, N=NLDAS, G=GLEAM, W=WBET, S=SSEBop**. Additionally, we can look at the unbiased SNR estimates that we generated as well.\n",
    "\n",
    "Additionally, since we suppressed the `sqrt` and `log` run time warnings, we can expect to see `NaN`s throughout the maps, where the TC calculation resulted in negative values. This works as intended as any negative error variances should be flagged as incorrect (i.e., this is done with `NaN`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d0054b-a755-4a4c-9eae-79765d55fe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = tc_est.error.hvplot(groupby=['dataset_combo', 'combo_idx'], geo=True, \n",
    "                          coastline=True, clim=(0, 50)).opts(frame_width=500) + \\\n",
    "tc_est.snr.hvplot(groupby=['dataset_combo', 'combo_idx'], geo=True,\n",
    "                                    coastline=True, clim=(0.1, 50), cnorm='log').opts(frame_width=500)\n",
    "\n",
    "import panel as pn\n",
    "pn.panel(plt, widget_location='top')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6c82e1-fb6a-4543-9304-8262f97d14b0",
   "metadata": {},
   "source": [
    "## TC Discussion\n",
    "\n",
    "Some of the datasets (Mainly GLEAM and ERA5, and some NLDAS and WBET) have large swaths of `NaN` values caused by negative variances. This is typically caused by one of two things. (1) Covariances in the errors, which are assumed to not be present, or (2) two datasets have approximately order of magnitude larger error variances compared to the third.\n",
    "\n",
    "Let's begin exploring this by checking what combinations of datasets regularly result in large `NaN` regions. To do this, we can simply check the fraction of finite values produced by the TC estimate by the number of finite values in the original data. If certain data set combinations typically result in small fraction, it is likely that these datasets have correlated errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e60ad3-df43-4a9a-a61c-5cfef56d56f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfinite = np.isfinite(ds.aet.sel(time='2010-07')).sum(dim=['lat', 'lon']).compute()\n",
    "nNaN = np.isnan(ds.aet.sel(time='2010-07')).sum(dim=['lat', 'lon']).compute()\n",
    "nfinite['dataset_name'] = dataset_abrv\n",
    "nNaN['dataset_name'] = dataset_abrv\n",
    "\n",
    "for combo in tc_est.dataset_combo.data:\n",
    "    temp = tc_est.error.sel(dataset_combo=combo)\n",
    "    tc_nfinite = np.isfinite(temp).sum(dim=['lat', 'lon'])\n",
    "\n",
    "    expand_combo = [i for i in combo]\n",
    "    min_ref_finite = nfinite.sel(dataset_name=expand_combo).min().data\n",
    "\n",
    "    # This was printed and manually sorted below for investigative purposes.\n",
    "    # print(expand_combo, tc_nfinite.data/min_ref_finite)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "27aaab9a-b23b-406c-a459-ee40155c7979",
   "metadata": {},
   "source": [
    "['S', 'G', 'E'] [0.96143136 0.63842645 0.94383554]  G       G good 4, bad 0\n",
    "['S', 'G', 'N'] [0.95076723 0.92641744 0.80609041]  N       E good 4, bad 0\n",
    "['S', 'G', 'T'] [0.97002192 0.47852361 0.97446531]  G       N good 4, bad 0\n",
    "['S', 'G', 'W'] [0.96669107 0.95351391 0.94407028]          T good 4, bad 0\n",
    "['S', 'E', 'N'] [0.95118194 0.95580307 0.7040109 ]  N       W good 4, bad 0\n",
    "['S', 'E', 'T'] [0.98986907 0.54878844 0.97162154]  E\n",
    "['S', 'E', 'W'] [0.94824305 0.9806735  0.86969253]  W?\n",
    "['S', 'N', 'T'] [0.96101665 0.61413591 0.96865928]  N\n",
    "['S', 'N', 'W'] [0.97093704 0.89224012 0.9420205 ]  N?\n",
    "['S', 'T', 'W'] [0.97306003 0.99260615 0.60395315]  W\n",
    "\n",
    "['G', 'S', 'E'] [0.63842645 0.96143136 0.94383554]  X       S good 2, bad 2\n",
    "['G', 'S', 'N'] [0.92641744 0.95076723 0.80609041]  N       E good 1, bad 3\n",
    "['G', 'S', 'T'] [0.47852361 0.97002192 0.97446531]  X       N good 2, bad 2\n",
    "['G', 'S', 'W'] [0.95351391 0.96669107 0.94407028]          T good 1, bad 3\n",
    "['G', 'E', 'N'] [0.73910836 0.96643648 0.96455773]  X       W good 2, bad 2\n",
    "['G', 'E', 'T'] [0.93840032 0.79475169 0.98972499]  E\n",
    "['G', 'E', 'W'] [0.58265007 0.92789165 0.96398243]  X\n",
    "['G', 'N', 'T'] [0.59454656 0.94978166 0.97948614]  X\n",
    "['G', 'N', 'W'] [0.93235725 0.8295754  0.96786237]  N\n",
    "['G', 'T', 'W'] [0.71654466 0.97562225 0.96756955]  X\n",
    "\n",
    "['E', 'S', 'G'] [0.94383554 0.96143136 0.63842645]  G       S good 3, bad 1\n",
    "['E', 'S', 'N'] [0.95580307 0.95118194 0.7040109 ]  N       G good 3, bad 1\n",
    "['E', 'S', 'T'] [0.54878844 0.98986907 0.97162154]  X       N good 3, bad 1\n",
    "['E', 'S', 'W'] [0.9806735  0.94824305 0.86969253]  W?      T good 0, bad 4\n",
    "['E', 'G', 'N'] [0.96643648 0.73910836 0.96455773]  G       W good 3, bad 1\n",
    "['E', 'G', 'T'] [0.79475169 0.93840032 0.98972499]  X\n",
    "['E', 'G', 'W'] [0.92789165 0.58265007 0.96398243]  G\n",
    "['E', 'N', 'T'] [0.69361227 0.97146339 0.98893064]  X\n",
    "['E', 'N', 'W'] [0.96559297 0.67715959 0.95651537]  N\n",
    "['E', 'T', 'W'] [0.84685212 0.97759883 0.95929722]  X\n",
    "\n",
    "['N', 'S', 'G'] [0.80609041 0.95076723 0.92641744]  X       S good 0, bad 4\n",
    "['N', 'S', 'E'] [0.7040109  0.95118194 0.95580307]  X       G good 2, bad 2\n",
    "['N', 'S', 'T'] [0.61413591 0.96101665 0.96865928]  X       E good 2, bad 2\n",
    "['N', 'S', 'W'] [0.89224012 0.97093704 0.9420205 ]  X?      T good 2, bad 2\n",
    "['N', 'G', 'E'] [0.96455773 0.73910836 0.96643648]  G       W good 0, bad 4\n",
    "['N', 'G', 'T'] [0.94978166 0.59454656 0.97948614]  G\n",
    "['N', 'G', 'W'] [0.8295754  0.93235725 0.96786237]  X\n",
    "['N', 'E', 'T'] [0.97146339 0.69361227 0.98893064]  E\n",
    "['N', 'E', 'W'] [0.67715959 0.96559297 0.95651537]  X\n",
    "['N', 'T', 'W'] [0.84824305 0.97752562 0.80717423]  W, X\n",
    "\n",
    "['T', 'S', 'G'] [0.97446531 0.97002192 0.47852361]  G       S good 4, bad 0\n",
    "['T', 'S', 'E'] [0.97162154 0.98986907 0.54878844]  E       G good 4, bad 0 \n",
    "['T', 'S', 'N'] [0.96865928 0.96101665 0.61413591]  N       E good 4, bad 0 \n",
    "['T', 'S', 'W'] [0.99260615 0.97306003 0.60395315]  W       N good 4, bad 0 \n",
    "['T', 'G', 'E'] [0.98972499 0.93840032 0.79475169]  E       W good 4, bad 0 \n",
    "['T', 'G', 'N'] [0.97948614 0.59454656 0.94978166]  G\n",
    "['T', 'G', 'W'] [0.97562225 0.71654466 0.96756955]  G\n",
    "['T', 'E', 'N'] [0.98893064 0.69361227 0.97146339]  E\n",
    "['T', 'E', 'W'] [0.97759883 0.84685212 0.95929722]  E\n",
    "['T', 'N', 'W'] [0.97752562 0.84824305 0.80717423]  W, N\n",
    "\n",
    "['W', 'S', 'G'] [0.94407028 0.96669107 0.95351391]          S good 2, bad 2\n",
    "['W', 'S', 'E'] [0.86969253 0.94824305 0.9806735 ]  X?      G good 4, bad 0\n",
    "['W', 'S', 'N'] [0.9420205  0.97093704 0.89224012]  N?      E good 3, bad 1\n",
    "['W', 'S', 'T'] [0.60395315 0.97306003 0.99260615]  X       N good 3, bad 1\n",
    "['W', 'G', 'E'] [0.96398243 0.58265007 0.92789165]  G       T good 2, bad 2\n",
    "['W', 'G', 'N'] [0.96786237 0.93235725 0.8295754 ]  N\n",
    "['W', 'G', 'T'] [0.96756955 0.71654466 0.97562225]  G\n",
    "['W', 'E', 'N'] [0.95651537 0.96559297 0.67715959]  N\n",
    "['W', 'E', 'T'] [0.95929722 0.84685212 0.97759883]  E\n",
    "['W', 'N', 'T'] [0.80717423 0.84824305 0.97752562]  X, N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6168c746-3d07-41df-bcd4-512a29aa1aa4",
   "metadata": {},
   "source": [
    "From this fraction comparison, we can see for sure that GLEAM does not pair well with ERA5 or TerraClimate, ERA5 does not pair well with TerraClimate, and NLDAS does not pair well with SSEBop or WBET. (WBET bad pairings are slightly more ambiguous. So, we will leave them for now.) This means that GLEAM, ERA5, and TerraClimate fail when any are combined, and NDLAS fail when combined with SSEBop or WBET. Therefore, the only viable combinations are SSEBop-GLEAM-WBET, SSEBop-ERA5-WBET, and SSEBop-TerraClimate-WBET.\n",
    "\n",
    "Let's explore the failing combinations some by seeing how the covariances of the data compare. This may clarify these poor pairings. In the plots below, starting at the top left and moving left to right, top to bottom, the plots show each dataset's variance and covariance between each data set. If we see that the covariance (variance) of a datasets is significantly larger (smaller) than the covariance (variance) of the other datasets, we can expect there to potentially be negative error variance estimates. This is due to how the error variance is calculated:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_{\\varepsilon_i}^2 = \\sigma_{i}^2 - \\frac{\\sigma_{ij} \\sigma_{ik}}{\\sigma_{jk}},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\sigma_{ij} = {\\rm Cov}(\\boldsymbol{X}_i, \\boldsymbol{X}_j)$, and $\\boldsymbol{X_i}$ is data set $i$ from the data set triplet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba5e9b-273f-4650-9643-5c431a10cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "einsum_subscripts = 'abde,acde->bcde'\n",
    "\n",
    "def cov_plots(dataset_combo='SGE'):\n",
    "\n",
    "    ds['dataset_name'] = dataset_abrv\n",
    "    combo = [i for i in dataset_combo]\n",
    "    ds_combo = ds.sel(time=common_date_range(ds, combo), dataset_name=combo)\n",
    "    deviation = ds_combo.aet - ds_combo.aet.sum(axis=0) / ds_combo.aet.shape[0]\n",
    "    covar = np.einsum(einsum_subscripts, deviation, deviation)  / (ds_combo.aet.shape[0] - 1)\n",
    "    covar_ds = xr.DataArray(data=covar, coords={'dataset1': combo, 'dataset2': combo, 'lat': ds.lat, 'lon': ds.lon})\n",
    "\n",
    "    plt = covar_ds.isel(dataset1=0, dataset2=0).hvplot(geo=True, coastline=True).opts(frame_width=400)\n",
    "    plt = plt + covar_ds.isel(dataset1=1, dataset2=1).hvplot(geo=True, coastline=True).opts(frame_width=400)\n",
    "    plt = plt + covar_ds.isel(dataset1=2, dataset2=2).hvplot(geo=True, coastline=True).opts(frame_width=400)\n",
    "    plt = plt + covar_ds.isel(dataset1=0, dataset2=1).hvplot(geo=True, coastline=True).opts(frame_width=400)\n",
    "    plt = plt + covar_ds.isel(dataset1=0, dataset2=2).hvplot(geo=True, coastline=True).opts(frame_width=400)\n",
    "    plt = plt + covar_ds.isel(dataset1=1, dataset2=2).hvplot(geo=True, coastline=True).opts(frame_width=400)\n",
    "\n",
    "    ds['dataset_name'] = dataset_name\n",
    "    \n",
    "    return plt.cols(2)\n",
    "\n",
    "dataset_combo_widget = pn.widgets.Select(name=\"dataset_combo\", value=\"SGE\", options=list(tc_est.dataset_combo.values))\n",
    "\n",
    "bound_plot = pn.bind(cov_plots, dataset_combo=dataset_combo_widget)\n",
    "\n",
    "pn.Row(bound_plot, dataset_combo_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1fc3b0-5569-46ef-aac9-23a1153a505e",
   "metadata": {},
   "source": [
    "As expected, this does seem to be the case. While determining the issue in the data is doable (relative variance/covariance issues), translating that into what TC assumption is not met is a more challenging problem. To remind ourselves of these assumptions, they are:\n",
    "\n",
    "1. The signal and random errors are stationary (i.e., the mean of each is constant with time).\n",
    "2. All data sets are represent exactly the same ET state (i.e., the three data sets have the same spatial resolution and sampling intervals).\n",
    "3. No cross-correlation of errors (i.e., measurement system errors are independent of each other).\n",
    "4. Error orthogonality (i.e., the measurement system errors are independent of the true value).\n",
    "5. No error autocorrelation (i.e., the error estimates are not correlated with time).\n",
    "\n",
    "Of these assumptions, it is possible that each is influencing the result. First, it is likely that our signal and random errors are not stationary. We have performed some stationarity tests off-hand and found that the signal is not always stationary (this is expected since ET has likely changed with time). Additionally, the error likely has a non-stationary seasonal component. As discussed in [Gruber et al. (2016)](http://dx.doi.org/10.1016/j.jag.2015.09.002), this is not an issue if the datasets all have the same non-stationarity effect. However, determining this is difficult and likely not the major contributing factor to the error variance.\n",
    "\n",
    "Next, representativeness can bias one or two of the error variances in the triplet if the spatial representativeness is highly different between the data sets [(Gruber et al. 2016)](http://dx.doi.org/10.1016/j.jag.2015.09.002). In our case, the data sets all originally had their own native resolution, which we degraded to match the GLEAM resolution. Therefore, it is possible that data sets with high native resolution may be penalizing the data sets with lower resolution. Of the six data sets, SSEBop and WBET were similar in resolution, with TerraClimate close to their resolution as well. ERA5 and NLDAS are both also similar in resolution, with them being almost double GLEAM and 10x lower than SSEBop and WBET.\n",
    "\n",
    "| Dataset | SSEBop | GLEAM v3b | ERA5 | NLDAS | TerraClimate | WBET |\n",
    "| ------  |  ----  | -----     | ---- | ----- | ----         | ---- |\n",
    "| Resolution | 0.01 deg (1 km) | 0.25 deg (22.5 km) | 0.1 deg (9 km) | 0.125 deg (11.25 km) | 0.04166 deg (3.75 km) | 0.009 deg (800 m) |\n",
    "\n",
    "While this does help us understand why combinations of SSEBop and WBET resulted in better TC error variance estimates, it does not give a reason why the lower resolution data sets were leading to poor results. Mainly, we should keep this issue in mind as we further investigate expanding TC to extended Collocation (EC).\n",
    "\n",
    "Finally, the largest issue in our error variance estimates is likely the inclusion of datasets with cross-correlated errors. Meeting this assumption has been shown to be the most influential in getting correct error variance estimates, more than error orthogonality and error autocorrelation [(Yilmax & Crow 2014)](https://doi.org/10.1175/JHM-D-13-0158.1). From a basic assumption standpoint, as discussed above, we would have expected that SSEBop and GLEAM would likely have issues along TerraClimate and WBET, as they are generated from similar measurement systems. Additionally, we would expect ERA5 and NLDAS to be relatively independent of the others, since they are unique measurement systems. However, this is almost the opposite situation, as they seem to have the most problems. This could be that their forcing data, which we did not expand to its individual part, is the same data used in other measurement systems. Therefore, this TC application demonstrates that using basic TC is likely not a reliable measure of error variances, as knowing what data sets are truly independent is almost and insurmountable task. However, we can expand TC to EC and see if coupling data sets results in different error variance estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cb6a5c-125c-402e-8a76-2d425a801761",
   "metadata": {},
   "source": [
    "Combine each error standard deviations estimate to find the mean and standard deviation for each computation, excluding the `NaN`s. First rearrange the data to be by data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84031d5-3e41-4827-9ff4-b90868d7c74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_est_by_dataset = []\n",
    "for abrv, name in zip(dataset_abrv, dataset_name):\n",
    "    idx_loc = np.char.find(tc_est.dataset_combo.data, abrv)\n",
    "    dataset_loc = np.where(idx_loc != -1)[0]\n",
    "    idx_loc = idx_loc[dataset_loc]\n",
    "    combos_single_dataset = tc_est.isel(dataset_combo=dataset_loc)\n",
    "    tc_est_dataset = []\n",
    "    for i in range(len(combos_single_dataset.dataset_combo.values)):\n",
    "        tc_est_single_dataset = combos_single_dataset.isel(dataset_combo=i, combo_idx=idx_loc[i])\n",
    "        tc_est_single_dataset = tc_est_single_dataset.drop_vars(['dataset_combo', 'combo_idx'])\n",
    "\n",
    "        tc_est_dataset.append(xr.Dataset(data_vars=tc_est_single_dataset,\n",
    "                                coords={'dataset_name': name, 'est_idx': i, 'lat': ds.lat, 'lon': ds.lon}))\n",
    "    \n",
    "    tc_est_dataset = xr.concat(tc_est_dataset, dim='est_idx')\n",
    "    \n",
    "    tc_est_by_dataset.append(tc_est_dataset)\n",
    "\n",
    "tc_est_by_dataset = xr.concat(tc_est_by_dataset, dim='dataset_name')\n",
    "\n",
    "tc_est_by_dataset.dataset_name.attrs['description'] = 'Dataset names.'\n",
    "tc_est_by_dataset.est_idx.attrs['description'] = 'Index of the TC triplet set that the estimate was calculated.'\n",
    "\n",
    "tc_est_by_dataset = tc_est_by_dataset.compute().chunk(-1)\n",
    "tc_est_by_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2258da-1572-41ea-903c-84012102a71d",
   "metadata": {},
   "source": [
    "Now let's compute and plot the means and standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99861940-d810-44cc-aca8-dfbb0f20c463",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tc_est = tc_est_by_dataset.error.mean(dim='est_idx', skipna=True, keep_attrs=True)\n",
    "std_tc_est = tc_est_by_dataset.error.std(dim='est_idx', ddof=1, skipna=True, keep_attrs=True)\n",
    "count_tc_est = np.isfinite(tc_est_by_dataset.error).sum(dim='est_idx')\n",
    "count_tc_est.attrs['units'] = 'counts'\n",
    "\n",
    "plt = mean_tc_est.hvplot\n",
    "plt = mean_tc_est.hvplot(groupby=['dataset_name'], geo=True, coastline=True, \n",
    "                         clim=(0,50), title='Mean Error').opts(frame_width=500) + \\\n",
    "      std_tc_est.hvplot(groupby=['dataset_name'], geo=True, coastline=True,\n",
    "                        title='Std of Error').opts(frame_width=500) + \\\n",
    "      count_tc_est.hvplot(groupby=['dataset_name'], geo=True, coastline=True,\n",
    "                          title='Number of data points used in calculation').opts(frame_width=500)\n",
    "\n",
    "import panel as pn\n",
    "pn.panel(plt.cols(2), widget_location='top')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "et_tc",
   "language": "python",
   "name": "et_tc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
