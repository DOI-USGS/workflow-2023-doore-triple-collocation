{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdf842e6-d027-4526-8452-99f49681bb7f",
   "metadata": {},
   "source": [
    "# Compile ET Monthly Datsets\n",
    "\n",
    "Below are the Python codes needed to download and process six ET datasets with monthly time steps. On my Dell Precision 3570 with a 12th Gen Intel i7-1255U 1.70 GHz processor and a 250Mb/s internet connection, each of these datasets takes less than 15 minutes to download and process, except for the WBET dataset. This dataset can take ~24 hours to fully process due to the high resolution and large file sizes. While the other datasets use at most 1GB of disk space each, the WBET dataset will use ~100GB at peak useage and ~50GB for the final processed file. This can be reduced if the full date range is not utilized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb6d5bd-5838-4aee-aa39-cf3441c3bc25",
   "metadata": {},
   "source": [
    "## TerraClimate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32511002-51c9-474a-9fa6-418bacdf109b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TerraClimate Dataset\n",
    "import fsspec\n",
    "import xarray as xr\n",
    "import os\n",
    "import hvplot.xarray\n",
    "\n",
    "if not os.path.isdir('terraclimate'): os.mkdir('terraclimate')\n",
    "\n",
    "# If compiled netcdf is not made from downloads, make it\n",
    "if not os.path.isfile('terraclimate/TerraClimate_AET.nc'):\n",
    "    fs = fsspec.filesystem('https', timeout=3600)\n",
    "    url = 'https://climate.northwestknowledge.net/TERRACLIMATE-DATA/'\n",
    "    \n",
    "    years = range(1958, 2023)\n",
    "    \n",
    "    # Download all of the individual year files. Do this recursively as fs may timeout if\n",
    "    #   a full list of files is called at once\n",
    "    paths = []\n",
    "    for year in years:\n",
    "        file = f\"TerraClimate_aet_{year}.nc\"\n",
    "        # Create full URL\n",
    "        paths.append(url +file)\n",
    "        \n",
    "    fs.get(paths, 'terraclimate/')\n",
    "\n",
    "    # Open the files and combine\n",
    "    ds = xr.open_mfdataset(['terraclimate/'+f\"TerraClimate_aet_{year}.nc\" for year in years], \n",
    "                           engine='netcdf4', chunks={'lat': 116, 'lon': 288, 'time': -1})\n",
    "    \n",
    "    # Only keep CONUS range of data\n",
    "    ds = ds.sel(lat=slice(53, 24))\n",
    "    ds = ds.sel(lon=slice(-126, -66))\n",
    "\n",
    "    # Rechunk to appropriate size\n",
    "    ds = ds.chunk({'lat': 116, 'lon': 288, 'time': -1})\n",
    "\n",
    "    # Replace unicode characters in summary (degree symbol)\n",
    "    ds.attrs['summary'] = ds.attrs['summary'].replace(ds.attrs['summary'][64:66], ' deg')\n",
    "\n",
    "    # Update aet units to include time span\n",
    "    ds['aet'].attrs['units'] = 'mm.month-1'\n",
    "    ds['aet'].attrs['long_name'] = 'Total Actual Evapotranspiration'\n",
    "\n",
    "    ds['time'].attrs['unit'] = 'month'    \n",
    "    ds['time'].attrs['description'] = 'Monthly time step indicated by the first day of the month.'    \n",
    "\n",
    "    # Save xarray dataset to netcdf\n",
    "    _ = ds.to_netcdf(path='terraclimate/TerraClimate_AET.nc', engine='netcdf4', format='NETCDF4')\n",
    "\n",
    "    # Remove downloaded files to reduce storage, as the data is now in the combined netcdf\n",
    "    for year in years: os.remove('terraclimate/'+f\"TerraClimate_aet_{year}.nc\")\n",
    "    \n",
    "ds = xr.open_dataset('terraclimate/TerraClimate_AET.nc', engine='netcdf4', chunks={'lat': 116, 'lon': 288, 'time': -1})\n",
    "ds\n",
    "#ds.aet.hvplot(groupby='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314b3c0f-047f-4a50-8f0e-36d859b5ea04",
   "metadata": {},
   "source": [
    "## ERA-5 Dataset\n",
    "\n",
    "> NOTE: You will need a [Climate Data Store](https://cds.climate.copernicus.eu/cdsapp) (CDS) [account](https://cds.climate.copernicus.eu/user/register) to access this data. Once you have an account, make sure to [configure `cdsapi`](https://github.com/ecmwf/cdsapi#configure) for the download to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e960d505-bdf8-49cd-960d-f8ab8462c273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERA-5 Dataset\n",
    "import xarray as xr\n",
    "import cdsapi\n",
    "import os\n",
    "import zipfile\n",
    "import hvplot.xarray\n",
    "\n",
    "c = cdsapi.Client()\n",
    "\n",
    "if not os.path.isdir('era5'): os.mkdir('era5')\n",
    "\n",
    "if not os.path.isfile('era5/ERA5_AET.nc'):\n",
    "    # This command is generated from Climate Data Store\n",
    "    #   (https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-land-monthly-means?tab=overview)\n",
    "    c.retrieve(\n",
    "        'reanalysis-era5-land-monthly-means',\n",
    "        {\n",
    "            'product_type': 'monthly_averaged_reanalysis',\n",
    "            'variable': 'total_evaporation',\n",
    "            'year': [\n",
    "                '1950', '1951', '1952', '1953', '1954', '1955', '1956', '1957', '1958', '1959',\n",
    "                '1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968', '1969',\n",
    "                '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979',\n",
    "                '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989',\n",
    "                '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999',\n",
    "                '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009',\n",
    "                '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019',\n",
    "                '2020', '2021', '2022',\n",
    "            ],\n",
    "            'month': [\n",
    "                '01', '02', '03',\n",
    "                '04', '05', '06',\n",
    "                '07', '08', '09',\n",
    "                '10', '11', '12',\n",
    "            ],\n",
    "            'time': '00:00',\n",
    "            'format': 'netcdf.zip',\n",
    "            'area': [\n",
    "            53, -126, 24,\n",
    "            -66,\n",
    "            ],\n",
    "        },\n",
    "        'era5/ERA5_AET.netcdf.zip')\n",
    "\n",
    "    # Extract the contents of zip file and remove zip file\n",
    "    zipfile.ZipFile('era5/ERA5_AET.netcdf.zip').extract('data.nc', 'era5/')\n",
    "    os.remove('era5/ERA5_AET.netcdf.zip')\n",
    "\n",
    "    # Data values are negative to indicate inverse of precipitation (see docs) and in meters.\n",
    "    #   We want to switch to positive values and mm, along with setting any then negative values to 0.\n",
    "    #   Additionally, from the documentation, the monthly means have units that include \"per day\". We want \"per month\".\n",
    "    #   So, we need to multiply each month by the number of days in it.\n",
    "    ds = xr.open_dataset('era5/data.nc', engine='netcdf4', chunks={'longitude': 601, 'latitude': 97, 'time': -1})\n",
    "    ds = -1e3 * ds\n",
    "    ds = ds.where(~(ds < 0), 0)\n",
    "    ds = ds * ds.get_index('time').days_in_month.values.reshape(len(ds.get_index('time')), 1, 1)\n",
    "\n",
    "    # Add new metadata attributes\n",
    "    ds['e'].attrs['units'] = 'mm.month-1'\n",
    "    ds['e'].attrs['description'] = \"Accumulated amount of water that has evaporated from the Earth's surface, \"+ \\\n",
    "                                   'including a simplified representation of transpiration (from vegetation), '+ \\\n",
    "                                   'into vapour in the air above.'\n",
    "    ds['e'].attrs['long_name'] = 'Total Evaporation'\n",
    "    ds['e'].attrs['dimensions'] = 'lon lat time'\n",
    "\n",
    "    # Rename the coordinates to a common format and add some metadata attributes\n",
    "    ds = ds.rename({'longitude': 'lon','latitude': 'lat', 'e': 'aet'})\n",
    "    ds['lat'].attrs['description'] = 'Latitude of the center of the grid cell'\n",
    "    ds['lat'].attrs['standard_name'] = 'latitude'\n",
    "    ds['lat'].attrs['axis'] = 'Y'\n",
    "\n",
    "    ds['lon'].attrs['description'] = 'Longitude of the center of the grid cell'\n",
    "    ds['lon'].attrs['standard_name'] = 'longitude'\n",
    "    ds['lon'].attrs['axis'] = 'X'\n",
    "\n",
    "    ds['time'].attrs['standard_name'] = 'time'\n",
    "    ds['time'].attrs['unit'] = 'month'    \n",
    "    ds['time'].attrs['axis'] = 'T'\n",
    "    ds['time'].attrs['description'] = 'Monthly time step indicated by the first day of the month.'    \n",
    "\n",
    "    # Chunk and save\n",
    "    ds = ds.chunk({'lon': 601, 'lat': 97, 'time': -1})\n",
    "    ds.to_netcdf(path='era5/ERA5_AET.nc', engine='netcdf4', format='NETCDF4')\n",
    "\n",
    "    os.remove('era5/data.nc')\n",
    "\n",
    "ds = xr.open_dataset('era5/ERA5_AET.nc', engine='netcdf4', chunks={'lon': 601, 'lat': 97, 'time': -1})\n",
    "ds\n",
    "#ds.aet.hvplot(groupby='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45e04df-a49b-470a-b530-1a30c417ff7a",
   "metadata": {},
   "source": [
    "## NLDAS Dataset\n",
    "\n",
    "> NOTE: You will need a [EarthData Login](https://wiki.earthdata.nasa.gov/display/EL/How+To+Register+For+an+EarthData+Login+Profile) to access this data. Once you have an login, make sure to [link the login to the NASA GESDISC Data Archive](https://disc.gsfc.nasa.gov/earthdata-login) for the download to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6000bf-f519-4037-b76e-7e828c5bbc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLDAS Dataset\n",
    "import fsspec\n",
    "import xarray as xr\n",
    "import aiohttp\n",
    "import os\n",
    "import hvplot.xarray\n",
    "\n",
    "if not os.path.isdir('nldas'): os.mkdir('nldas')\n",
    "\n",
    "# If compiled netcdf is not made from downloads, make it\n",
    "if not os.path.isfile('nldas/NLDAS_AET.nc'):\n",
    "\n",
    "    # Requires an account to access the data\n",
    "    # Username and Password are given as OS environmental variables (NASA_EARTHDATA_USERNAME and NASAS_EARTHDATA_PASSWORD)\n",
    "    fs = fsspec.filesystem('https', timeout=3600, client_kwargs={'auth': aiohttp.BasicAuth(os.environ['NASA_EARTHDATA_USERNAME'],\n",
    "                                                                                           password=os.environ['NASA_EARTHDATA_PASSWORD'])})\n",
    "    base_url = 'https://data.gesdisc.earthdata.nasa.gov/data/NLDAS/NLDAS_NOAH0125_M.2.0/'\n",
    "    \n",
    "    months = range(1, 13)\n",
    "    years = range(1979, 2023)\n",
    "    \n",
    "    # Make list of paths\n",
    "    paths = []\n",
    "    first_iter = True\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            # Create full URL\n",
    "            filepath = f\"{year}/NLDAS_NOAH0125_M.A{year}\"+str(month).zfill(2)+'.020.nc'\n",
    "            paths.append(base_url + filepath)\n",
    "\n",
    "    fs.get(paths, 'nldas/')\n",
    "\n",
    "    # Open first year to get list of variables to drop\n",
    "    ds = xr.open_dataset('nldas/NLDAS_NOAH0125_M.A197901.020.nc', engine='netcdf4', chunks={})\n",
    "    drop_vars = [var for var in list(ds.data_vars) if var != 'Evap']\n",
    "                        \n",
    "    # Open all files and combine. Use one chunk as file is only 200MB total\n",
    "    ds = xr.open_mfdataset(['nldas/'+f\"NLDAS_NOAH0125_M.A{year}\"+str(month).zfill(2)+'.020.nc' for year in years for month in months],\n",
    "                           drop_variables=drop_vars, engine='netcdf4', chunks={'lat': -1, 'lon': -1, 'time': -1})\n",
    "\n",
    "    # Drop January of 1979 as it starts on the 2nd. See NLDAS docs for details.\n",
    "    ds = ds.where(ds.time != ds.time[0], drop=True)\n",
    "\n",
    "    # Rename variable and coords to common names and add attributes\n",
    "    ds = ds.rename({'Evap': 'aet'})\n",
    "    # Units are in kg.m-2.month-1, which is equivalent to mm.month-1 assuming a water density of 1g.cm-3\n",
    "    #   (mm = kg.m-2 / g.cm-3 * 1e3g.kg-1 * 1e-6m3.cm-3 * 1e3mm.m-1) \n",
    "    ds['aet'].attrs['units'] = 'mm.month-1'\n",
    "    ds['aet'].attrs['description'] = 'Actual Total Evapotranspiration'\n",
    "    ds['aet'].attrs['dimensions'] = 'lon lat time'\n",
    "\n",
    "    # Add some metadata attributes\n",
    "    ds['lat'].attrs['description'] = 'Latitude of the center of the grid cell'\n",
    "    ds['lat'].attrs['axis'] = 'Y'\n",
    "\n",
    "    ds['lon'].attrs['description'] = 'Longitude of the center of the grid cell'\n",
    "    ds['lon'].attrs['axis'] = 'X'\n",
    "\n",
    "    ds['time'].attrs['standard_name'] = 'time'\n",
    "    ds['time'].attrs['unit'] = 'month'    \n",
    "    ds['time'].attrs['axis'] = 'T'\n",
    "    ds['time'].attrs['description'] = 'Monthly time step indicated by the first day of the month.'\n",
    "    del ds['time'].attrs['begin_date']\n",
    "    del ds['time'].attrs['begin_time']\n",
    "    del ds['time'].attrs['end_date']\n",
    "    del ds['time'].attrs['end_time']\n",
    "    del ds['time'].attrs['bounds']\n",
    "   \n",
    "    # Save dataset to netcdf\n",
    "    ds.to_netcdf(path='nldas/NLDAS_AET.nc', format='NETCDF4', engine='netcdf4')\n",
    "    \n",
    "    # Remove downloaded files to reduce storage, as the data is now in the combined netcdf\n",
    "    for year in years:\n",
    "        for month in months: os.remove('nldas/'+f\"NLDAS_NOAH0125_M.A{year}\"+str(month).zfill(2)+'.020.nc')\n",
    "\n",
    "ds = xr.open_dataset('nldas/NLDAS_AET.nc', engine='netcdf4', chunks={'lat': 224, 'lon': 232, 'time': -1})\n",
    "ds\n",
    "#ds.aet.hvplot(groupby='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9584dd08-4d51-4b0c-b602-54e8e3c2e42c",
   "metadata": {},
   "source": [
    "## GLEAM Dataset\n",
    "\n",
    "> NOTE: You will need a [GLEAM Login](https://www.gleam.eu/#downloads) to access this data. Once you have an login, you can use it to access the sftp site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c3c5dc-6f84-4e60-8b98-52ca7c6b2611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLEAM v3.7b Dataset\n",
    "import fsspec\n",
    "import xarray as xr\n",
    "import os\n",
    "import pandas as pd\n",
    "import hvplot.xarray\n",
    "\n",
    "sftp_host = 'sftp://hydras.ugent.be'\n",
    "gleam_creds_sftp = dict(\n",
    "    username = os.environ[\"GLEAM_USERNAME\"],\n",
    "    password = os.environ[\"GLEAM_PASSWORD\"],\n",
    "    port = int(os.environ[\"GLEAM_PORT\"])\n",
    "    )\n",
    "\n",
    "if not os.path.isdir('gleam'): os.mkdir('gleam')\n",
    "\n",
    "# Download the GLEAM data\n",
    "if not os.path.isfile('gleam/GLEAM_AET.nc'):\n",
    "    # host input excludes the sftp prefix\n",
    "    fs = fsspec.filesystem('sftp', host=sftp_host[7:], **gleam_creds_sftp)\n",
    "\n",
    "    filepath = \"/data/v3.7b/monthly/E_2003-2022_GLEAM_v3.7b_MO.nc\"\n",
    "    path = sftp_host + filepath\n",
    "\n",
    "    fs.get(path, 'gleam/')\n",
    "    \n",
    "    # Open the file\n",
    "    ds = xr.open_dataset(\"gleam/E_2003-2022_GLEAM_v3.7b_MO.nc\", engine='netcdf4', chunks={'lon': 480, 'lat': 240, 'time': -1})\n",
    "\n",
    "    # Only keep CONUS range of data and rechunk\n",
    "    ds = ds.sel(lat=slice(53, 24))\n",
    "    ds = ds.sel(lon=slice(-126, -66))\n",
    "    ds = ds.chunk({'lat': -1, 'lon': -1, 'time': -1})\n",
    "\n",
    "    # Adjust month dates to be first of month rather than end of month for consistency with other datasets\n",
    "    ds = ds.reindex({'time': ds.get_index('time').shift(periods=-1, freq='MS')}, method='backfill')\n",
    "\n",
    "    # Rename variable to common name and add new metadata attributes\n",
    "    ds = ds.rename({'E': 'aet'})\n",
    "    ds['aet'].attrs['description'] = 'Actual total evaporation from GLEAM 3.7b'\n",
    "    ds['aet'].attrs['long_name'] = 'Actual evaporation'\n",
    "    ds['aet'].attrs['dimensions'] = 'lon lat time'\n",
    "\n",
    "    # Add some coordinate metadata attributes\n",
    "    ds['lat'].attrs['units'] = 'degrees_north'\n",
    "    ds['lat'].attrs['description'] = 'Latitude of the center of the grid cell'\n",
    "    ds['lat'].attrs['long_name'] = 'latitude'\n",
    "    ds['lat'].attrs['standard_name'] = 'latitude'\n",
    "    ds['lat'].attrs['axis'] = 'Y'\n",
    "  \n",
    "    ds['lon'].attrs['units'] = 'degrees_east'\n",
    "    ds['lon'].attrs['description'] = 'Longitude of the center of the grid cell'\n",
    "    ds['lon'].attrs['long_name'] = 'longitude'\n",
    "    ds['lon'].attrs['standard_name'] = 'longitude'\n",
    "    ds['lon'].attrs['axis'] = 'X'\n",
    "\n",
    "    ds['time'].attrs['long_name'] = 'time'\n",
    "    ds['time'].attrs['standard_name'] = 'time'\n",
    "    ds['time'].attrs['unit'] = 'month'    \n",
    "    ds['time'].attrs['description'] = 'Monthly time step indicated by the first day of the month.'\n",
    "    ds['time'].attrs['axis'] = 'T'\n",
    "\n",
    "    ds.to_netcdf(path='gleam/GLEAM_AET.nc', format='NETCDF4', engine='netcdf4')\n",
    "\n",
    "    # Remove downloaded file to reduce storage, as the data is now in the new netcdf\n",
    "    os.remove(\"gleam/E_2003-2022_GLEAM_v3.7b_MO.nc\")\n",
    "    \n",
    "ds = xr.open_dataset('gleam/GLEAM_AET.nc', engine='netcdf4', chunks={'lat': 116, 'lon': 288, 'time': -1})\n",
    "ds\n",
    "#ds.aet.hvplot(groupby='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b9e451-e65a-40bf-90f5-45ee24b497cb",
   "metadata": {},
   "source": [
    "## WBET (Rietz et al. 2023) Dataset\n",
    "\n",
    "> NOTE: You will need a [SciencBase Account](https://www.sciencebase.gov/directory/newUser/create) to download this data via the Python code. If you don't have an account, you will have to manually download [the data](https://www.sciencebase.gov/catalog/item/64135576d34eb496d1ce3d2e)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e278d8-6858-4e11-aa5e-61ba2c8f77d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sciencebasepy\n",
    "import os\n",
    "import re\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import zipfile_deflate64 as zipfile\n",
    "import io\n",
    "import hvplot.xarray\n",
    "\n",
    "if not os.path.isdir('wbet'): os.mkdir('wbet')\n",
    "\n",
    "if not os.path.isfile('wbet/WBET_AET.nc'):\n",
    "    # Establish a session.\n",
    "    sb = sciencebasepy.SbSession()\n",
    "    \n",
    "    # Login required to access cloud files via sciencebasepy\n",
    "    sb.login(os.environ[\"SCIENCEBASE_USERNAME\"], os.environ[\"SCIENCEBASE_PASSWORD\"])\n",
    "        \n",
    "    # Get list of files for monthly ET data\n",
    "    file_list = sb.get_item_file_info(sb.get_item('64135576d34eb496d1ce3d2e'))\n",
    "    filenames = [i['name'] for i in file_list if re.search('ET.*_monthly.zip', i['name']) is not None]\n",
    "\n",
    "    # Download the files (these files are big (6GB a piece), so this will take a while...)\n",
    "    _ = sb.download_cloud_files(filenames, sb.generate_S3_download_links('64135576d34eb496d1ce3d2e', filenames), 'wbet')\n",
    "\n",
    "    # Open the GeoTIFF files to xarray\n",
    "    ds_monthly_list = []\n",
    "    for zippedfiles in filenames:\n",
    "\n",
    "        # Access zip file without unzipping\n",
    "        zfile = zipfile.ZipFile('wbet/' + zippedfiles)\n",
    "        zip_file_list = zfile.namelist()\n",
    "        \n",
    "        # Select the GeoTIFF files from zip_file_list and extract\n",
    "        gtif_files = [file for file in zip_file_list if re.search('.*(\\.tif)$', file) is not None]\n",
    "        zfile.extractall('wbet/', gtif_files)\n",
    "\n",
    "        # Delete downloaded file to save disk space, since files are now extracted\n",
    "        os.remove('wbet/'+zippedfiles)\n",
    "       \n",
    "        for gtif in gtif_files:\n",
    "            # Read in each extracted GeoTIFF\n",
    "            ds_month = rioxarray.open_rasterio('wbet/'+gtif, chunks={}, band_as_variable=True)\n",
    "            \n",
    "            # Remove spatial_ref coord and rename coords to corresponding names. Assign the date to the Dataset\n",
    "            # ds_month = ds_month.reset_coords('spatial_ref', drop=True)\n",
    "            # 5th-8th characters of file indicate year, 10th-11th indicate month (index start at 0, characters at 1)\n",
    "            year, month = gtif[4:8], gtif[9:11]\n",
    "            date = year+'-'+month\n",
    "            ds_month = ds_month.rename({'x': 'lon','y': 'lat', 'band_1': 'aet'}).assign_coords(time=pd.to_datetime(date)).expand_dims(dim=\"time\")\n",
    "            ds_month = ds_month.drop_vars('spatial_ref')\n",
    "\n",
    "            # Stack the monthly Datasets to list for concatenating\n",
    "            ds_monthly_list.append(ds_month)\n",
    "            ds_month.close()\n",
    "\n",
    "        # Concatenate and save to netcdf\n",
    "        ds = xr.concat(ds_monthly_list, dim='time')\n",
    "        ds.to_netcdf(path='wbet/'+zippedfiles[:-4]+'.nc', format='NETCDF4', engine='netcdf4', \n",
    "                     encoding={'aet':{'zlib': True, 'complevel': 4}})\n",
    "\n",
    "        # Delete extracted files to save disk space, since files are now compiled to netcdf\n",
    "        for gtif in gtif_files: os.remove('wbet/'+gtif)\n",
    "\n",
    "        # Reset variables\n",
    "        del ds\n",
    "        del ds_monthly_list\n",
    "        ds_monthly_list = []\n",
    "\n",
    "    # Open processed decade files as single dataset\n",
    "    ds = xr.open_mfdataset(['wbet/'+file[:-4]+'.nc' for file in filenames], engine='netcdf4', \n",
    "                           chunks={'lat': 69, 'lon': 281, 'time': -1})\n",
    "\n",
    "    #   From the metadata xml file, the monthly data are in units of mm.day-1. We want mm.month-1.\n",
    "    #   So, we need to multiply each month by the number of days in it.\n",
    "    ds = ds * ds.get_index('time').days_in_month.values.reshape(len(ds.get_index('time')), 1, 1)\n",
    "    \n",
    "    # Add new metadata attributes to variable and coordinates\n",
    "    ds['aet'].attrs['unit'] = 'mm.month-1'\n",
    "    ds['aet'].attrs['description'] = 'Actual Total Evapotranspiration via WBET from Reitz+2023'\n",
    "    ds['aet'].attrs['long_name'] = 'Actual Evapotranspiration'\n",
    "    ds['aet'].attrs['standard_name'] = 'Actual Evapotranspiration'\n",
    "    ds['aet'].attrs['dimensions'] = 'lon lat time'\n",
    "\n",
    "    ds['lat'].attrs['units'] = 'degrees_north'\n",
    "    ds['lat'].attrs['description'] = 'Latitude of the center of the grid cell'\n",
    "    ds['lat'].attrs['long_name'] = 'latitude'\n",
    "    ds['lat'].attrs['standard_name'] = 'latitude'\n",
    "    ds['lat'].attrs['axis'] = 'Y'\n",
    "  \n",
    "    ds['lon'].attrs['units'] = 'degrees_east'\n",
    "    ds['lon'].attrs['description'] = 'Longitude of the center of the grid cell'\n",
    "    ds['lon'].attrs['long_name'] = 'longitude'\n",
    "    ds['lon'].attrs['standard_name'] = 'longitude'\n",
    "    ds['lon'].attrs['axis'] = 'X'\n",
    "\n",
    "    ds['time'].attrs['long_name'] = 'time'\n",
    "    ds['time'].attrs['standard_name'] = 'time'\n",
    "    ds['time'].attrs['unit'] = 'month'    \n",
    "    ds['time'].attrs['description'] = 'Monthly time step indicated by the first day of the month.'\n",
    "    ds['time'].attrs['axis'] = 'T'\n",
    "\n",
    "    # Save dataset and remove processed files\n",
    "    ds.to_netcdf(path='wbet/WBET_AET.nc', format='NETCDF4', engine='netcdf4', \n",
    "                     encoding={'aet':{'zlib': True, 'complevel': 9}})\n",
    "\n",
    "    for file in filenames: os.remove('wbet/'+file[:-4]+'.nc')\n",
    "\n",
    "\n",
    "# Open the saved netcdf\n",
    "ds = xr.open_dataset('wbet/WBET_AET.nc', engine='netcdf4', chunks={'lat': 69, 'lon': 281, 'time': -1})\n",
    "ds\n",
    "#ds.aet.sel(time='2010-08', method='nearest').hvplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa99f2e-2bd3-47e9-ba2e-729dcf01bb81",
   "metadata": {},
   "source": [
    "## SSEBop MODIS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9be8ec-a883-4274-9aa4-d3815b88a855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSEBop Dataset\n",
    "import fsspec\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "import hvplot.xarray\n",
    "import numpy as np\n",
    "\n",
    "fs = fsspec.filesystem('https', timeout=3600)\n",
    "url = 'https://edcintl.cr.usgs.gov/downloads/sciweb1/shared/uswem/web/conus/eta/modis_eta/monthly/downloads/'\n",
    "\n",
    "dates = pd.date_range('2000-01-01', '2022-12-31', freq='MS')\n",
    "\n",
    "# Make a directory for holding and extracting the zip files\n",
    "if not os.path.isdir('ssebop'): os.mkdir('ssebop')\n",
    "\n",
    "if not os.path.isfile('ssebop/SSEBop_AET.nc'):\n",
    "    # Read the files to xarray and compile to monthly data\n",
    "    for date in dates:\n",
    "        # Download the zipfiles\n",
    "        # (Do this individually vs all files in a list as server may disconnect downloading for list)\n",
    "        fs.get(url + 'm' + date.strftime('%Y%m') + '.zip', 'ssebop/')\n",
    "        \n",
    "        # Access zip file without unzipping\n",
    "        zfile = zipfile.ZipFile('ssebop/m' + date.strftime('%Y%m') + '.zip')\n",
    "\n",
    "        # Read in GeoTIFF\n",
    "        ds_monthly = rioxarray.open_rasterio(io.BytesIO(zfile.read('m' + date.strftime('%Y%m') + '.modisSSEBopETactual.tif')),\n",
    "                                             chunks={}, band_as_variable=True)\n",
    "\n",
    "        # Rename coords to corresponding names. Assign the date to the Dataset\n",
    "        ds_monthly = ds_monthly.rename({'x': 'lon','y': 'lat', 'band_1': 'aet'}).assign_coords(time=date).expand_dims(dim=\"time\")\n",
    "\n",
    "        # Save monthly compiled dataset\n",
    "        ds_monthly.to_netcdf(path='ssebop/'+date.strftime('%Y-%m')+'.modisSSEBopETactual.nc', engine='netcdf4')\n",
    "        \n",
    "        # Delete downloaded zipfile as we no longer need it\n",
    "        os.remove('ssebop/m' + date.strftime('%Y%m') + '.zip')\n",
    "\n",
    "            \n",
    "    # Read in monthly netcdf Datasets into full Dataset and chunk\n",
    "    #   Dates after 2016-07 have less indices values than those before. Read these date groups in seperately, slice,\n",
    "    #   and align for concatenating into single file. Additionally after 2016-07, bodies of water have a fill value\n",
    "    #   of 0 rather than NaN. Replace the 0 fill with NaN using a mask from before 2016-07 as 0 can be present on\n",
    "    #   land surface in winter.\n",
    "    pre_201608 = pd.date_range('2001-01-01', '2016-7-31', freq='MS')\n",
    "    ds1 = xr.open_mfdataset(['ssebop/'+date+'.modisSSEBopETactual.nc' for date in pre_201608.strftime('%Y-%m')], engine='netcdf4')\n",
    "    \n",
    "    post_201608 = pd.date_range('2016-08-01', '2022-12-31', freq='MS')\n",
    "    ds2 = xr.open_mfdataset(['ssebop/'+date+'.modisSSEBopETactual.nc' for date in post_201608.strftime('%Y-%m')], engine='netcdf4')\n",
    "\n",
    "    # reindex lat and lon to match, values are off by floating point rounding errors\n",
    "    # Match lat/lon range\n",
    "    ds1 = ds1.sel(lon=ds2['lon'], lat=ds2['lat'], method='nearest', tolerance=1e-10)\n",
    "    ds1, ds2 = xr.align(ds1, ds2, join='override', exclude='time')\n",
    "\n",
    "    # Concatenate, remove spatial_ref var, and chunk\n",
    "    ds = xr.concat([ds1, ds2], dim='time')\n",
    "    ds = ds.drop_vars('spatial_ref')\n",
    "    ds = ds.chunk({'lon': 348, 'lat': 218, 'time': -1})\n",
    "\n",
    "    # Set 0 fill values to NaNs\n",
    "    ds = ds.where(~np.isnan(ds.aet.isel(time=0)))\n",
    "\n",
    "    # Add new metadata attributes\n",
    "    ds['aet'].attrs['description'] = 'Actual evaporation from SSEBop MODIS, monthly total'\n",
    "    ds['aet'].attrs['dimensions'] = 'lon lat time'\n",
    "    ds['aet'].attrs['standard_name'] = 'Actual evaporation'\n",
    "    ds['aet'].attrs['long_name'] = 'Actual evaporation'\n",
    "    ds['aet'].attrs['units'] = 'mm.month-1'\n",
    "\n",
    "    # Add some coordinate metadata attributes\n",
    "    ds['lat'].attrs['units'] = 'degrees_north'\n",
    "    ds['lat'].attrs['description'] = 'Latitude of the center of the grid cell'\n",
    "    ds['lat'].attrs['long_name'] = 'latitude'\n",
    "    ds['lat'].attrs['standard_name'] = 'latitude'\n",
    "    ds['lat'].attrs['axis'] = 'Y'\n",
    "  \n",
    "    ds['lon'].attrs['units'] = 'degrees_east'\n",
    "    ds['lon'].attrs['description'] = 'Longitude of the center of the grid cell'\n",
    "    ds['lon'].attrs['long_name'] = 'longitude'\n",
    "    ds['lon'].attrs['standard_name'] = 'longitude'\n",
    "    ds['lon'].attrs['axis'] = 'X'\n",
    "\n",
    "    ds['time'].attrs['long_name'] = 'time'\n",
    "    ds['time'].attrs['standard_name'] = 'time'\n",
    "    ds['time'].attrs['description'] = 'Monthly time step indicated by the first day of the month.'\n",
    "    ds['time'].attrs['unit'] = 'month'    \n",
    "    ds['time'].attrs['axis'] = 'T'\n",
    "\n",
    "\n",
    "    # Save full Dataset\n",
    "    ds.to_netcdf(path='ssebop/SSEBop_AET.nc', format='NETCDF4', engine='netcdf4', encoding={'aet':{'zlib': True, 'complevel': 4}})\n",
    "\n",
    "    # Remove intermediate monthly files\n",
    "    for date in dates.strftime('%Y-%m'): os.remove('ssebop/'+date+'.modisSSEBopETactual.nc')\n",
    "\n",
    "ds = xr.open_dataset('ssebop/SSEBop_AET.nc', engine='netcdf4', chunks={'lon': 348, 'lat': 218, 'time': -1})\n",
    "ds\n",
    "#ds.aet.hvplot(groupby='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b498bca-ec0d-4ea8-80b3-b393f47b2461",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "et_tc",
   "language": "python",
   "name": "et_tc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
