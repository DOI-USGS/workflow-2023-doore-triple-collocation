{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "618cbb1c-7d2a-45d0-b251-21e273b6e6bd",
   "metadata": {},
   "source": [
    "# Extended Collocation Uncertainty Analysis\n",
    "\n",
    "Since we saw in the [Triple Collocation (TC) notebook](2_TC_application.ipynb#TC-Discussion) that the estimated ET error variances using TC potentially had error cross-correlations, we will look into how expanding TC to include additional data sets can help estimate this cross-correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40648d66-c4e6-4dbc-ada8-5f49d2e9e28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "import hvplot.xarray\n",
    "import panel as pn\n",
    "import cartopy.crs as ccrs\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from xarray_einstats import linalg\n",
    "from scipy.stats import percentileofscore\n",
    "import itertools\n",
    "import warnings\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d13faa3-f172-4434-9230-d945784d0aba",
   "metadata": {},
   "source": [
    "First, we will run in the Extended Collocation (EC) notebook to create our EC function. (See [the notebook](../TC/EC_function.ipynb) for details on the EC method.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5c04ff-5224-44f9-b9c5-52c05fe0ee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../TC/EC_function.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4500288f-aad3-43b3-80db-02da36a70beb",
   "metadata": {},
   "source": [
    "## Combine Data Sets in Xarray\n",
    "Next, we need to load in our data sets and limit them to a common date range. Since we need at least four data sets to utilized EC, we will restrict the data ranges of all data sets to have the beginning date of the fourth oldest starting date and ending data of the fourth most recent ending date. This choice allows us to save memory usage, while also utilizing the largest amount of data. For data sets with a more restricted date range, due to one data set having a smaller date range, we will limit the date range further at the time of the EC computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0068f110-21d0-42f9-b9be-61b2a199b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['../Data/ssebop/ssebop_aet_regridded.nc',\n",
    "         '../Data/gleam/gleam_aet.nc',\n",
    "         '../Data/era5/era5_aet_regridded.nc',\n",
    "         '../Data/nldas/nldas_aet_regridded.nc',\n",
    "         '../Data/terraclimate/terraclimate_aet_regridded.nc',        \n",
    "         '../Data/wbet/wbet_aet_regridded.nc',\n",
    "         ]\n",
    "dataset_name = ['SSEBop', 'GLEAM', 'ERA5', 'NLDAS', 'TerraClimate', 'WBET']\n",
    "\n",
    "date_ranges = {}\n",
    "for file, name in zip(files, dataset_name):\n",
    "    ds_temp = xr.open_dataset(file, engine='netcdf4', chunks={'lon': -1, 'lat': -1, 'time': -1})\n",
    "    date_ranges[name] = [ds_temp.time.min().values, ds_temp.time.max().values]\n",
    "\n",
    "# Take the third oldest start and third most recent end dates\n",
    "date_range = [np.sort(np.array(list(date_ranges.values()))[:, 0])[3],\n",
    "              np.sort(np.array(list(date_ranges.values()))[:, 1])[-4]]\n",
    "date_range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e0ad14-7c6b-4627-a446-88d44b75eaf1",
   "metadata": {},
   "source": [
    "Using the date range, we can now combine all of the data sets into a single `xarray.DataSet` for easy computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0109e5e5-0caa-47ae-95d7-5093f4f8f810",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(ds):\n",
    "    \"\"\"\n",
    "    Keep only the specified time range for each file.\n",
    "    \"\"\"\n",
    "    return ds.sel(time=slice(date_range[0], date_range[1]))\n",
    "\n",
    "ds = xr.open_mfdataset(files, engine='netcdf4', preprocess=preprocess, combine='nested', concat_dim='dataset_name')\n",
    "ds = ds.assign_coords({'dataset_name': dataset_name})\n",
    "ds.dataset_name.attrs['description'] = 'Dataset name'\n",
    "\n",
    "# Need time as first index for TC computation\n",
    "ds = ds.transpose('time', ...)\n",
    "# The data set is less than 1GiB, so let's read it into memory vs keeping as a dask array\n",
    "ds = ds.compute()\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f5b54e-b47d-45bf-8a91-266a473c4f37",
   "metadata": {},
   "source": [
    "## EC Estimation\n",
    "\n",
    "As stated above, since we have data sets with different date ranges, we will need to trim the date ranges here before computing the EC error covariance matrix. This will be slightly complicated. So, let's make it the date range selection its own function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3b6d3f-7634-472e-aa01-982580af435c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_date_range(ds, combo):\n",
    "    \"\"\"Return the common date slice of the datasets.\"\"\"\n",
    "    old_common_date = []\n",
    "    recent_common_date = []\n",
    "    for name in combo:\n",
    "        old_common_date.append(date_ranges[name][0])\n",
    "        recent_common_date.append(date_ranges[name][1])\n",
    "    \n",
    "    return slice(np.max(old_common_date), np.min(recent_common_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d907417-9f53-44d9-a273-4921643dea0b",
   "metadata": {},
   "source": [
    "Like the TC error variance estimates in the [TC notebook](2_TC_application.ipynb#TC-Estimation), we will compute the error covariance matrix for all 90 possible combinations of four data sets (i.e., Quadruple Collocation or QC), since the computation is fast. Additionally, since we only want the subset of the error covariance matrix corresponding to the correlated data sets (i.e., the two data sets with non-zero covariances) and not the whole covariance matrix, the resulting subset of the EC covariance matrices will be reasonably small in memory (~375MiB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d2e2c7-d56f-46c7-8a72-232db2c12bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of the combinations, need two correlated data sets, then two additional ones\n",
    "combos = list(itertools.combinations(dataset_name, 2))\n",
    "combos = [list(corr_combo + indep_combo) \n",
    "          for corr_combo in combos \n",
    "          for indep_combo in combos \n",
    "          if ((corr_combo[0] not in indep_combo) and (corr_combo[1] not in indep_combo))]\n",
    "combos[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6042136-aeb1-4326-ba8b-6a76fff4bf78",
   "metadata": {},
   "source": [
    "Now that we have our data set combinations, let's compute the EC error covariance matrices and extract the subset. We will do this for each season independently along with the full year. Additionally, we will normalize the subset of the covariance matrices to get the error cross-correlation matrices. (The season and full year will be denoted with the monthly abbreviations contained within the season or `All`, respectively.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1d9dc9-a778-4eec-b181-17d8bcdf8d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to ignore all of the sqrt warnings with negative values\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# Create list of seasons\n",
    "seasons = ['All'] + list(np.unique(ds.time.dt.season))\n",
    "\n",
    "ec_covar_est = []\n",
    "ec_covar_est_season = []\n",
    "\n",
    "\n",
    "for combo in combos:\n",
    "    for season in seasons:\n",
    "        if season == 'All':\n",
    "            ds_season = ds\n",
    "        else:\n",
    "            ds_season = ds.isel(time=(ds.time.dt.season == season))\n",
    "\n",
    "        ds_combo = ds_season.sel(time=common_date_range(ds, combo), dataset_name=combo)\n",
    "        \n",
    "        ec_covar = ec_covar_multi(ds_combo.aet.data, corr_sets=[1, 1, 2, 3])\n",
    "        \n",
    "        # Extract the subset for our correlated pair (i.e., the first two indices)\n",
    "        ec_covar = ec_covar[0:2, 0:2, ...]\n",
    "\n",
    "        # We will want the covariance matrix to be symmetric. Therefore,\n",
    "        # average together the matrix and its transpose as sig_01 != sig10\n",
    "        # as discussed in our random example.\n",
    "        covar = (ec_covar + np.swapaxes(ec_covar, 0, 1)) / 2\n",
    "        # Compute the cross-correlation matrix\n",
    "        d = np.moveaxis(np.diagonal(covar), -1, 0)\n",
    "        rho = covar / np.sqrt(d[:, None, ...])\n",
    "        rho /= np.sqrt(d[None, :, ...])\n",
    "        # We only want the off diagonal as the diagonals will be 1\n",
    "        rho = np.diagonal(rho, 1).squeeze()\n",
    "       \n",
    "        ec_covar_est_season.append(xr.Dataset(data_vars={'covar': (['dataset_combo', 'season', 'dataset_idx_1', 'dataset_idx_2', 'lat', 'lon'],\n",
    "                                                                   covar[None, None, ...]),\n",
    "                                                         'rho': (['dataset_combo', 'season', 'lat', 'lon'],\n",
    "                                                                 rho[None, None, ...])},\n",
    "                                              coords={'dataset_combo': [' '.join(combo)], 'season': [season],\n",
    "                                                      'dataset_idx_1': [0, 1], 'dataset_idx_2': [0, 1], \n",
    "                                                      'lat': ds.lat, 'lon': ds.lon}))\n",
    "    ec_covar_est.append(xr.concat(ec_covar_est_season, dim='season'))\n",
    "    ec_covar_est_season = []\n",
    "\n",
    "ec_covar_est = xr.concat(ec_covar_est, dim='dataset_combo')\n",
    "\n",
    "# Convert layout to be by covariance pair vs long dataset_combo list\n",
    "covar_pairs = [' '.join(combo) for combo in list(itertools.combinations(dataset_name, 2))]\n",
    "covar_est_by_dataset_pair = []\n",
    "est_pair = []\n",
    "for covar_pair in covar_pairs:\n",
    "    idx_loc = np.char.startswith(ec_covar_est.dataset_combo.data, covar_pair)\n",
    "    dataset_loc = np.where(idx_loc)[0]\n",
    "    covar_pair_datasets = ec_covar_est.isel(dataset_combo=dataset_loc)\n",
    "\n",
    "    est_pair.append([combinations.replace(covar_pair, '').strip() for combinations in ec_covar_est.dataset_combo.data[dataset_loc]])\n",
    "\n",
    "    covar_est_by_dataset_pair.append(xr.Dataset(data_vars={'covar': (['est_idx', 'season', 'covar_pair_idx_1',\n",
    "                                                                      'covar_pair_idx_2', 'lat', 'lon'], \n",
    "                                                                     covar_pair_datasets.covar.data),\n",
    "                                                           'rho': (['est_idx', 'season', 'lat', 'lon'],\n",
    "                                                                     covar_pair_datasets.rho.data)},\n",
    "                                     coords={'covar_pair': covar_pair, 'est_idx': np.arange(len(dataset_loc)),\n",
    "                                             'season': seasons, 'covar_pair_idx_1': [0, 1], 'covar_pair_idx_2':[0, 1],\n",
    "                                             'lat': ec_covar_est.lat, 'lon': ec_covar_est.lon}))\n",
    "\n",
    "del ec_covar_est\n",
    "\n",
    "covar_est_by_dataset_pair = xr.concat(covar_est_by_dataset_pair, dim='covar_pair')\n",
    "\n",
    "covar_est_by_dataset_pair = covar_est_by_dataset_pair.assign_coords(est_pair=(['covar_pair', 'est_idx'], np.array(est_pair)))\n",
    "\n",
    "covar_est_by_dataset_pair.covar.attrs['description'] = 'EC error covariance matrix estimate for the data sets in covar_pair.'\n",
    "covar_est_by_dataset_pair.covar.attrs['units'] = 'mm2.month-2'\n",
    "covar_est_by_dataset_pair.covar_pair.attrs['description'] = 'Correlated data set pair used in EC evaluation.'\n",
    "covar_est_by_dataset_pair.covar_pair_idx_1.attrs['description'] = ('Index of the correlated data set in the covariance matrix '\n",
    "                                                                   'along the first dimesion as contained in covar_pair.')\n",
    "covar_est_by_dataset_pair.covar_pair_idx_2.attrs['description'] = ('Index of the correlated data set in the covariance matrix '\n",
    "                                                                   'along the second dimesion as contained in covar_pair.')\n",
    "covar_est_by_dataset_pair.est_idx.attrs['description'] = 'Index of the other two data sets used in the EC evaluation as contained in est_pair.'\n",
    "covar_est_by_dataset_pair.season.attrs['description'] = ('Season of the year given by the first letter of each month within '\n",
    "                                                         'the season. The full year is given by \"All\".')\n",
    "covar_est_by_dataset_pair.est_pair.attrs['description'] = 'Names of the other two data sets used in the EC evaluation.'\n",
    "\n",
    "covar_est_by_dataset_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9e1d12-7ad3-4512-8afc-416345d56aa7",
   "metadata": {},
   "source": [
    "Now, let's see how the resulting error covariance and cross-correlation estimates look. (We can simply extract them from one of the diagonals of the corresponding matrix.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e224fef-b97e-4bae-bf48-9cb4e463a209",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ec_plt(covar_pair='SSEBop GLEAM', est_idx=0, season='All'):\n",
    "\n",
    "    ec_data = covar_est_by_dataset_pair.sel(covar_pair=covar_pair, est_idx=est_idx, season=season,\n",
    "                                            covar_pair_idx_1=1, covar_pair_idx_2=0)\n",
    "    \n",
    "    est_pairs = str(covar_est_by_dataset_pair.est_pair.sel(covar_pair=covar_pair, est_idx=est_idx).data)\n",
    "    plt = (ec_data.covar.hvplot(geo=True, coastline=True, clim=(-500, 500), cmap='RdBu',\n",
    "                               title='Error Covariance (other EC datasets: '+est_pairs+')'\n",
    "                              ).opts(frame_width=500)\n",
    "           + ec_data.rho.hvplot(geo=True, coastline=True, clim=(-1, 1), cmap='RdBu',\n",
    "                                title='Error Cross-Correlation (other EC datasets: '+est_pairs+')'\n",
    "                               ).opts(frame_width=500))\n",
    "\n",
    "    return plt\n",
    "\n",
    "covar_pair_widget = pn.widgets.Select(name=\"covar_pair\", value=\"SSEBop GLEAM\", options=list(covar_est_by_dataset_pair.covar_pair.values))\n",
    "est_idx_widget = pn.widgets.IntSlider(name=\"est_idx\", start=0, end=5, step=1, value=0)\n",
    "season_widget = pn.widgets.Select(name=\"season\", value=\"All\", options=['All', 'DJF', 'MAM', 'JJA', 'SON'])\n",
    "\n",
    "bound_plot = pn.bind(ec_plt, covar_pair=covar_pair_widget, season=season_widget, est_idx=est_idx_widget)\n",
    "\n",
    "pn.Column(covar_pair_widget, est_idx_widget, season_widget, bound_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd11817-253f-449c-ad3e-7777090a88bd",
   "metadata": {},
   "source": [
    "## EC Discussion\n",
    "\n",
    "Looking at the error covariances, we can see a large variation in how the covariance estimates change with the other independent pairs. For example, the covariance of SSEBop and WBET seem to just change intensity across CONUS while keeping relatively similar spatial patterns, while the the covariance of GLEAM and ERA5 change both intensity and spatial patterns. Additionally, certain regions of each covariance map show constant positive covariances, while others show constant negative covariances. Only one data set pair shows constant net positive error covariances, ERA5 and TerraClimate. This likely indicates that these two data sets have errors that are indeed cross-correlated. Interestingly though, we also find some data sets that have constant net negative covariances, for example SSEBop and TerraClimate along with ERA5 and WBET. This is highly unexpected from an input data perspective as this seems to indicate that as the uncertainty in one data set increases, the other decreases. Typically, we would expect the data sets to have net positive covariances due to commonalities in the input data or the calculation method propagating the same errors into the data set. For regional negative covariances, this could be reasonable as one measurement system could be optimized for that geographic region, while the other struggles in that region. However, for SSEBop and TerraClimate, we are seeing almost the entire map as negative covariances. This is related to the fact that these two datasets have the largest error variances compared to the other four data sets, which is causing issues when estimating their covariances.\n",
    "\n",
    "As for the error cross-correlations, there are large swaths of `NaN` values in certain data set covariance pairs. This is due to the `NaN`s in the error variance estimates as discussed in [TC notebook](2_TC_application.ipynb#TC-Discussion), since the error cross-correlation is calculated as:\n",
    "\n",
    "$$\\rho_{\\varepsilon_i, \\varepsilon_l} = \\frac{\\sigma_{\\varepsilon_i, \\varepsilon_l}}{\\sqrt{\\sigma_{\\varepsilon_i}^2\\sigma_{\\varepsilon_l}^2}}$$\n",
    "\n",
    "where $\\rho_{\\varepsilon_i, \\varepsilon_l}$ is the error cross-correlation, $\\sigma_{\\varepsilon_i, \\varepsilon_l}$ is the error covariance, and $\\sigma_{\\varepsilon_i}^2$ and $\\sigma_{\\varepsilon_l}^2$ are the error variances. Again, this issue is related to SSEBop and TerraClimate having approximately an order of magnitude larger error variances compared to the other data sets. This causes negative error variances (and covariance) estimates, which in turn result in `NaN`s across the cross-correlation maps. Therefore, we will likely need to average the data set combinations to get a single map of the error covariances and cross-correlation for each covariance pair. Additionally, this will help with make looking at the covariance estimates less overwhelming as it will give one covariance estimate per covariance pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44318f9-84bc-4c9d-8f4b-f94db2884bee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_covar_est = covar_est_by_dataset_pair.covar.mean(dim='est_idx', skipna=True, keep_attrs=True)\n",
    "mean_covar_est.name = 'mean_covar'\n",
    "mean_covar_est.attrs['description'] = 'Mean EC error covariance estimate for all possible combinations with other datasets.'\n",
    "median_covar_est = covar_est_by_dataset_pair.covar.median(dim='est_idx', skipna=True, keep_attrs=True)\n",
    "median_covar_est.name = 'median_covar'\n",
    "median_covar_est.attrs['description'] = 'Median EC error covariance estimate for all possible combinations with other datasets.'\n",
    "std_covar_est = covar_est_by_dataset_pair.covar.std(dim='est_idx', ddof=1, skipna=True, keep_attrs=True)\n",
    "std_covar_est.name = 'std_of_covar_std'\n",
    "std_covar_est.attrs['description'] = ('Standard deviation of the EC error covariance estimate for all '\n",
    "                                      'possible combinations with other datasets.')\n",
    "\n",
    "mean_rho_est = covar_est_by_dataset_pair.rho.mean(dim='est_idx', skipna=True, keep_attrs=True)\n",
    "mean_rho_est.name = 'mean_rho'\n",
    "mean_rho_est.attrs['description'] = 'Mean EC error cross-correlation estimate for all possible combinations with other datasets.'\n",
    "median_rho_est = covar_est_by_dataset_pair.rho.median(dim='est_idx', skipna=True, keep_attrs=True)\n",
    "median_rho_est.name = 'median_rho'\n",
    "median_rho_est.attrs['description'] = 'Median EC error cross-correlation estimate for all possible combinations with other datasets.'\n",
    "std_rho_est = covar_est_by_dataset_pair.rho.std(dim='est_idx', ddof=1, skipna=True, keep_attrs=True)\n",
    "std_rho_est.name = 'std_of_rho_std'\n",
    "std_rho_est.attrs['description'] = ('Standard deviation of the EC error cross-correlation estimate for all '\n",
    "                                    'possible combinations with other datasets.')\n",
    "\n",
    "count_covar_est = np.isfinite(covar_est_by_dataset_pair.covar).sum(dim='est_idx')\n",
    "count_covar_est.name = 'counts'\n",
    "count_covar_est.attrs['description'] = ('Number of datasets used in the average EC error covariance '\n",
    "                                        'estimates (i.e., number of finite values in a given pixel).')\n",
    "count_covar_est.attrs['units'] = 'counts'\n",
    "\n",
    "# Compile these DataSets into one and save for use in notebook 4_Bias\n",
    "ec_est_averages = xr.merge([mean_covar_est, median_covar_est, std_covar_est, mean_rho_est,\n",
    "                            median_rho_est, std_rho_est, count_covar_est], join='exact')\n",
    "\n",
    "if not os.path.isfile('../Data/compiled_EC_avg_covar_errs.nc'):\n",
    "    _ = ec_est_averages.to_netcdf(path='../Data/compiled_EC_avg_covar_errs.nc', format='NETCDF4', engine='netcdf4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd8f276-6755-496f-8596-7bb32f8850df",
   "metadata": {},
   "source": [
    "Now that we have our median error covariance and cross-correlation estimates, let's plot them and see how they look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb0f2e4-4880-4c38-9c1f-fc5a67825bb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select the covariance element from the matrix\n",
    "ec_est_averages_co = ec_est_averages.sel(covar_pair_idx_1=1, covar_pair_idx_2=0)\n",
    "\n",
    "plt = ec_est_averages_co.median_covar.hvplot(\n",
    "        groupby=['covar_pair', 'season'], geo=True, coastline=True, cmap='RdBu',\n",
    "        clim=(-500,500), title='Median Error Covariance'\n",
    "      ).opts(frame_width=500) + \\\n",
    "      ec_est_averages_co.median_rho.hvplot(\n",
    "          groupby=['covar_pair', 'season'], geo=True, coastline=True, cmap='RdBu', \n",
    "          clim=(-1,1), title='Median Error Cross-Correlation'\n",
    "      ).opts(frame_width=500) + \\\n",
    "      np.abs(ec_est_averages_co.median_covar/ec_est_averages_co.std_of_covar_std).hvplot(\n",
    "          groupby=['covar_pair', 'season'], geo=True, coastline=True, cmap='RdBu',\n",
    "          clim=(0.01,100), title='SNR of Error Covariance', logz=True\n",
    "      ).opts(frame_width=500) + \\\n",
    "      np.abs(ec_est_averages_co.median_rho/ec_est_averages_co.std_of_rho_std).hvplot(\n",
    "          groupby=['covar_pair', 'season'], geo=True, coastline=True, cmap='RdBu',\n",
    "          clim=(0.01,100), title='SNR of Error Cross-Correlation', logz=True\n",
    "      ).opts(frame_width=500)\n",
    "\n",
    "pn.panel(plt.cols(2), widget_location='top')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5d36e0-5cb4-40eb-8928-4ccabedfe61a",
   "metadata": {},
   "source": [
    "From these plots, we can see that some data sets may be correlated with each other. To visualize this clearly, we can plot all of the pixels from each independent data set pair and average of pairs as histograms, where one count is a pixel. This will allow us to see if a whole map shows a net error covariance or if the covariances are evenly distributed around zero, which could indicate minimal cross-correlation of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b26e92-a7d7-4583-a593-0b8e79f07f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the covariance element from the matrix\n",
    "covar_est_by_dataset_pair = covar_est_by_dataset_pair.sel(covar_pair_idx_1=1, covar_pair_idx_2=0)\n",
    "\n",
    "def histogram_plts(covar_pair='SSEBop GLEAM', season='All'):\n",
    "    da_pair = []\n",
    "    for i in range(6):\n",
    "        da_pair.append(covar_est_by_dataset_pair.covar.sel(covar_pair=covar_pair, est_idx=i, season=season))\n",
    "        da_pair[i].name = covar_est_by_dataset_pair.est_pair.sel(covar_pair=covar_pair, est_idx=i).data.item()\n",
    "    da_mean = mean_covar_est.sel(covar_pair=covar_pair, season=season)\n",
    "    da_median = median_covar_est.sel(covar_pair=covar_pair, season=season)\n",
    "    da_mean.name = 'Mean'\n",
    "    da_median.name = 'Median'\n",
    "\n",
    "    plt = da_pair[0].hvplot.hist(bins=50, bin_range=(-500,500), title='EC Error Covariance Distribution of '+covar_pair, \n",
    "                           xlabel='Error Covariance (mm2.month-2)', ylabel='Counts', alpha=1, normed=True)\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        plt *= da_pair[i].hvplot.hist(bins=50, bin_range=(-500,500), alpha=1, normed=True)\n",
    "        \n",
    "    plt *= da_mean.hvplot.hist(bins=50, bin_range=(-500,500), alpha=1, normed=True)\n",
    "    plt *= da_median.hvplot.hist(bins=50, bin_range=(-500,500), alpha=1, normed=True)\n",
    "\n",
    "    return plt\n",
    "\n",
    "def percent_table(covar_pair='SSEBop GLEAM', season='All'):\n",
    "    da_pair = []\n",
    "    for i in range(6):\n",
    "        da_pair.append(covar_est_by_dataset_pair.covar.sel(covar_pair=covar_pair, est_idx=i, season=season))\n",
    "    da_mean = mean_covar_est.sel(covar_pair=covar_pair, season=season)\n",
    "    da_median = median_covar_est.sel(covar_pair=covar_pair, season=season)\n",
    "\n",
    "    percentiles = ([percentileofscore(da.data.flatten(), 0, kind='strict', \n",
    "                                     nan_policy='omit') for da in da_pair]\n",
    "                   + [percentileofscore(da_mean.data.flatten(), 0, kind='strict',  nan_policy='omit')]\n",
    "                   + [percentileofscore(da_median.data.flatten(), 0, kind='strict',  nan_policy='omit')])\n",
    "    correlation_str = np.array(['Neutral '] * len(percentiles))\n",
    "    correlation_str[np.array(percentiles) <= 35] = 'Positive'\n",
    "    correlation_str[np.array(percentiles) >= 65] = 'Negative'\n",
    "    table = hv.Table({'Independent Pair': list(covar_est_by_dataset_pair.est_pair.sel(covar_pair=covar_pair).data) + ['Mean', 'Median'], \n",
    "                      'Percentile of 0': np.round(percentiles, 2),\n",
    "                      'Net Correlation': correlation_str}, ['Independent Pair', 'Percentile of 0', 'Net Correlation']).opts(width=350)\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "covar_pair_widget = pn.widgets.Select(name=\"covar_pair\", value=\"SSEBop GLEAM\", options=list(covar_est_by_dataset_pair.covar_pair.values))\n",
    "season_widget = pn.widgets.Select(name=\"season\", value=\"All\", options=['All', 'DJF', 'MAM', 'JJA', 'SON'])\n",
    "\n",
    "bound_plot = pn.bind(histogram_plts, covar_pair=covar_pair_widget, season=season_widget)\n",
    "bound_table = pn.bind(percent_table, covar_pair=covar_pair_widget, season=season_widget)\n",
    "\n",
    "pn.Column(covar_pair_widget, season_widget, pn.Row(bound_plot, bound_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174e8c06-6b67-447c-8574-b6ca3611ed17",
   "metadata": {},
   "source": [
    "From these results, we can see that indeed some data sets do have net positive covariances. For example SSEBop and NLDAS, along with ERA-5 and TerraClimate, both have strong net positive covariances. This indicates that these data sets likely have some common modeling assumption or input data that are causing them to have correlated errors.\n",
    "\n",
    "Interestingly though, like we noticed above in the unaggregated maps, we also find some data sets that have net negative covariances, for example SSEBop and TerraClimate along with ERA-5 and WBET. As stated above This, while initially unexpected, likely indicates that one data set may have be optimized for a aspecific geographic region, while the other struggles in that region. However, for SSEBop and TerraClimate, we are seeing almost the entire map as negative covariances. This is related to the fact that these two datasets have the largest error variances compared to the other four data sets, which is causing issues when estimating their covariances."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "et_tc",
   "language": "python",
   "name": "et_tc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
