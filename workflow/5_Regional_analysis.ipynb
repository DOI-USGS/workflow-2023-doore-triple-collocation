{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0872207f-ce5f-4ff3-8769-68aa7a7ef895",
   "metadata": {},
   "source": [
    "# Regional Analysis\n",
    "\n",
    "In this notebook we will explore the results of the three previous notebooks in terms of specific regions in the US. We will look at the High Plains Aquifer, which has been shown to have accelerated decline in aquifer storage, the Central Valley, as it is another region that has suffered from declines in aquifer storage due to agricultural usage, and the Upper Colorado River Basin, as it is a critical basin for understanding the down stream flow of the Colorado River in the southwest US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abfb55c-25f4-4bd1-a49f-adb26cc71665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely import unary_union\n",
    "from shapely.geometry import Polygon\n",
    "from scipy.stats import kstest\n",
    "import holoviews as hv\n",
    "import hvplot.xarray\n",
    "import hvplot.pandas\n",
    "import panel as pn\n",
    "import cartopy.crs as ccrs\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import sciencebasepy\n",
    "import os\n",
    "import zipfile\n",
    "import itertools\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162105f0-dfa5-47c3-871f-7d34e9a9cc25",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "To begin, let's read in the data we will need:\n",
    "\n",
    "  1. the region shapefiles,\n",
    "  2. the regridded ET data for each data set,\n",
    "  3. the average TC error variances and SNR estimates,\n",
    "  4. the average EC error covariance and cross-correlation estimates, and\n",
    "  5. the relative bias of each data set to the other data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4181f8-8d9e-4fd1-ae17-501bdb436321",
   "metadata": {},
   "source": [
    "### Region Shapefiles\n",
    "\n",
    "We will start with the region shapefiles ([High Plains aquifer](https://www.sciencebase.gov/catalog/item/6314061bd34e36012efa397b), [Central Valley](https://www.sciencebase.gov/catalog/item/63140570d34e36012efa2c01), and [Upper Colorado River Basin](https://www.sciencebase.gov/catalog/item/4f4e4a38e4b07f02db61cebb)), which we will download from USGS ScienceBase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e52e794-f122-4f6c-9ef5-2446a1209d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('../Data/regions'):\n",
    "    os.mkdir('../Data/regions')\n",
    "\n",
    "sciencebase_regions = {'High Plains aquifer': '6314061bd34e36012efa397b',\n",
    "                       'Central Valley': '63140570d34e36012efa2c01',\n",
    "                       'Upper Colorado River Basin': '4f4e4a38e4b07f02db61cebb'}\n",
    "\n",
    "for region in sciencebase_regions:\n",
    "    filename = region.replace(' ', '_')\n",
    "\n",
    "    # Only download file if we don't already have it\n",
    "    if not os.path.isfile(f'../Data/regions/{filename}.zip'):\n",
    "        # Establish a ScienceBase session.\n",
    "        sb = sciencebasepy.SbSession()\n",
    "    \n",
    "        # Get list of files on the ScienceBase page\n",
    "        file_list = sb.get_item_file_info(sb.get_item(sciencebase_regions[region]))\n",
    "\n",
    "        # If the ScienceBase page has the shapefile in zip format, get that.\n",
    "        # Otherwise, download everything and place it into a zip file\n",
    "        zip_url = [f['url'] for f in file_list if 'zip' in f['name']]\n",
    "        if zip_url:\n",
    "            sb.download_file(zip_url[0], f'{filename}.zip', '../Data/regions/')\n",
    "        else:\n",
    "            response = sb.get_item_files(sb.get_item(sciencebase_regions[region]),\n",
    "                                             '../Data/regions/')\n",
    "            # zipfile requires you to work in the directory containing the files...\n",
    "            cwd = os.getcwd()\n",
    "            os.chdir('../Data/regions/')\n",
    "            with zipfile.ZipFile(f'{filename}.zip', 'w') as myzip:\n",
    "                for file in [item['name'] for item in response]:\n",
    "                    myzip.write(file)\n",
    "                    os.remove(file)\n",
    "\n",
    "            os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955bcf0a-6c8b-4663-a143-5c14ca28173d",
   "metadata": {},
   "source": [
    "Now that we have the region files, we will read each of them in, convert them to a coordinate reference system of equal distance, and combine them into a single data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9fde38-fb4b-4f32-82ef-e6126067ed8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "high_plns_aqfr = gpd.read_file('../Data/regions/High_Plains_aquifer.zip')\n",
    "# Exclude the non-aquifer regions\n",
    "high_plns_aqfr = high_plns_aqfr[high_plns_aqfr['AQUIFER'] == 'High Plains aquifer']\n",
    "# Combine the polygons into one single MultiPolygon\n",
    "high_plns_aqfr = gpd.GeoDataFrame(geometry=[unary_union(high_plns_aqfr.geometry.values)],\n",
    "                                  crs=high_plns_aqfr.crs)\n",
    "high_plns_aqfr = high_plns_aqfr.to_crs('EPSG:4269').to_crs('EPSG:4326')\n",
    "high_plns_aqfr['region_name'] = 'High Plains Aquifer'\n",
    "\n",
    "cntrl_valley = gpd.read_file('../Data/regions/Central_Valley.zip')\n",
    "# Buffer is needed to ensure boundary lines of Major Areas overlap.\n",
    "# CRS is in units of meters, so we only buffer by 1mm\n",
    "cntrl_valley = gpd.GeoDataFrame(geometry=[unary_union(cntrl_valley.geometry.buffer(0.001).values)],\n",
    "                                crs=cntrl_valley.crs)\n",
    "cntrl_valley = cntrl_valley.to_crs('EPSG:4269').to_crs('EPSG:4326')\n",
    "cntrl_valley['region_name'] = 'Central Valley'\n",
    "\n",
    "ucrb = gpd.read_file('../Data/regions/Upper_Colorado_River_Basin.zip')\n",
    "# Drop the unneeded variables\n",
    "ucrb = ucrb.drop(columns=['EXT_ID', 'EXT_TYP_ID', 'NAME'])\n",
    "# UCRB CRS is in 4326 already, but let's add this in just in case\n",
    "ucrb = ucrb.to_crs('EPSG:4326')\n",
    "ucrb['region_name'] = 'Upper Colorado River Basin'\n",
    "\n",
    "# Group the regions into one DataFrame\n",
    "regions = pd.concat([high_plns_aqfr, cntrl_valley, ucrb], ignore_index=True)\n",
    "regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e192e12-954d-4561-b0c0-02b9dec28ad3",
   "metadata": {},
   "source": [
    "### Regridded ET data\n",
    "\n",
    "Next, we will read in the regridded ET data and combine the data sets into one `xarray.Dataset`. We will not reduce the data sets to a common data range like the previous notebooks. Instead, we will keep all the data so we can get a time series of the total ET in each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a35b87-836f-4902-aea5-1d450c5798d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "files = ['../Data/ssebop/ssebop_aet_regridded.nc',\n",
    "         '../Data/gleam/gleam_aet.nc',\n",
    "         '../Data/era5/era5_aet_regridded.nc',\n",
    "         '../Data/nldas/nldas_aet_regridded.nc',\n",
    "         '../Data/terraclimate/terraclimate_aet_regridded.nc',        \n",
    "         '../Data/wbet/wbet_aet_regridded.nc',\n",
    "         ]\n",
    "dataset_name = ['SSEBop', 'GLEAM', 'ERA5', 'NLDAS', 'TerraClimate', 'WBET']\n",
    "\n",
    "ds_et = xr.open_mfdataset(files, engine='netcdf4', combine='nested', concat_dim='dataset_name')\n",
    "ds_et = ds_et.assign_coords({'dataset_name': dataset_name})\n",
    "ds_et.dataset_name.attrs['description'] = 'Dataset name'\n",
    "ds_et.aet.attrs['description'] = 'Actual evapotranspiration, monthly total'\n",
    "ds_et.aet.attrs['standard_name'] = 'Actual evapotranspiration'\n",
    "ds_et.aet.attrs['long_name'] = 'Actual evapotranspiration'\n",
    "\n",
    "# Extract the date range of each data set for later\n",
    "date_ranges = {}\n",
    "for file, name in zip(files, dataset_name):\n",
    "    ds_temp = xr.open_dataset(file, engine='netcdf4', chunks={'lon': -1, 'lat': -1, 'time': -1})\n",
    "    date_ranges[name] = [ds_temp.time.min().values, ds_temp.time.max().values]\n",
    "\n",
    "# Transpose to have time first\n",
    "ds_et = ds_et.transpose('time', ...)\n",
    "# The data set is less than 2GiB, so let's read it into memory vs keeping as a dask array\n",
    "ds_et = ds_et.compute()\n",
    "ds_et"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1413b215-9a36-4149-8566-bfdf51fb509a",
   "metadata": {},
   "source": [
    "### Collocation and Bias Results\n",
    "\n",
    "Since we formatted the TC, EC, and Bias data before saving, these are quick and easy reads. We can simply read them in and combine them into one results Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385aa343-655e-4724-8342-73fd59680dd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_tc = xr.open_dataset('../Data/compiled_TC_avg_errs.nc', engine='netcdf4')\n",
    "ds_ec = xr.open_dataset('../Data/compiled_EC_avg_covar_errs.nc', engine='netcdf4')\n",
    "ds_bias = xr.open_dataset('../Data/compiled_avg_bias.nc', engine='netcdf4')\n",
    "ds_results = xr.merge([ds_tc.rename({'Counts': 'var_counts'}),\n",
    "                       ds_ec.rename({'Counts': 'covar_counts', 'covar_pair': 'dataset_pairs'}),\n",
    "                       ds_bias.rename({'Counts': 'bias_counts'})])\n",
    "ds_results.attrs = None\n",
    "ds_results.dataset_pairs.attrs['description'] = 'Data set pair used in the EC or relative bias evaluation.'\n",
    "ds_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e755ef-0f96-4085-9a6b-2e4fda239acb",
   "metadata": {},
   "source": [
    "Now that we have read in our data, let's make a plot that shows the regions overlaid on the ET data. That way we have an idea of the location of the regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b84fb0-d798-4144-9614-d55d48591f08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "et_data_snapshot = ds_et.aet.sel(dataset_name='SSEBop', time='2001-07').squeeze()\n",
    "plt = (et_data_snapshot.hvplot(x='lon', y='lat', geo=True, coastline=True)\n",
    "       * regions.hvplot(c='region_name', alpha=0.5))\n",
    "plt.opts(frame_width=500, frame_height=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd024073-7610-477d-861f-7ee34a035c28",
   "metadata": {},
   "source": [
    "## Creating a Weight Map\n",
    "\n",
    "To do a regional analysis, we need to be able to aggregate our gridded data products over each region. This aggregation requires a weight map that shows the fractional area of each grid pixel that is within the regions. This is the same idea as the conservative regridding that we used to regrid the ET data to a common grid back in the [regridding notebook](1_regrid.ipynb). The only difference is rather than going from a rectangular region to a rectangular region (i.e. pixel to pixel), we are going from a rectangular region to a complex polygon. Unfortunately, we cannot currently use `xarray.regrid` for this, as it only works with rectilinear regridding. However, it looks like it may be a possibility in the future ([Issue #36](https://github.com/EXCITED-CO2/xarray-regrid/issues/36)). In the mean time, we will create a process that allows for regional aggregation and the creation of a weight map following the general methods discussed in [this Pangeo Discourse](https://discourse.pangeo.io/t/conservative-region-aggregation-with-xarray-geopandas-and-sparse/2715).\n",
    "\n",
    "First, let's start be defining a function that converts our grid to polygon boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6684c8d2-639c-4ba1-8895-f5ca188d4515",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grid_to_poly_boxes(grid, lat_coord='latitude', lon_coord='longitude'):\n",
    "    \"\"\"\n",
    "    Creates a collection of polygons that define each grid cell.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    grid : Dataset or DataArray\n",
    "        The grid that has the grid points defined in its coordinates.\n",
    "    lat_coord: str\n",
    "        The coordinate in ``grid`` that defines the latitude at the center\n",
    "        of each grid cell.\n",
    "    lon_coord : str\n",
    "        The coordinate in ``grid`` that defines the longitude at the center\n",
    "        of each grid cell.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    boxes : DataArray\n",
    "        A collection of polygon boxes that define each grid cell.\n",
    "    \"\"\"\n",
    "    # Get the bounds of the grid from the center points.\n",
    "    # NOTE: this assumes the grid is rectilinear!!!\n",
    "    lat_diff = grid[lat_coord].diff(dim=lat_coord)\n",
    "    if len(np.unique(lat_diff)) != 1:\n",
    "        raise ValueError('Latitude grid is not equally spaced.')\n",
    "    else:   \n",
    "        lat_spacing = np.abs(np.unique(lat_diff))\n",
    "        \n",
    "    lon_diff = grid[lon_coord].diff(dim=lon_coord)\n",
    "    if len(np.unique(lon_diff)) != 1:\n",
    "        raise ValueError('Longitude grid is not equally spaced.')\n",
    "    else:   \n",
    "        lon_spacing = np.abs(np.unique(lon_diff))\n",
    "        \n",
    "    # Generate the bounds on the grid\n",
    "    bounds = np.vstack((grid[lat_coord] + lat_spacing/2,\n",
    "                        grid[lat_coord] - lat_spacing/2)).T\n",
    "    grid[lat_coord+'_bounds'] = ((lat_coord, 'bound'), bounds)\n",
    "    bounds = np.vstack((grid[lon_coord] + lon_spacing/2,\n",
    "                        grid[lon_coord] - lon_spacing/2)).T\n",
    "    grid[lon_coord+'_bounds'] = ((lon_coord, 'bound'), bounds)\n",
    "\n",
    "    # Convert the grid and its bounds to a collection of points\n",
    "    points = grid.stack(point=(lat_coord, lon_coord))\n",
    "    \n",
    "    # Create polygons from the grid bounds\n",
    "    def bounds_to_poly(lat_bounds, lon_bounds):\n",
    "        if lon_bounds[0] >= 180:\n",
    "            # geopandas needs this as it goes from -180 to 180\n",
    "            lon_bounds = lon_bounds - 360\n",
    "        return Polygon([\n",
    "            (lon_bounds[0], lat_bounds[0]),\n",
    "            (lon_bounds[0], lat_bounds[1]),\n",
    "            (lon_bounds[1], lat_bounds[1]),\n",
    "            (lon_bounds[1], lat_bounds[0])\n",
    "        ])\n",
    "\n",
    "    boxes = xr.apply_ufunc(\n",
    "        bounds_to_poly,\n",
    "        points[lat_coord+'_bounds'],\n",
    "        points[lon_coord+'_bounds'],\n",
    "        input_core_dims=[('bound',),  ('bound',)],\n",
    "        output_dtypes=[np.dtype('O')],\n",
    "        vectorize=True\n",
    "    )\n",
    "\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b12395c-ec96-4420-8e27-69b981343525",
   "metadata": {},
   "source": [
    "Now that we have the function to create the polygons, we can convert the grid (which is the same for all data sets since we regridded to a common grid) to the polygon boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6083597e-018e-4a18-a3c6-6b8b08f048e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select the lat/lon grid from the ET data set\n",
    "grid = ds_et.drop_vars(['time', 'dataset_name']).reset_coords()\n",
    "boxes = grid_to_poly_boxes(grid, lat_coord='lat', lon_coord='lon')\n",
    "\n",
    "# Place the boxes into a geodataframe\n",
    "grid_df= gpd.GeoDataFrame(\n",
    "    data={'geometry': boxes.data, 'lat': boxes.lat, 'lon': boxes.lon},\n",
    "    index=boxes.indexes['point'],\n",
    "    crs='EPSG:4326'\n",
    ")\n",
    "grid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97f41c1-11bc-49df-a416-cf04c68a5da9",
   "metadata": {},
   "source": [
    "With the polygon boxes, we are now ready to make our weight map. We can do this simply with [`geopandas.overlay`](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.overlay.html#geopandas.GeoDataFrame.overlay), which overlays our polygon boxes over the region polygons, thereby only keeping the boxes and fraction of boxes that are contained with in the polygons. With the overlay, we can then extract the area of each box and box fraction from the geopandas geometry and divided that by the total area of the region to get the box's (i.e. pixel's) area weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54ccf34-06e4-4e95-a7c8-80b0edee9f28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use an area preserving projection\n",
    "equal_area_crs = 'EPSG:5070'\n",
    "\n",
    "grid_df = grid_df.to_crs(equal_area_crs)\n",
    "overlay = grid_df.overlay(regions.to_crs(equal_area_crs))\n",
    "\n",
    "# Get the grid box fractional area\n",
    "grid_cell_fraction = (\n",
    "    overlay.geometry.area.groupby(overlay['region_name'])\n",
    "    .transform(lambda x: x / x.sum())\n",
    ")\n",
    "\n",
    "# Place the fractional areas back into a xarray.Dataset with the same\n",
    "# lat/lon grid as the original data.\n",
    "multi_index = overlay.set_index(['lat', 'lon', 'region_name']).index\n",
    "df_weights = pd.DataFrame({'weights': grid_cell_fraction.values}, index=multi_index)\n",
    "# Place in Dataset and extract the MultiIndex to dimensions\n",
    "# We could convert to a sparse matrix by adding `sparse=True` to the unstack\n",
    "# call, but there is currently no need as our grid is small (~700 kiB).\n",
    "da_weights = xr.Dataset(df_weights).unstack(fill_value=0).weights\n",
    "# Align the weight grid to the data grid, as the weight grid\n",
    "# currently only has the weights in the regions\n",
    "da_weights, _ = xr.align(da_weights, ds_et, join='outer',\n",
    "                         exclude=['time', 'dataset_name'],\n",
    "                         fill_value=0)\n",
    "da_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f410af80-d78e-48e0-9d30-422bb0a862dc",
   "metadata": {},
   "source": [
    "## Regional Aggregation\n",
    "\n",
    "### Regridded ET data\n",
    "\n",
    "Now that we have our weighted grid for each region, we can aggregate our data however we need. As noted in a comment line, we could make the weights a sparse matrix, but we don't since the weight grid is small. One other advantage of not using a sparse matrix is that we can take advantage of [`xarray` `Dataset.weighted()`](https://docs.xarray.dev/en/stable/user-guide/computation.html#weighted-array-reductions). This simplifies the aggregation as we do not need to use the dot product as in the [Pangeo Discourse](https://discourse.pangeo.io/t/conservative-region-aggregation-with-xarray-geopandas-and-sparse/2715).\n",
    "\n",
    "First, let's start by aggregating the regridded ET data. Since our weights for each region were designed to sum to 1, we can easily compute the total ET of the regions using `Dataset.weighted()`.\n",
    "\n",
    "> Note we will use the `mean` to sum over our regions rather than `sum`, since `mean` will result in `NaN` if all lat/lon values are `NaN`, whereas `sum` would result in 0. We can do this since we designed our weights to sum to 1 for each region. These methods can be seen to be equivalent with:\n",
    ">\n",
    "> $$ \\bar{x} = \\frac{\\sum^n_{i=1} w_i x_i}{\\sum^n_{i=1} w_i} = \\sum^n_{i=1} w_i x_i,\\ \\textrm{if}\\ \\sum^n_{i=1} w_i = 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0591d03d-e802-4596-b23b-49df32f20e49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weighted_et = ds_et.weighted(da_weights)\n",
    "ds_et_regional = weighted_et.mean(dim=['lat', 'lon'], skipna=True, keep_attrs=True)\n",
    "\n",
    "ds_et_regional.time.attrs = ds_et.time.attrs\n",
    "ds_et_regional.dataset_name.attrs = ds_et.dataset_name.attrs\n",
    "ds_et_regional.region_name.attrs['description'] = 'Region name'\n",
    "ds_et_regional.aet.attrs['description'] = 'Regionally aggregated total actual evapotranspiration'\n",
    "ds_et_regional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cdc556-4028-4fa0-8e6b-ec07e329a073",
   "metadata": {},
   "source": [
    "With the computed regional ET estimates, let's go ahead and plot the time series and see how they look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f335be45-36d1-4ecd-902a-a121ad6a3770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt = ds_et_regional.aet.hvplot(\n",
    "    groupby=['dataset_name', 'region_name'], label='Total'\n",
    ").overlay('dataset_name').opts(legend_position='right', frame_width=500)\n",
    "\n",
    "pn.panel(plt, widget_location='top')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23d2511-6fdb-4076-bb65-e3f304eb193e",
   "metadata": {},
   "source": [
    "From the time series for each region, we can see that each ET data set is relatively consistent. All six data sets have overlapping ranges in ET values, indicating that none are estimating ET values that are highly inconsistent with the other data sets. There are a few interesting things to note though. One is that GLEAM and NLDAS typically have more limited ranges in ET compared to the others in all regions. This comparatively limited range could be one reason that these two data sets have the lowest TC estimated error variances of the six data sets (i.e., lower range would mean lower variance, which would result in lower TC errors). However, the affine error model assumed by our TC implementation should account for any scaling differences, if the data sets are truly independent. Since we found in [the EC notebook](3_EC_application.ipynb) that the data sets may not be fully independent, it could be that these lower ranges are resulting in artifically lower error variances. Therefore, it is safest to conclude that error variances estimated by TC are lower limits on the true errors.\n",
    "\n",
    "The other interesting observation to note is that the WBET data set has a noticable decrease in average ET in the Central Valley around 1980. After reviewing the paper describing the WBET data set [(Reitz et al. 2023)](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2022WR034012) and remembering that the Central Valley is a highly irrigated region, this decrease is due to how the WBET data set implements irrigation into its water balance calculations. The calculation only includes irrigation estimates from 1980 to 2018. As per the paper \"Because  we  lacked  consistent  national-scale  data  sets for years prior to 1980, those years do not include estimates for irrigation\". Since the Central Valley is so heavily irrigated, this inclusion of irrigation is most pronounced for this region compared to the High Plains Aquifer and Upper Colorado River Basin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275dd19c-8f7a-4200-8fbf-8c588b4e9bca",
   "metadata": {},
   "source": [
    "### TC of Regional Totals\n",
    "\n",
    "Since the regional ET totals are technically collocated data sets, we can apply TC to estimate the error variances of these regional total time series. We can then compare these errors to those estimated by aggregating the gridded TC error variances from the [TC notebook](2_TC_application.ipynb) over the regions. This comparison can help give us an idea of how aggregating at different points in the TC process can affect the resulting error variance estimates.\n",
    "\n",
    "First, we read in the EC function to compute the TC error estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e18c91f-ace5-40ce-aa56-280c81d5a19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../TC/EC_function.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe459a3-ed52-446d-8a7e-ef6e4b26d1ea",
   "metadata": {},
   "source": [
    "Then we generate the list all possible data set triplets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dc3a96-7e22-4dea-b365-595e6de6793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of the combinations, need two correlated data sets, then two additional ones\n",
    "combos = list(itertools.combinations(dataset_name, 3))\n",
    "combos = [list(combo) for combo in combos]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96515deb-789a-4d7e-8b94-f13459265550",
   "metadata": {},
   "source": [
    "Since the data sets span different data ranges, we need to create a function that will limit the triplet to their common date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0537bc-3fa4-492b-bbdc-75e3435328aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_date_range(ds, combo):\n",
    "    \"\"\"Return the common date slice of the datasets.\"\"\"\n",
    "    old_common_date = []\n",
    "    recent_common_date = []\n",
    "    for name in combo:\n",
    "        old_common_date.append(date_ranges[name][0])\n",
    "        recent_common_date.append(date_ranges[name][1])\n",
    "    \n",
    "    return slice(np.max(old_common_date), np.min(recent_common_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3fe8fe-86ed-40f2-aef3-86b984318a91",
   "metadata": {},
   "source": [
    "Finally, we can compute the TC error and unbiased SNR estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1364bd-1455-4396-9b89-5f8494a65a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to ignore all of the sqrt and log warnings with negative values\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# Create list of seasons\n",
    "seasons = ['All'] + list(np.unique(ds_et_regional.time.dt.season))\n",
    "\n",
    "ec_var_est = []\n",
    "for season in seasons:\n",
    "    if season == 'All':\n",
    "        ds_season = ds_et_regional\n",
    "    else:\n",
    "        ds_season = ds_et_regional.isel(time=(ds_et_regional.time.dt.season == season))\n",
    "\n",
    "    ec_var_est_combo = []\n",
    "    for combo in combos:\n",
    "        ds_combo = ds_season.sel(time=common_date_range(ds_et_regional, combo), dataset_name=combo)\n",
    "\n",
    "        ec_covar, snr_temp = ec_covar_multi(ds_combo.aet.data, corr_sets=[1, 2, 3], return_snr=True)\n",
    "\n",
    "        ec_var_est_combo.append(xr.Dataset(data_vars={'error': (['var_combo', 'season', 'combo_idx', 'region_name'],\n",
    "                                                               np.sqrt(np.diagonal(ec_covar).T)[None, None, ...]),\n",
    "                                                      'snr': (['var_combo', 'season', 'combo_idx', 'region_name'],\n",
    "                                                              (10 ** np.log10(snr_temp[None, None, ...]))),},\n",
    "                                           coords={'var_combo': [' '.join(combo)], 'season': [season], \n",
    "                                                   'combo_idx': [0, 1, 2], 'region_name': ds_et_regional.region_name}))\n",
    "\n",
    "    ec_var_est.append(xr.concat(ec_var_est_combo, dim='var_combo'))\n",
    "\n",
    "ec_est = xr.concat(ec_var_est, dim='season')\n",
    "\n",
    "# Restructure the data to be by data set vs by data set pair.\n",
    "ec_est_by_dataset = []\n",
    "est_pair = []\n",
    "for name in dataset_name:\n",
    "    idx_loc = np.char.find(ec_est.var_combo.data, name)\n",
    "    dataset_loc = np.where(idx_loc != -1)[0]\n",
    "    combos_single_dataset = ec_est.isel(var_combo=dataset_loc)\n",
    "    ec_est_dataset = []\n",
    "    for i in range(len(combos_single_dataset.var_combo.values)):\n",
    "        ec_est_single_dataset = combos_single_dataset.isel(var_combo=i)\n",
    "        idx = str(ec_est_single_dataset.var_combo.data).split(' ').index(name)\n",
    "        ec_est_single_dataset = ec_est_single_dataset.sel(combo_idx=idx)\n",
    "        ec_est_single_dataset = ec_est_single_dataset.drop_vars(['var_combo', 'combo_idx'])\n",
    "\n",
    "        ec_est_dataset.append(xr.Dataset(data_vars=ec_est_single_dataset[['error', 'snr']],\n",
    "                              coords={'dataset_name': name, 'combo_idx': i, 'region_name': ec_est.region_name}))\n",
    "    \n",
    "    ec_est_dataset = xr.concat(ec_est_dataset, dim='combo_idx')\n",
    "    \n",
    "    ec_est_by_dataset.append(ec_est_dataset)\n",
    "    est_pair.append([combinations.replace(name, '').strip() for combinations in ec_est.var_combo.data[dataset_loc]])\n",
    "\n",
    "ec_est = xr.concat(ec_est_by_dataset, dim='dataset_name')\n",
    "ec_est = ec_est.assign_coords(est_pair=(['dataset_name', 'est_idx'], np.array(est_pair)))\n",
    "ec_est"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a49193-6d33-46a4-9e07-cf1f3824c5a0",
   "metadata": {},
   "source": [
    "### Aggregation of Gridded Collocation and Bias Results\n",
    "\n",
    "Next, we can apply the same aggregation method as we did with the regridded ET data set on the collocation and bias results. This is just as easy as the regirdded data since we use ``xarray.weighted``. One thing we must note and update though, is the aggregation of the TC error variances (i.e., ``mean_error`` and ``median_error`` in ``ds_et_results``). Since we saved these as error standard deviations, we must square them back to variances before summing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50fe40c-dc58-4bf4-a1fc-286ef966511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_results = ds_results.drop_vars(['mean_error', 'median_error']).weighted(da_weights)\n",
    "weighted_errors = (ds_results[['mean_error', 'median_error']] ** 2).weighted(da_weights)\n",
    "\n",
    "# Use mean rather than sum as it accounts for NaNs\n",
    "# The are equivalent since the sum of weights is 1\n",
    "ds_et_results = weighted_results.mean(dim=['lat', 'lon'], skipna=True, keep_attrs=True)\n",
    "temp = weighted_errors.mean(dim=['lat', 'lon'], skipna=True, keep_attrs=True)\n",
    "\n",
    "# Revert the error variances back to error standard deviations\n",
    "ds_et_results = xr.merge([ds_et_results, np.sqrt(temp)])\n",
    "\n",
    "ds_et_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749f916c-f5b3-4b3b-bf86-a8c91ab2b741",
   "metadata": {},
   "source": [
    "Now that we have aggregated our gridded TC error estimates, let's make a plot that will compare the error estimates generated by TC after aggregating and before aggregating to the regions. Actually, rather than comparing the error estimates, let's compare the unbiased SNRs. This will allow for an easier comparison, since the SNR is just unbaised signal divided by the error variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d821f40c-cb83-48f7-a7e6-4b388234a28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "before = ds_et_results.median_snr\n",
    "before.name = ''\n",
    "after = ec_est.snr.median(dim='combo_idx')\n",
    "after.name = ''\n",
    "\n",
    "def snr_plt(grouping='season'):\n",
    "\n",
    "    coords = list(before.coords)\n",
    "    coords.remove(grouping)\n",
    "    \n",
    "    plt = (after.hvplot.bar(x=grouping, groupby=coords, label='After aggregation', color='orange',\n",
    "                             ylabel='Unbiased SNR', xlabel=' '.join(w.capitalize() for w in grouping.split('_')),\n",
    "                             rot=90)\n",
    "           * before.hvplot.bar(x=grouping, groupby=coords, label='Before aggregation')\n",
    "           * hv.HLine(1).opts(color='black', line_width=1, line_dash='dashed')).opts(frame_height=200)\n",
    "\n",
    "    return pn.panel(plt, widget_location='top')\n",
    "\n",
    "grouping_widget = pn.widgets.Select(name=\"grouping\", value=\"season\", options=list(before.coords))\n",
    "\n",
    "bound_plot = pn.bind(snr_plt, grouping=grouping_widget)\n",
    "\n",
    "pn.Column(grouping_widget, bound_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1133933-74e8-4d7f-aa79-4d793fe182dd",
   "metadata": {},
   "source": [
    "From this comparison, we can see that the SNR generated after aggregation tends to be larger (i.e., smaller error variances) compared to those generated before the regional aggregations. From here on, let's continue to use the errors and SNRs that were generated before aggregation. This way our aggregated results retain the error and SNR variation across the region rather then aggregating that variation out by aggregating before computing the TC errors.\n",
    "\n",
    "Since that is the case, lets remake the above plot, but without the different calculation comparision. In other words, lets just plot the SNR of each data set for each season and region. This will allow us to explore what data sets are performing better (i.e. higher SNR) in each region and season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae17a02-cb3f-408b-9de8-f75e06d40a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_snr = ds_et_results.median_snr.to_dataset(dim='dataset_name')\n",
    "\n",
    "plt = (ds_snr.hvplot.bar(x='season', groupby='region_name', ylabel='Unbiased SNR',\n",
    "                            xlabel='Season', rot=90)\n",
    "       * hv.HLine(1).opts(color='black', line_width=1, line_dash='dashed')).opts(frame_height=200)\n",
    "\n",
    "pn.panel(plt, widget_location='top')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a2478d-0663-4dc0-a54f-03015a139449",
   "metadata": {},
   "source": [
    "From this comparison, we can see that in the Central Valley NLDAS, ERA5, and GLEAM have relatively high (>2) SNR during every season, whereas WBET, TerraClimate, and SSEBop rarely have an SNR >1 at any point. This likely indicates that these three data sets are not optimized to function in this region, which could have implications on their use there. As for the High Plains Aquifer region, all data sets besides TerraClimate seem to have relativley high SNR values across each seaons, excluding winter. This low SNR in winter is generally expected. As we saw in the [TC notebook](2_TC_application.ipynb), the ET across CONUS is very low if not zero in for regions that actually experience a winter season. Therefore, we would expect a low SNR for most if not all data sets during the winter months in both the High Plains and Upper Colorado. Finally, the Upper Colorado River Basin has a similar SNR pattern as the High Plains with the highest SNR in the Spring and Fall and lowest in the winter. For these two regions, no data sets really stand out as the optimal performer. Instead each seem to perform similarly well during each season.\n",
    "\n",
    "Finally, let's look at the relative bias of the data sets compared to its errors for each region to see if those that have the higher SNRs above are any more biased compared to the other data sets. To do this, we will use the same metric as in the [Bias notebook](4_Bias.ipynb), a scaled fractional difference between the error and bias. This will follow the simple formula of:\n",
    "\n",
    "$$\\textrm{frac\\_diff} = \\frac{\\textrm{error} - \\textrm{bias}}{\\textrm{error} + \\textrm{bias}}.$$\n",
    "\n",
    "This scales the difference to be between -1 and 1, where negative values indicate that the bias is larger than the errors and vice versa for the positive values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33cb35e-e098-4361-bc5d-b7c23205a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_plt(dataset_name='SSEBop', region_name='Central Valley'):\n",
    "\n",
    "    err = ds_et_results.median_error.sel(dataset_name=dataset_name, region_name=region_name)\n",
    "\n",
    "    bias = ds_et_results.median_bias.sel(region_name=region_name)\n",
    "    bias = bias.sel(dataset_pairs=(np.char.find(ds_et_results.dataset_pairs.data, dataset_name) != -1))\n",
    "    bias = (err - np.abs(bias))/(err + np.abs(bias))\n",
    "    bias = bias.assign_coords({'dataset_pairs': np.char.strip(np.char.replace(bias.dataset_pairs.data, dataset_name, ''))})\n",
    "    bias = bias.to_dataset(dim='dataset_pairs')\n",
    "    \n",
    "    plt = (bias.hvplot.bar(x='season', ylabel='(Error - Relative Bias)/(Error + Relative Bias)',\n",
    "                           xlabel='Season', rot=90)#, ylim=(-1.1, 1.1))\n",
    "           * hv.HLine(1).opts(color='black', line_width=1, line_dash='dashed')\n",
    "           * hv.HLine(-1).opts(color='black', line_width=1, line_dash='dashed')\n",
    "           * hv.HLine(0).opts(color='black', line_width=1)).opts(frame_height=200)\n",
    "\n",
    "    return plt\n",
    "\n",
    "dataset_name_widget = pn.widgets.Select(name=\"dataset_name\", value=\"SSEBop\", options=dataset_name)\n",
    "region_name_widget = pn.widgets.Select(name=\"region_name\", value=\"Central Valley\", options=list(ds_et_results.region_name.data))\n",
    "\n",
    "bound_plot = pn.bind(bias_plt, dataset_name=dataset_name_widget, region_name=region_name_widget)\n",
    "\n",
    "pn.Column(dataset_name_widget, region_name_widget, bound_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30959ceb-e958-4bc4-9608-d3b8abc6427d",
   "metadata": {},
   "source": [
    "This figure shows some interesting results. First, for the Central Valley, the errors are always larger than the bias when we look at all time. However, when we look at the seasonal data, the bias is typically large. This effect is likely due to the way we aggregated our relative biases back in the [bias notebook](4_Bias.ipynb). By aggregating all bias to the median across time, we are smoothing the variation across seasons. Therefore, it is likely that relative bias is indeed larger than the errors, indicating the the choice of ET data set in the Central Valley can have impact on any results derived from it. For the High Plains Aquifer and Upper Colorado River Basin regions, we see more mixed results of bias versus errors being larger, which makes a bias assessment harder to judge.\n",
    "\n",
    "Therefore, let's look deeper at the Central Valley to see how the uncertainty and bias may be affecting the ET data sets in that region. Specifically, let's focus in on the 2012-2016 extreme drought period that devastated the region. First, let's plot the time series data again for that period, but with the associated seasonal uncertainty included as a shaded area. Also, let's plot the cumulative ET of that time series to see how each data set compares in the estimated total ET during the drought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81060252-02e5-41a8-b006-739fb1b83066",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_drought = ds_et_regional.copy()\n",
    "\n",
    "ds_drought['error'] = xr.zeros_like(ds_drought['aet'])\n",
    "seasons = ds_drought.time.dt.season\n",
    "for season in np.unique(seasons):\n",
    "    ds_drought['error'] = ds_drought['error'].where(~(seasons == season), ds_et_results.median_error.sel(season=season))\n",
    "\n",
    "ds_drought['high'] = ds_drought['aet'] + ds_drought['error']\n",
    "ds_drought['low'] = ds_drought['aet'] - ds_drought['error']\n",
    "ds_drought = ds_drought.drop_vars('season')\n",
    "\n",
    "ds_cumsum = ds_drought.aet.cumsum(dim='time').to_dataset()\n",
    "ds_cumsum['aet'] = ds_cumsum.aet - ds_cumsum.aet.sel(time='10-01-2011')\n",
    "ds_cumsum.aet.attrs['long_name'] = 'Cumulative AET'\n",
    "ds_cumsum.aet.attrs['units'] = 'mm.month-1'\n",
    "\n",
    "plt = ((ds_drought.aet.hvplot(groupby=['dataset_name', 'region_name'], xlim=(pd.to_datetime('10-01-2011'), pd.to_datetime('09-30-2016')),\n",
    "                              ylim=(-20, 150)).overlay('dataset_name').opts(legend_position='right')\n",
    "        * ds_drought.hvplot.area(x='time', y='high', y2='low', groupby=['dataset_name', 'region_name'], alpha=0.5).overlay('dataset_name'))\n",
    "       + (ds_cumsum.aet.hvplot(groupby=['dataset_name', 'region_name'], ylim=(-100, 2700),\n",
    "                               xlim=(pd.to_datetime('10-01-2011'), pd.to_datetime('09-30-2016'))).overlay('dataset_name').opts(legend_position='right'))).cols(1)\n",
    "\n",
    "pn.panel(plt, widget_location='top')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aaed51-e06f-4d45-bbe5-d2d2d0ab2990",
   "metadata": {},
   "source": [
    "Well, these are some very interesting results. Looking at the time series first, the Central Valley shows a misalignment between several of the data set (while the High Plains and Upper Colorado regions show each data set aligning their peaks around the same month, typically July). SSEBop and WBET have a peak ET in July as is typically expected, while the others have a peak ET in April. This misalignment in peaks could have a serious bias issue on any derived product as the timings would be off by three months.\n",
    "\n",
    "As for the cumulative ET plot, it is clear that there is a bias issue between each of the data sets, with WBET (SSEBop and ERA5) estimating almost double (one and a half) the ET as GLEAM, NLDAS, and TerraClimate. Since we have the errors on the time series, let's use that to generate some simulations to estimate if the differences in the cumulative distributions are statistically significant. We can do this with a simple two sample KS test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db874412-6ab0-4887-8bd1-356339ac3705",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "\n",
    "ds_drought = ds_drought.sel(time=slice('10-01-2011', '09-30-2016'))\n",
    "x = rng.normal(loc=np.expand_dims(ds_drought.aet, -1),\n",
    "               scale=np.expand_dims(ds_drought.error, -1),\n",
    "               size=ds_drought.error.shape + (1000,))\n",
    "ds_drought['aet_sim'] = (['time', 'dataset_name', 'region_name', 'sim'], x)\n",
    "\n",
    "pairs = list(itertools.combinations(dataset_name, 2))\n",
    "pairs = [list(pair) for pair in pairs]\n",
    "\n",
    "for region_name in ds_drought.region_name.data:\n",
    "    ds_region = ds_drought.sel(region_name=region_name)\n",
    "    print(region_name)\n",
    "    for pair in pairs:\n",
    "        results = kstest(ds_region.aet_sim.sel(dataset_name=pair[0]), ds_region.aet_sim.sel(dataset_name=pair[1]))\n",
    "        print(f'K-S Test results for {pair[0]} and {pair[1]} p-value = {np.median(results[1]):.4f} +/- {np.std(results[1]):.4f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c03aabd-87d5-4a78-948b-5b5ed5ebcf17",
   "metadata": {},
   "source": [
    "From the KS tests, we can see that as per the plot only GLEAM, NLDAS, and TerraClimate cannot be deemed to have a statistically significance difference in their cumulative ET. Even SSEBop and ERA5 have different cumulative ET, while visually looking relatively similar. This disagreement is likely from the offset in the time series causing the significant difference. Therefore, the differences in the bias of these ET data sets should be taken into consideration when utilizing them in other studies.\n",
    "\n",
    "As a final thing to check, if we zoom out time-wise on the cumulative distributions, we can see that each data set seems to follow a constant slope. If this is the case, we could easily bias correct these data sets to some chosen standard. While exploring this further is beyond these notebooks, let's just make one last plot to see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333b65ef-07fd-47fc-afc3-b8ddcf202d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_cumsum_fit = ds_drought.aet.cumsum(dim='time')\n",
    "ds_cumsum_fit['time'] = list(range(len(ds_cumsum_fit['time'])))\n",
    "fit_results = ds_cumsum_fit.curvefit('time', lambda params, x: params * x).squeeze()\n",
    "ds_drought_bias_adj = fit_results.curvefit_coefficients.sel(dataset_name='WBET') / fit_results.curvefit_coefficients * ds_drought.aet\n",
    "ds_drought_bias_adj.attrs['long_name'] = 'Bias corrected AET'\n",
    "ds_drought_bias_adj.attrs['units'] = 'mm.month-1'\n",
    "\n",
    "(ds_drought.aet.hvplot(groupby=['dataset_name', 'region_name'], ylim=(-20, 160)).overlay('dataset_name').opts(legend_position='right')\n",
    "+ ds_drought_bias_adj.hvplot(groupby=['dataset_name', 'region_name'], ylim=(-20, 160)).overlay('dataset_name').opts(legend_position='right')\n",
    "+ ds_drought_bias_adj.cumsum(dim='time').hvplot(groupby=['dataset_name', 'region_name']).overlay('dataset_name').opts(legend_position='right')).cols(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a401e983-9046-4880-8b6d-1ea7a64fff88",
   "metadata": {},
   "source": [
    "Hey look at that, it seems like this kind of bias correction may work! So, these data sets could be corrected. One last thing to note is that while the raw data sets may likely need a bias correction, the TC error estimates that were at the center of this work should not change after a bias correction. As you can check and remember from the [TC function](../TC/TC_function.ipynb), the TC method we used assumes an affine error model that will include any linear biases that maybe present between data sets. Therefore, even in the presence of this bias, the error estimates made from the TC method should remain unchanged."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
