{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0872207f-ce5f-4ff3-8769-68aa7a7ef895",
   "metadata": {},
   "source": [
    "# Regional Analysis\n",
    "\n",
    "In this notebook we will explore the results of the three previous notebooks in terms of specific regions in the US. We will look at the High Plains Aquifer, which has been shown to have accelerated decline in aquifer storage, the Central Valley, as it is another region that has suffered from declines in aquifer storage due to agricultural usage, and the Upper Colorado River Basin, as it is a critical basin for understanding the down stream flow of the Colorado River in the southwest US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abfb55c-25f4-4bd1-a49f-adb26cc71665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely import unary_union, box\n",
    "from shapely.geometry import Polygon\n",
    "from scipy.stats import norm, kstest\n",
    "import holoviews as hv\n",
    "import hvplot.xarray\n",
    "import hvplot.pandas\n",
    "import panel as pn\n",
    "import cartopy.crs as ccrs\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from xarray_einstats import linalg, stats\n",
    "import sciencebasepy\n",
    "import os\n",
    "import zipfile\n",
    "import itertools\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162105f0-dfa5-47c3-871f-7d34e9a9cc25",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "To begin, let's read in the data we will need:\n",
    "\n",
    "  1. the region shapefiles,\n",
    "  2. the regridded ET data for each data set,\n",
    "  3. the TC error variances and SNR estimates,\n",
    "  4. the EC error covariance and cross-correlation estimates, and\n",
    "  5. the relative bias of each data set to the other data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4181f8-8d9e-4fd1-ae17-501bdb436321",
   "metadata": {},
   "source": [
    "### Region Shapefiles\n",
    "\n",
    "We will start with the region shapefiles ([High Plains aquifer](https://www.sciencebase.gov/catalog/item/6314061bd34e36012efa397b), [Central Valley](https://www.sciencebase.gov/catalog/item/63140570d34e36012efa2c01), and [Upper Colorado River Basin](https://www.sciencebase.gov/catalog/item/4f4e4a38e4b07f02db61cebb)), which we will download from USGS ScienceBase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e52e794-f122-4f6c-9ef5-2446a1209d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('../Data/regions'):\n",
    "    os.mkdir('../Data/regions')\n",
    "\n",
    "sciencebase_regions = {'High Plains aquifer': '6314061bd34e36012efa397b',\n",
    "                       'Central Valley': '63140570d34e36012efa2c01',\n",
    "                       'Upper Colorado River Basin': '4f4e4a38e4b07f02db61cebb'}\n",
    "\n",
    "for region in sciencebase_regions:\n",
    "    filename = region.replace(' ', '_')\n",
    "\n",
    "    # Only download file if we don't already have it\n",
    "    if not os.path.isfile(f'../Data/regions/{filename}.zip'):\n",
    "        # Establish a ScienceBase session.\n",
    "        sb = sciencebasepy.SbSession()\n",
    "    \n",
    "        # Get list of files on the ScienceBase page\n",
    "        file_list = sb.get_item_file_info(sb.get_item(sciencebase_regions[region]))\n",
    "\n",
    "        # If the ScienceBase page has the shapefile in zip format, get that.\n",
    "        # Otherwise, download everything and place it into a zip file\n",
    "        zip_url = [f['url'] for f in file_list if 'zip' in f['name']]\n",
    "        if zip_url:\n",
    "            sb.download_file(zip_url[0], f'{filename}.zip', '../Data/regions/')\n",
    "        else:\n",
    "            response = sb.get_item_files(sb.get_item(sciencebase_regions[region]),\n",
    "                                             '../Data/regions/')\n",
    "            # zipfile requires you to work in the directory containing the files...\n",
    "            cwd = os.getcwd()\n",
    "            os.chdir('../Data/regions/')\n",
    "            with zipfile.ZipFile(f'{filename}.zip', 'w') as myzip:\n",
    "                for file in [item['name'] for item in response]:\n",
    "                    myzip.write(file)\n",
    "                    os.remove(file)\n",
    "\n",
    "            os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955bcf0a-6c8b-4663-a143-5c14ca28173d",
   "metadata": {},
   "source": [
    "Now that we have the region files, we will read each of them in, convert them to a coordinate reference system of equal distance, and combine them into a single data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d15532-daf0-4d1b-979a-7e13f363ea83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "high_plns_aqfr = gpd.read_file('../Data/regions/High_Plains_aquifer.zip')\n",
    "\n",
    "# Exclude the non-aquifer regions\n",
    "high_plns_aqfr = high_plns_aqfr[high_plns_aqfr['AQUIFER'] == 'High Plains aquifer']\n",
    "# Combine the polygons into one single MultiPolygon\n",
    "high_plns_aqfr = gpd.GeoDataFrame(geometry=[unary_union(high_plns_aqfr.geometry.values)],\n",
    "                                  crs=high_plns_aqfr.crs)\n",
    "high_plns_aqfr = high_plns_aqfr.to_crs('EPSG:4269').to_crs('EPSG:4326')\n",
    "high_plns_aqfr['region_name'] = 'High Plains Aquifer'\n",
    "\n",
    "cntrl_valley = gpd.read_file('../Data/regions/Central_Valley.zip')\n",
    "# Buffer is needed to ensure boundary lines of Major Areas overlap.\n",
    "# CRS is in units of meters, so we only buffer by 1mm\n",
    "cntrl_valley = gpd.GeoDataFrame(geometry=[unary_union(cntrl_valley.geometry.buffer(0.001).values)],\n",
    "                                crs=cntrl_valley.crs)\n",
    "cntrl_valley = cntrl_valley.to_crs('EPSG:4269').to_crs('EPSG:4326')\n",
    "cntrl_valley['region_name'] = 'Central Valley'\n",
    "\n",
    "ucrb = gpd.read_file('../Data/regions/Upper_Colorado_River_Basin.zip')\n",
    "\n",
    "# Drop the unneeded variables\n",
    "ucrb = ucrb.drop(columns=['EXT_ID', 'EXT_TYP_ID', 'NAME'])\n",
    "# UCRB CRS is in 4326 already, but let's add this in just in case\n",
    "ucrb = ucrb.to_crs('EPSG:4326')\n",
    "ucrb['region_name'] = 'Upper Colorado River Basin'\n",
    "\n",
    "# Group the regions into one DataFrame\n",
    "regions = pd.concat([high_plns_aqfr, cntrl_valley, ucrb], ignore_index=True)\n",
    "regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e192e12-954d-4561-b0c0-02b9dec28ad3",
   "metadata": {},
   "source": [
    "### Regridded ET data\n",
    "\n",
    "Next, we will read in the regridded ET data and combine the data sets into one `xarray.Dataset`. We will not reduce the data sets to a common data range like the previous notebooks. Instead, we will keep all the data so we can get a time series of the total ET in each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a35b87-836f-4902-aea5-1d450c5798d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "files = ['../Data/ssebop/ssebop_aet_regridded.nc',\n",
    "         '../Data/gleam/gleam_aet.nc',\n",
    "         '../Data/era5/era5_aet_regridded.nc',\n",
    "         '../Data/nldas/nldas_aet_regridded.nc',\n",
    "         '../Data/terraclimate/terraclimate_aet_regridded.nc',        \n",
    "         '../Data/wbet/wbet_aet_regridded.nc',\n",
    "         ]\n",
    "dataset_name = ['SSEBop', 'GLEAM', 'ERA5', 'NLDAS', 'TerraClimate', 'WBET']\n",
    "\n",
    "ds_et = xr.open_mfdataset(files, engine='netcdf4', combine='nested', concat_dim='dataset_name')\n",
    "ds_et = ds_et.assign_coords({'dataset_name': dataset_name})\n",
    "ds_et.dataset_name.attrs['description'] = 'Dataset name'\n",
    "ds_et.aet.attrs['description'] = 'Actual evapotranspiration, monthly total'\n",
    "ds_et.aet.attrs['standard_name'] = 'Actual evapotranspiration'\n",
    "ds_et.aet.attrs['long_name'] = 'Actual evapotranspiration'\n",
    "\n",
    "# Extract the date range of each data set for later\n",
    "date_ranges = {}\n",
    "for file, name in zip(files, dataset_name):\n",
    "    ds_temp = xr.open_dataset(file, engine='netcdf4', chunks={'lon': -1, 'lat': -1, 'time': -1})\n",
    "    date_ranges[name] = [ds_temp.time.min().values, ds_temp.time.max().values]\n",
    "\n",
    "# Transpose to have time first\n",
    "ds_et = ds_et.transpose('time', ...)\n",
    "# The data set is less than 2GiB, so let's read it into memory vs keeping as a dask array\n",
    "ds_et = ds_et.compute()\n",
    "ds_et"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1413b215-9a36-4149-8566-bfdf51fb509a",
   "metadata": {},
   "source": [
    "### Collocation and Bias Results\n",
    "\n",
    "Since we formatted the TC, EC, and Bias data before saving, these are quick and easy reads. We can simply read them in and combine them into one results `Dataset` after simply renaming some variables and coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385aa343-655e-4724-8342-73fd59680dd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_tc = xr.open_dataset('../Data/TC_errs.nc', engine='netcdf4')\n",
    "ds_ec = xr.open_dataset('../Data/EC_errs.nc', engine='netcdf4')\n",
    "ds_bias = xr.open_dataset('../Data/avg_bias.nc', engine='netcdf4')\n",
    "ds_results = xr.merge([ds_tc.rename({'est_idx': 'var_est_idx', 'est_pair': 'var_est_pair'}),\n",
    "                       ds_ec.rename({'covar_pair': 'dataset_pair'}),\n",
    "                       ds_bias])\n",
    "ds_results.attrs = None\n",
    "ds_results.dataset_pair.attrs['description'] = 'Data set pair used in the EC or relative bias evaluation.'\n",
    "ds_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e755ef-0f96-4085-9a6b-2e4fda239acb",
   "metadata": {},
   "source": [
    "Now that we have read in our data, let's make a plot that shows the regions overlaid on the ET data. That way we have an idea of the location of the regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b84fb0-d798-4144-9614-d55d48591f08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "et_data_snapshot = ds_et.aet.sel(dataset_name='SSEBop', time='2001-07').squeeze()\n",
    "plt = (et_data_snapshot.hvplot(x='lon', y='lat', geo=True, coastline=True)\n",
    "       * regions.hvplot(c='region_name', alpha=0.5))\n",
    "plt.opts(frame_width=500, frame_height=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd024073-7610-477d-861f-7ee34a035c28",
   "metadata": {},
   "source": [
    "## Creating a Weight Map\n",
    "\n",
    "To do a regional analysis, we need to be able to aggregate our gridded data products over each region. This aggregation requires a weight map that shows the fractional area of each grid pixel that is within the regions. This is the same idea as the conservative regridding that we used to regrid the ET data to a common grid back in the [regridding notebook](1_regrid.ipynb). The only difference is rather than going from a rectangular region to a rectangular region (i.e. pixel to pixel), we are going from a rectangular region to a complex polygon. Unfortunately, we cannot currently use `xarray.regrid` for this, as it only works with rectilinear regridding. However, it looks like it may be a possibility in the future ([Issue #36](https://github.com/EXCITED-CO2/xarray-regrid/issues/36)). In the mean time, we will create a process that allows for regional aggregation and the creation of a weight map following the general methods discussed in [this Pangeo Discourse](https://discourse.pangeo.io/t/conservative-region-aggregation-with-xarray-geopandas-and-sparse/2715).\n",
    "\n",
    "First, let's start be defining a function that converts our grid to polygon boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6684c8d2-639c-4ba1-8895-f5ca188d4515",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grid_to_poly_boxes(grid, lat_coord='latitude', lon_coord='longitude'):\n",
    "    \"\"\"\n",
    "    Creates a collection of polygons that define each grid cell.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    grid : Dataset or DataArray\n",
    "        The grid that has the grid points defined in its coordinates.\n",
    "    lat_coord: str\n",
    "        The coordinate in ``grid`` that defines the latitude at the center\n",
    "        of each grid cell.\n",
    "    lon_coord : str\n",
    "        The coordinate in ``grid`` that defines the longitude at the center\n",
    "        of each grid cell.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    boxes : DataArray\n",
    "        A collection of polygon boxes that define each grid cell.\n",
    "    \"\"\"\n",
    "    # Get the bounds of the grid from the center points.\n",
    "    # NOTE: this assumes the grid is rectilinear!!!\n",
    "    lat_diff = grid[lat_coord].diff(dim=lat_coord)\n",
    "    if len(np.unique(lat_diff)) != 1:\n",
    "        raise ValueError('Latitude grid is not equally spaced.')\n",
    "    else:   \n",
    "        lat_spacing = np.abs(np.unique(lat_diff))\n",
    "        \n",
    "    lon_diff = grid[lon_coord].diff(dim=lon_coord)\n",
    "    if len(np.unique(lon_diff)) != 1:\n",
    "        raise ValueError('Longitude grid is not equally spaced.')\n",
    "    else:   \n",
    "        lon_spacing = np.abs(np.unique(lon_diff))\n",
    "        \n",
    "    # Generate the bounds on the grid\n",
    "    bounds = np.vstack((grid[lat_coord] + lat_spacing/2,\n",
    "                        grid[lat_coord] - lat_spacing/2)).T\n",
    "    grid[lat_coord+'_bounds'] = ((lat_coord, 'bound'), bounds)\n",
    "    bounds = np.vstack((grid[lon_coord] + lon_spacing/2,\n",
    "                        grid[lon_coord] - lon_spacing/2)).T\n",
    "    grid[lon_coord+'_bounds'] = ((lon_coord, 'bound'), bounds)\n",
    "\n",
    "    # Convert the grid and its bounds to a collection of points\n",
    "    points = grid.stack(point=(lat_coord, lon_coord))\n",
    "    \n",
    "    # Create polygons from the grid bounds\n",
    "    def bounds_to_poly(lat_bounds, lon_bounds):\n",
    "        if lon_bounds[0] >= 180:\n",
    "            # geopandas needs this as it goes from -180 to 180\n",
    "            lon_bounds = lon_bounds - 360\n",
    "        return box(lon_bounds[0],\n",
    "                   lat_bounds[0],\n",
    "                   lon_bounds[1],\n",
    "                   lat_bounds[1])\n",
    "\n",
    "    boxes = xr.apply_ufunc(\n",
    "        bounds_to_poly,\n",
    "        points[lat_coord+'_bounds'],\n",
    "        points[lon_coord+'_bounds'],\n",
    "        input_core_dims=[('bound',),  ('bound',)],\n",
    "        output_dtypes=[np.dtype('O')],\n",
    "        vectorize=True\n",
    "    )\n",
    "\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b12395c-ec96-4420-8e27-69b981343525",
   "metadata": {},
   "source": [
    "Now that we have the function to create the polygons, we can convert the grid (which is the same for all data sets since we regridded to a common grid) to the polygon boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6083597e-018e-4a18-a3c6-6b8b08f048e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select the lat/lon grid from the ET data set\n",
    "grid = ds_et[['lat', 'lon']]\n",
    "boxes = grid_to_poly_boxes(grid, lat_coord='lat', lon_coord='lon')\n",
    "\n",
    "# Place the boxes into a geodataframe\n",
    "grid_df= gpd.GeoDataFrame(\n",
    "    data={'geometry': boxes.data, 'lat': boxes.lat, 'lon': boxes.lon},\n",
    "    index=boxes.indexes['point'],\n",
    "    crs='EPSG:4326'\n",
    ")\n",
    "grid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97f41c1-11bc-49df-a416-cf04c68a5da9",
   "metadata": {},
   "source": [
    "With the polygon boxes, we are now ready to make our weight map. We can do this simply with [`geopandas.overlay`](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.overlay.html#geopandas.GeoDataFrame.overlay), which overlays our polygon boxes over the region polygons, thereby only keeping the boxes and fraction of boxes that are contained with in the polygons. With the overlay, we can then extract the area of each box and divide it by the total area of the region to get the box's (i.e. pixel's) area weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54ccf34-06e4-4e95-a7c8-80b0edee9f28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use an area preserving projection\n",
    "equal_area_crs = 'EPSG:5070'\n",
    "\n",
    "overlay = grid_df.to_crs(equal_area_crs).overlay(regions.to_crs(equal_area_crs))\n",
    "\n",
    "# Get the grid box fractional area\n",
    "grid_cell_fraction = (\n",
    "    overlay.geometry.area.groupby(overlay['region_name'])\n",
    "    .transform(lambda x: x / x.sum())\n",
    ")\n",
    "\n",
    "# Place the fractional areas back into a xarray.Dataset with the same\n",
    "# lat/lon grid as the original data.\n",
    "multi_index = overlay.set_index(['lat', 'lon', 'region_name']).index\n",
    "df_weights = pd.DataFrame({'weights': grid_cell_fraction.values}, index=multi_index)\n",
    "# Place in Dataset and extract the MultiIndex to dimensions\n",
    "# We could convert to a sparse matrix by adding `sparse=True` to the unstack\n",
    "# call, but there is currently no need as our grid is small (~700 kiB).\n",
    "da_weights = xr.Dataset(df_weights).unstack(fill_value=0).weights\n",
    "# Align the weight grid to the data grid, as the weight grid\n",
    "# currently only has the weights in the regions\n",
    "da_weights, _ = xr.align(da_weights, ds_et, join='outer',\n",
    "                         exclude=['time', 'dataset_name'],\n",
    "                         fill_value=0)\n",
    "da_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f410af80-d78e-48e0-9d30-422bb0a862dc",
   "metadata": {},
   "source": [
    "## Regional Aggregation\n",
    "\n",
    "### Regridded ET data\n",
    "\n",
    "Now that we have our weighted grid for each region, we can aggregate our data however we need. As noted in a comment line above, we could make the weights a sparse matrix, but we don't since the weight grid is small. One other advantage of not using a sparse matrix is that we can take advantage of [`xarray.Dataset.weighted()`](https://docs.xarray.dev/en/stable/user-guide/computation.html#weighted-array-reductions). This simplifies the aggregation as we do not need to use the dot product as in the [Pangeo Discourse](https://discourse.pangeo.io/t/conservative-region-aggregation-with-xarray-geopandas-and-sparse/2715).\n",
    "\n",
    "First, let's start by aggregating the regridded ET data. Since our weights for each region were designed to sum to 1, we can easily compute the area weighted average ET of the regions using `Dataset.weighted()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0591d03d-e802-4596-b23b-49df32f20e49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weighted_et = ds_et.weighted(da_weights)\n",
    "ds_et_regional = weighted_et.mean(dim=['lat', 'lon'], skipna=True, keep_attrs=True)\n",
    "\n",
    "ds_et_regional.time.attrs = ds_et.time.attrs\n",
    "ds_et_regional.dataset_name.attrs = ds_et.dataset_name.attrs\n",
    "ds_et_regional.region_name.attrs['description'] = 'Region name'\n",
    "ds_et_regional.aet.attrs['description'] = 'Regionally aggregated total actual evapotranspiration'\n",
    "ds_et_regional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cdc556-4028-4fa0-8e6b-ec07e329a073",
   "metadata": {},
   "source": [
    "With the computed regional ET estimates, let's go ahead and plot the time series and see how they look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f335be45-36d1-4ecd-902a-a121ad6a3770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt = ds_et_regional.aet.hvplot(\n",
    "    groupby=['dataset_name', 'region_name'], label='Total'\n",
    ").overlay('dataset_name').opts(legend_position='right', frame_width=500)\n",
    "\n",
    "pn.panel(plt, widget_location='top')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23d2511-6fdb-4076-bb65-e3f304eb193e",
   "metadata": {},
   "source": [
    "From the time series for the High Plains and Upper Colorado, we can see that each ET data set is relatively consistent. All six data sets have overlapping ranges in ET values, indicating that none are estimating ET values that are highly inconsistent with the other data sets. However, when looking at the Central Valley, we can see that the WBET data set has a noticable decrease in average ET in the Central Valley around 1980. After reviewing the paper describing the WBET data set [(Reitz et al. 2023)](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2022WR034012) and remembering that the Central Valley is a highly irrigated region, this decrease is due to how the WBET data set implements irrigation into its water balance calculations. The calculation only includes irrigation estimates from 1980 to 2018. As per the paper \"Because  we  lacked  consistent  national-scale  data  sets for years prior to 1980, those years do not include estimates for irrigation\". Since the Central Valley is so heavily irrigated, this inclusion of irrigation is most pronounced for this region compared to the High Plains Aquifer and Upper Colorado River Basin. \n",
    "\n",
    "Another interesting difference in the data sets for the Central Valley can be seen when zooming in on the 2000s. Here, we can see that the peak ET month is shifted earlier in the year (~April) for GLEAM, ERA5, NLDAS, and TerraClimate, whereas SSEBop and WBET have the peak ET around July, which is the peak month for the High Plains and Upper Colorado for all data sets. The reason for this likely stems from the precipitation patterns in the Central Valley and the lack of an irrigation component in these four ET data sets. In the Central Valley, precipitation primarily occurs in the winter months and is minimal in the summer. To account for this lack of precipitation, the region is highly irrigated. Therefore, based on weather alone, ET should peak in the spring as temperatures start to rise and the precipitation begins to decrease. However, if irrigation is included, the amount of ET should continue to rise as the temperature does, peaking in the summer months since water is continually supplied through irrigation. So, including an irrigation component when calculating ET can affect monthly timings of peak ET. Irrigation being the likely cause is further supported by the peak shift only occuring in the WBET data after 1980, when irrigation is included.\n",
    "\n",
    "Besides this temporal offset in the Central Valley, there is one other interesting thing to note. It is that GLEAM and NLDAS typically have more limited ranges in ET compared to the others in all regions. This comparatively limited range could be one reason that these two data sets have the lowest TC estimated error variances of the six data sets (i.e., lower range would mean lower variance, which would result in lower TC errors). However, the affine error model assumed by our TC implementation should account for any scaling differences, if the data sets are truly independent. Since we found in [the EC notebook](3_EC_application.ipynb#EC-Discussion) that the data sets may not be fully independent, it could be that these lower ranges are resulting in artifically lower error variances. Therefore, it is safest to conclude that error variances estimated by TC are lower limits on the true errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a49193-6d33-46a4-9e07-cf1f3824c5a0",
   "metadata": {},
   "source": [
    "### Collocation and Bias Results\n",
    "\n",
    "Next, we can apply the same aggregation method as we did with the regridded ET data set on the collocation and bias results. This is just as easy as the regirdded data since we use ``xarray.weighted``. One thing we must note and update though, is the aggregation of the TC error variances (i.e., ``error`` in ``ds_results``). Since we saved these as error standard deviations, we must square them back to variances before summing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50fe40c-dc58-4bf4-a1fc-286ef966511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_results = ds_results.drop_vars(['error']).weighted(da_weights)\n",
    "weighted_errors = (ds_results[['error']] ** 2).weighted(da_weights)\n",
    "\n",
    "# Use mean rather than sum as it accounts for NaNs\n",
    "# The are equivalent since the sum of weights is 1\n",
    "ds_et_results = weighted_results.mean(dim=['lat', 'lon'], skipna=True, keep_attrs=True)\n",
    "temp = weighted_errors.mean(dim=['lat', 'lon'], skipna=True, keep_attrs=True)\n",
    "\n",
    "# Revert the error variances back to error standard deviations\n",
    "ds_et_results = xr.merge([ds_et_results, np.sqrt(temp)])\n",
    "\n",
    "ds_et_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275dd19c-8f7a-4200-8fbf-8c588b4e9bca",
   "metadata": {},
   "source": [
    "### Comparison of TC Pre- and Post-Aggregation\n",
    "\n",
    "Since the regional ET totals are technically collocated data sets, we can apply TC to estimate the error variances of these regional time series and compare to the error variances estimated by aggregating the gridded TC error variances from the [TC notebook](2_TC_application.ipynb#TC-Discussion) over the regions. This comparison can help give us an idea of how utilizing TC pre- and post-aggregating can affect the resulting error variance estimates.\n",
    "\n",
    "First, we read in the EC function to compute the TC error estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e18c91f-ace5-40ce-aa56-280c81d5a19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../TC/EC_function.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe459a3-ed52-446d-8a7e-ef6e4b26d1ea",
   "metadata": {},
   "source": [
    "Then we generate the list all possible data set triplets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dc3a96-7e22-4dea-b365-595e6de6793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of the combinations\n",
    "combos = list(itertools.combinations(dataset_name, 3))\n",
    "combos = [list(combo) for combo in combos]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96515deb-789a-4d7e-8b94-f13459265550",
   "metadata": {},
   "source": [
    "Since the data sets span different data ranges, we need to create a function that will limit the triplet to their common date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0537bc-3fa4-492b-bbdc-75e3435328aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_date_range(ds, combo):\n",
    "    \"\"\"Return the common date slice of the datasets.\"\"\"\n",
    "    old_common_date = []\n",
    "    recent_common_date = []\n",
    "    for name in combo:\n",
    "        old_common_date.append(date_ranges[name][0])\n",
    "        recent_common_date.append(date_ranges[name][1])\n",
    "    \n",
    "    return slice(np.max(old_common_date), np.min(recent_common_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3fe8fe-86ed-40f2-aef3-86b984318a91",
   "metadata": {},
   "source": [
    "Finally, we can compute the TC error and unbiased SNR estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e8680f-4e8a-4a54-8557-fb9b9e5912af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to ignore all of the sqrt and log warnings with negative values\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "ec_var_est = []\n",
    "for season in ds_results.season.data:\n",
    "    if season == 'All':\n",
    "        ds_season = ds_et_regional\n",
    "    else:\n",
    "        ds_season = ds_et_regional.isel(time=(ds_et_regional.time.dt.season == season))\n",
    "\n",
    "    ec_var_est_combo = []\n",
    "    for combo in combos:\n",
    "        ds_combo = ds_season.sel(time=common_date_range(ds_et_regional, combo), dataset_name=combo)\n",
    "\n",
    "        ec_covar, snr_temp = ec_covar_multi(ds_combo.aet.data, corr_sets=[1, 2, 3], return_snr=True)\n",
    "\n",
    "        ec_var_est_combo.append(xr.Dataset(data_vars={'error': (['var_combo', 'season', 'est_idx', 'region_name'],\n",
    "                                                                np.sqrt(np.diagonal(ec_covar).T)[None, None, ...]),\n",
    "                                                      'snr': (['var_combo', 'season', 'est_idx', 'region_name'],\n",
    "                                                              (10 ** np.log10(snr_temp[None, None, ...]))),},\n",
    "                                           coords={'var_combo': [' '.join(combo)], 'season': [season], \n",
    "                                                   'est_idx': [0, 1, 2], 'region_name': ds_et_regional.region_name}))\n",
    "\n",
    "    ec_var_est.append(xr.concat(ec_var_est_combo, dim='var_combo'))\n",
    "\n",
    "ec_est = xr.concat(ec_var_est, dim='season')\n",
    "\n",
    "# Restructure the data to be by data set vs by data set pair.\n",
    "ec_est_by_dataset = []\n",
    "est_pair = []\n",
    "for name in dataset_name:\n",
    "    idx_loc = np.char.find(ec_est.var_combo.data, name)\n",
    "    dataset_loc = np.where(idx_loc != -1)[0]\n",
    "    combos_single_dataset = ec_est.isel(var_combo=dataset_loc)\n",
    "    ec_est_dataset = []\n",
    "    for i in range(len(combos_single_dataset.var_combo.values)):\n",
    "        ec_est_single_dataset = combos_single_dataset.isel(var_combo=i)\n",
    "        idx = str(ec_est_single_dataset.var_combo.data).split(' ').index(name)\n",
    "        ec_est_single_dataset = ec_est_single_dataset.sel(est_idx=idx)\n",
    "        ec_est_single_dataset = ec_est_single_dataset.drop_vars(['var_combo', 'est_idx'])\n",
    "\n",
    "        ec_est_dataset.append(xr.Dataset(data_vars=ec_est_single_dataset[['error', 'snr']],\n",
    "                              coords={'dataset_name': name, 'est_idx': i, 'region_name': ec_est.region_name}))\n",
    "    \n",
    "    ec_est_dataset = xr.concat(ec_est_dataset, dim='est_idx')\n",
    "    \n",
    "    ec_est_by_dataset.append(ec_est_dataset)\n",
    "    est_pair.append([combinations.replace(name, '').strip() for combinations in ec_est.var_combo.data[dataset_loc]])\n",
    "\n",
    "ec_est = xr.concat(ec_est_by_dataset, dim='dataset_name')\n",
    "ec_est = ec_est.assign_coords(est_pair=(['dataset_name', 'est_idx'], np.array(est_pair)))\n",
    "ec_est"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749f916c-f5b3-4b3b-bf86-a8c91ab2b741",
   "metadata": {},
   "source": [
    "Now, let's make a plot that will compare the error estimates generated by TC pre- and post-aggregation to the regions. Actually, rather than comparing the error estimates, let's compare the unbiased SNRs. This will allow for an easier comparison, since the SNR is just unbaised signal divided by the error variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d821f40c-cb83-48f7-a7e6-4b388234a28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_agg = xr.Dataset(data_vars={'pre-aggregation': ds_et_results.snr.rename({'var_est_idx': 'est_idx'}),\n",
    "                               'post-aggregation': ec_est.snr})\n",
    "\n",
    "def snr_plt(grouping='dataset_name'):\n",
    "\n",
    "    coords = ['season', 'dataset_name', 'region_name']\n",
    "    coords.remove(grouping)\n",
    "    \n",
    "    plt = (tc_agg.hvplot.bar(x=grouping, y=['pre-aggregation', 'post-aggregation'], groupby=['est_idx']+coords,\n",
    "                            ylabel='Unbiased SNR', xlabel=' '.join(w.capitalize() for w in grouping.split('_')),\n",
    "                            rot=90)\n",
    "            * hv.HLine(1).opts(color='black', line_width=1, line_dash='dashed')).opts(frame_height=200)\n",
    "\n",
    "    return pn.panel(plt, widget_location='top')\n",
    "\n",
    "grouping_widget = pn.widgets.Select(name=\"grouping\", value=\"dataset_name\",\n",
    "                                    options=['season', 'dataset_name', 'region_name'])\n",
    "\n",
    "bound_plot = pn.bind(snr_plt, grouping=grouping_widget)\n",
    "\n",
    "pn.Column(grouping_widget, bound_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8ae6e2-067f-4199-9dc7-5c2181d72b5a",
   "metadata": {},
   "source": [
    "From this comparison, we can see that the SNR generated post-aggregation tends to have numerous `NaN` values, due to ET estimating negative error variances. This also occurred in the pre-aggregation maps, where serveral pixels in the region could have negative error variances. However, in our regional aggregation, we exclude these `NaN` values and only estimate the area weighted average using valid pixels. This process was cleanly hidden by `xarray.weighted.mean` as this method [automatically excludes `NaN` values and adjusts the weights accordingly](https://docs.xarray.dev/en/stable/user-guide/computation.html#comput-weighted). Therefore, let's continue to use the errors and SNRs that were generated from TC pre-aggregation as they have the `NaN` values averaged out. Also, using the pre-aggregated results retain the variation across the region rather then aggregating that variation out by aggregating before computing the TC errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98982232-9e95-4f0c-bb20-4fec117cada4",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "### Regional TC Estimates\n",
    "\n",
    "Since we decided to use the pre-aggregation TC error estimates, let's remake the above plot, but without the different calculation comparision. In other words, lets just plot the SNR of each data set for each season and region. This will allow us to explore what data sets are performing better (i.e., higher SNR) in each region and season. Also, since we are following the layout of the [TC notebook](2_TC_application.ipynb), let's plot the median of the different triplet estimates in addition to each individual estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae17a02-cb3f-408b-9de8-f75e06d40a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_snr = ds_et_results.snr.to_dataset(dim='dataset_name').drop_vars('var_est_pair')\n",
    "\n",
    "plt = (ds_snr.hvplot.bar(x='season', groupby=['region_name', 'var_est_idx'], ylabel='Unbiased SNR',\n",
    "                            xlabel='Season', rot=90, ylim=(0, 25))\n",
    "       * hv.HLine(1).opts(color='black', line_width=1, line_dash='dashed')\n",
    "       * hv.HLine(5).opts(color='black', line_width=1, line_dash='dashed')).opts(frame_height=150)\n",
    "plt =plt + (ds_snr\n",
    "            .median(dim='var_est_idx')\n",
    "            .hvplot.bar(x='season', groupby=['region_name'], ylabel='Median Unbiased SNR',\n",
    "                        xlabel='Season', rot=90, ylim=(0, 25))\n",
    "            * hv.HLine(1).opts(color='black', line_width=1, line_dash='dashed')\n",
    "            * hv.HLine(5).opts(color='black', line_width=1, line_dash='dashed')).opts(frame_height=150)\n",
    "\n",
    "pn.panel(plt.cols(1), widget_location='top')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a35c3e1-b594-449e-bbfd-6dfd28b145bf",
   "metadata": {},
   "source": [
    "From this comparison, we can see that in the Central Valley NLDAS, ERA5, and GLEAM have relatively high (>5) SNR for most seasons and combinations of data set triplets, whereas WBET has some high and low SNRs, and TerraClimate and SSEBop rarely have an SNR >1 at any point. This likely indicates that these three data sets (especially TerraClimate and SSEBop) are not optimized to function in this region, which could have implications on their use there. As for the High Plains Aquifer region, all data sets besides TerraClimate seem to have relativley high (>5) SNR values across all seaons. Looking at the seasons individually, most data sets have at least moderate (>1) SNR (besides TerraClimate). The SNR in winter is more variable and can have low (<1) SNR, which is expected. As we saw in the [TC notebook](2_TC_application.ipynb#TC-Estimation), the ET across CONUS is very low if not zero in for regions that actually experience a winter season (i.e. snow). Therefore, we would expect a low SNR for most if not all data sets during the winter months in both the High Plains and Upper Colorado. Finally, the Upper Colorado River Basin has a similar SNR pattern as the High Plains with the highest SNR in the Spring and Fall and lowest in the winter. For these two regions, no data sets really stand out as the optimal performer. Instead each seem to perform similarly well during each season."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e97554-5866-46d1-a20a-06fa173a787f",
   "metadata": {},
   "source": [
    "### Regional EC Cross-Correlation Estimates\n",
    "\n",
    "Using the pre-aggregation EC error estimates like the SNR plot, let's make another figure. This time of the EC error cross correlations of each data set for each season and region. This way we can check if any data sets are consistently correlated in our regions of interest. Again, to follow the previous [EC notebook](3_EC_application.ipynb), let's also plot the median cross correlation. We will not plot the cross-correlation SNR as we can easily flip through each estimate to get a view of how they vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea1c53e-6483-4a1e-a680-8d0a83e5ba15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agreement_by_dataset_plt(dataset_name='SSEBop', region_name='Central Valley', est_idx=0):\n",
    "\n",
    "    idx_loc = np.char.find(ds_et_results.dataset_pair.data, dataset_name)\n",
    "    ds_rho = (ds_et_results['rho']\n",
    "              .isel(dataset_pair=(idx_loc != -1))\n",
    "              .sel(region_name=region_name)\n",
    "              .drop_vars(['est_pair']))\n",
    "    ds_rho['dataset_pair'] = np.char.strip(\n",
    "        np.char.replace(ds_rho['dataset_pair'].data, dataset_name, '')\n",
    "    )\n",
    "    ds_rho = ds_rho.to_dataset(dim='dataset_pair')\n",
    "\n",
    "    plt = (ds_rho.sel(est_idx=est_idx).hvplot.bar(x='season', ylabel='Error Cross Correlation',\n",
    "                                   xlabel='Season', rot=90, ylim=(-1, 1))\n",
    "           * hv.HLine(0).opts(color='black', line_width=1, line_dash='dashed')).opts(frame_height=200, frame_width=500)\n",
    "\n",
    "    plt = plt + (ds_rho.median(dim='est_idx').hvplot.bar(x='season', ylabel='Median Error Cross Correlation',\n",
    "                                   xlabel='Season', rot=90, ylim=(-1, 1))\n",
    "                 * hv.HLine(0).opts(color='black', line_width=1, line_dash='dashed')).opts(frame_height=200, frame_width=500)\n",
    "\n",
    "    return plt.cols(2)\n",
    "\n",
    "dataset_name_widget = pn.widgets.Select(name=\"dataset_name\", value=\"SSEBop\", options=dataset_name)\n",
    "region_name_widget = pn.widgets.Select(name=\"region_name\", value=\"Central Valley\", options=list(ds_et_results.region_name.data))\n",
    "est_idx_widget = pn.widgets.IntSlider(name='est_idx', start=0, end=5)\n",
    "\n",
    "bound_plot = pn.bind(agreement_by_dataset_plt, dataset_name=dataset_name_widget, region_name=region_name_widget, est_idx=est_idx_widget)\n",
    "\n",
    "pn.Column(dataset_name_widget, region_name_widget, est_idx_widget, bound_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb366c2-eb53-4c83-a83f-3ada2aca7894",
   "metadata": {},
   "source": [
    "From the plots, we can see that the trends we saw in the maps in the [EC notebook](3_EC_application.ipynb) still hold. Those being SSEBop and WBET, along with ERA-5 and TerraClimate, have strong positive covariances, while SSEBop and TerraClimate have negative covariances. Besides these correlations, we can also see that GLEAM and ERA-5 seem to have positive correlations as well in all three regions. Therefore, this regional analysis allowed us to find another correlated pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04385d32-ef9c-4b8f-b897-579f42b1613f",
   "metadata": {},
   "source": [
    "### Data Set Agreement\n",
    "\n",
    "Finally, let's look at the agreement between data sets for each region to see if those that have the higher SNRs (i.e., smaller errors) or strong correlations are in any less agreement with the other data sets. To do this, we will use the same method as in the [Bias notebook](4_Bias.ipynb#Relative-Bias-Discussion) to calculate the agreement probability. As a reminder, this method consists of:\n",
    "\n",
    "1. Calculating the absolute relative bias ($\\textrm{bias}_\\textrm{abs} = | \\textrm{dataset}_A - \\textrm{dataset}_B |$),\n",
    "2. [Propagating the data sets' error variances and covariance](https://en.wikipedia.org/wiki/Propagation_of_uncertainty#Example_formulae) to the bias uncertainty ($\\sigma_{\\varepsilon_{\\rm bias}} = \\sqrt{\\sigma_{\\varepsilon_A}^2 + \\sigma_{\\varepsilon_B}^2 - 2 \\sigma_{\\varepsilon_{AB}}}$), and\n",
    "3. Calculating the probability density of a normal distribution of mean $\\textrm{bias}_\\textrm{abs}$ and variance $\\sigma_{\\varepsilon_{\\rm bias}}^2$ that is less than or equal 0.\n",
    "\n",
    "So, let's go ahead and calculate the agreement probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7dc438-a2ba-4aed-be5e-377ed8f16dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = ds_et_results['covar'].linalg.diagonal(\n",
    "                dims=['covar_pair_idx_1', 'covar_pair_idx_2'], offset=0,\n",
    "            )\n",
    "covariances = ds_et_results['covar'].linalg.diagonal(\n",
    "                dims=['covar_pair_idx_1', 'covar_pair_idx_2'], offset=1,\n",
    "              ).squeeze()\n",
    "ds_et_results['sigma_bias'] = np.sqrt(variances.sum(dim='covar_pair_idx_1') - 2 * covariances)\n",
    "\n",
    "norm_dist = stats.XrContinuousRV(norm,\n",
    "                                 loc=np.abs(ds_et_results['median_bias']),\n",
    "                                 scale=ds_et_results['sigma_bias'])\n",
    "# Set and name it as a DataArray as the attributes of the coordinates are not kept\n",
    "agreement_probability = norm_dist.cdf(0)\n",
    "agreement_probability.name = 'agreement_probability'\n",
    "\n",
    "# Merge to preserve coordinate attributes\n",
    "ds_et_results = xr.merge([ds_et_results, agreement_probability])\n",
    "ds_et_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7204de9e-fe16-4d95-aac3-f629fff6ddf1",
   "metadata": {},
   "source": [
    "Now that we have the agreement probabilities, let's make a figure to visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32eeadc-f388-42fa-8772-f4d6bc3be685",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_agreement = ds_et_results['agreement_probability'].drop_vars('est_pair').to_dataset('dataset_pair')\n",
    "\n",
    "plt = (ds_agreement.hvplot.bar(x='region_name', groupby=['season', 'est_idx'], ylabel='Agreement Probability',\n",
    "                               xlabel='Region', rot=90, ylim=(0, 0.5))\n",
    "       * hv.HLine(0.16).opts(color='black', line_width=1, line_dash='dashed')\n",
    "       * hv.HLine(0.05).opts(color='black', line_width=1, line_dash='dashed')).opts(frame_height=200, frame_width=750)\n",
    "pn.panel(plt, widget_location='top')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30959ceb-e958-4bc4-9608-d3b8abc6427d",
   "metadata": {},
   "source": [
    "First from this figure, we can see that each of the agreement probability estimates for the data set pairs are very consistent, just as we saw in the [agreement notebook](4_dataset_agreement.ipynb). This is good as it shows that the variation in covariance matrices is not having a major influence on the agreement estimates. In terms of values, we can see that the High Plains and Upper Colorado have high agreement between all data sets when looking over all time (p > 0.16). When looking at their seasonal data, we can see that in winter and spring only about half of the data set pairs have high agreement (p > 0.16) and even some with low agreement (p < 0.05). This isn't unexpected as these regions can have snow during these months which can make ET estimates less reliable. Therefore, we would expect greater variation and bias during these months. As for the summer and fall, we see high agreement in almost all data sets in the High Plains and only a few low agreement probabilities in the Upper Colorado. Therefore, in these two regions, the data sets seem be in reasonable agreement within their errors.\n",
    "\n",
    "However, this is not the case for the Central Valley. Overall time, the data sets do appear to be in high agreement (p > 0.16) except for NLDAS with ERA5, which have only moderate agreement (0.05 < p < 0.16). Yet, when looking at the seasons, any overall agreement seems to disappear. In the winter and summer, only a few data sets have any agreement, with the majority of data set pairs having extremely low agreement (<0.01). In the spring and fall, about two thirds of the data set pairs have high agreement, with the other third having moderate to low agreement (p > 0.16). This difference in agreement between seasonal and all season data is likely due to the way we aggregated our relative biases back in the [agreement notebook](4_dataset_agreement.ipynb). By aggregating all biases to the median across time, we are smoothing the variation across seasons. Therefore, it is likely that most ET data sets are not in agreement in the Central Valley in the peak ET season of summer, indicating the the choice of ET data set in the Central Valley can have impact on any results derived from it. \n",
    "\n",
    "Therefore, let's look deeper at the Central Valley to see how the agreement may be affecting the ET data sets in that region. Specifically, let's focus in on the 2012-2016 extreme drought period that devastated the region. First, let's plot the time series data again for that period, but with the associated median seasonal uncertainty included as a shaded area. Also, let's plot the cumulative ET of that time series to see how each data set compares in the estimated total ET during the drought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81060252-02e5-41a8-b006-739fb1b83066",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_drought = ds_et_regional.copy()\n",
    "\n",
    "ds_drought['error'] = xr.zeros_like(ds_drought['aet'])\n",
    "seasons = ds_drought.time.dt.season\n",
    "for season in np.unique(seasons):\n",
    "    ds_drought['error'] = ds_drought['error'].where(~(seasons == season),\n",
    "                                                    ds_et_results.error.median(dim='var_est_idx').sel(season=season))\n",
    "\n",
    "ds_drought['high'] = ds_drought['aet'] + ds_drought['error']\n",
    "ds_drought['low'] = ds_drought['aet'] - ds_drought['error']\n",
    "ds_drought = ds_drought.drop_vars('season')\n",
    "\n",
    "ds_cumsum = ds_drought.aet.cumsum(dim='time').to_dataset()\n",
    "ds_cumsum['aet'] = ds_cumsum.aet - ds_cumsum.aet.sel(time='10-01-2011')\n",
    "ds_cumsum.aet.attrs['long_name'] = 'Cumulative AET'\n",
    "ds_cumsum.aet.attrs['units'] = 'mm.month-1'\n",
    "\n",
    "xlim = (pd.to_datetime('10-01-2011'), pd.to_datetime('09-30-2016'))\n",
    "plt = ((ds_drought.aet.hvplot(groupby=['dataset_name', 'region_name'], xlim=xlim,\n",
    "                              ylim=(-20, 150)).overlay('dataset_name').opts(legend_position='right')\n",
    "        * ds_drought.hvplot.area(x='time', y='high', y2='low', groupby=['dataset_name', 'region_name'], alpha=0.5).overlay('dataset_name'))\n",
    "       + (ds_cumsum.aet.hvplot(groupby=['dataset_name', 'region_name'], ylim=(-100, 2700),\n",
    "                               xlim=xlim).overlay('dataset_name').opts(legend_position='right')))\n",
    "\n",
    "pn.panel(plt.cols(1), widget_location='top')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aaed51-e06f-4d45-bbe5-d2d2d0ab2990",
   "metadata": {},
   "source": [
    "Well, these are some very interesting results. As we saw above in the first time series plot, the Central Valley shows a misalignment between several of the data set. Even when including the errors, we do not see any better alignment or agreement between data sets. As stated above this offset is likely due to the high levels of irrigation in the Central Valley that are not being included in GLEAM, ERA5, NLDAS, and TerraClimate. Therefore, the misalignment in peak ET could explain some of the low agreement during the summer and potentially cause a serious bias issue on any derived product as the ET timings would be off by three months.\n",
    "\n",
    "As for the cumulative ET plot, it is clear that there is a bias issue between each of the data sets, with WBET (SSEBop and ERA5) estimating almost double (one and a half) the ET as GLEAM, NLDAS, and TerraClimate. Since we have the errors on the time series, let's use that to generate some simulations to estimate if the differences in the cumulative distributions are statistically significant. We can do this with a simple two sample KS test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db874412-6ab0-4887-8bd1-356339ac3705",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "\n",
    "ds_drought_ks = ds_drought.sel(time=slice('10-01-2011', '09-30-2016'))\n",
    "x = rng.normal(loc=np.expand_dims(ds_drought_ks.aet, -1),\n",
    "               scale=np.expand_dims(ds_drought_ks.error, -1),\n",
    "               size=ds_drought_ks.error.shape + (1000,))\n",
    "ds_drought_ks['aet_sim'] = (['time', 'dataset_name', 'region_name', 'sim'], x)\n",
    "\n",
    "pairs = list(itertools.combinations(dataset_name, 2))\n",
    "pairs = [list(pair) for pair in pairs]\n",
    "\n",
    "for region_name in ds_drought_ks.region_name.data:\n",
    "    ds_region = ds_drought_ks.sel(region_name=region_name)\n",
    "    print(region_name)\n",
    "    for pair in pairs:\n",
    "        results = kstest(ds_region.aet_sim.sel(dataset_name=pair[0]), ds_region.aet_sim.sel(dataset_name=pair[1]))\n",
    "        print(f'K-S Test results for {pair[0]} and {pair[1]} p-value = {np.median(results[1]):.4f} +/- {np.std(results[1]):.4f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c03aabd-87d5-4a78-948b-5b5ed5ebcf17",
   "metadata": {},
   "source": [
    "From the KS tests, we can see that as per the plot only GLEAM, NLDAS, and TerraClimate cannot be deemed to have a statistically significance difference (p < 0.05) in their cumulative ET. Even SSEBop and ERA5 have different cumulative ET, while visually looking relatively similar. This disagreement is likely from the offset in the time series causing the significant difference. Therefore, the differences in the bias of these ET data sets should be taken into consideration when utilizing them in other studies.\n",
    "\n",
    "As a final thing to check, if we zoom out time-wise on the cumulative distributions, we can see that each data set seems to follow a constant slope. If this is the case, we could easily bias correct these data sets to some chosen standard. While exploring this further is beyond these notebooks, let's just make one last plot to see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef16a4ac-13f5-41f8-9676-46b91c761658",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_cumsum_fit = ds_drought.sel(time=slice('10-01-2011', '09-30-2016')).aet.cumsum(dim='time')\n",
    "ds_cumsum_fit['time'] = list(range(len(ds_cumsum_fit['time'])))\n",
    "fit_results = ds_cumsum_fit.curvefit('time', lambda params, x: params * x).squeeze()\n",
    "\n",
    "# For this test, we will choose WBET as our standard to correct to\n",
    "ds_drought_bias_adj = (fit_results.curvefit_coefficients.sel(dataset_name='WBET')\n",
    "                       / fit_results.curvefit_coefficients\n",
    "                       * ds_drought.aet)\n",
    "ds_drought_bias_adj.attrs['long_name'] = 'Bias corrected AET'\n",
    "ds_drought_bias_adj.attrs['units'] = 'mm.month-1'\n",
    "\n",
    "plt = (ds_drought.aet.hvplot(\n",
    "           groupby=['dataset_name', 'region_name'], ylim=(-20, 160), xlim=xlim\n",
    "       ).overlay('dataset_name').opts(legend_position='right')\n",
    "       + ds_drought_bias_adj.hvplot(\n",
    "           groupby=['dataset_name', 'region_name'], ylim=(-20, 160), xlim=xlim\n",
    "       ).overlay('dataset_name').opts(legend_position='right')\n",
    "       + (ds_drought_bias_adj.cumsum(dim='time') - ds_drought_bias_adj.cumsum(dim='time').sel(time='10-01-2011')).hvplot(\n",
    "           groupby=['dataset_name', 'region_name'], ylim=(-100, 2600), xlim=xlim\n",
    "       ).overlay('dataset_name').opts(legend_position='right'))\n",
    "\n",
    "pn.panel(plt.cols(1), widget_location='top')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a401e983-9046-4880-8b6d-1ea7a64fff88",
   "metadata": {},
   "source": [
    "Hey look at that, it seems like this kind of bias correction may work! So, these data sets could be corrected. One last thing to note is that while the raw data sets may likely need a bias correction, the TC error estimates that were at the center of this work should not change after a bias correction. As you can check and remember from the [TC function](../TC/TC_function.ipynb), the TC method we used assumes an affine error model that will include any linear biases that maybe present between data sets. Therefore, even in the presence of this bias, the error estimates made from the TC method should remain unchanged."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
